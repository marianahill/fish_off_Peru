---
title: "Fish-Exploration"
author: "Benjamin MÃ¼ller"
date: "2023-05-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Import data

Import the data

```{r import data, echo = FALSE, results = 'hide', message = FALSE}

#rm(list = ls())

library(readr)
df <- read_delim("G:/Benny/Lernkurse ohne Studium/Open Campus SH/SS 2023/Time Series/Project/Data/data.csv", 
                   delim = ";", escape_double = FALSE, locale = locale(decimal_mark = ","), 
                   trim_ws = TRUE)

#//time wurde mit 3 Nachkommastellen gerundet, sollte aber nicht passiert sein


#inspect the data
#are there columns with just NAs?

#check if each column has all missing values
all_miss <- apply(df, 2, function(x) all(is.na(x)))
#display columns with all missing values
#names(all_miss[all_miss>0])  

#Just column 35, which was empty in the original file anyway
#columns 39 and 40 also contain mostly NAs


df <- df[,c(-35, -39,-40)]

rm(all_miss)

```

##  Change the column names

```{r correct naming, echo=FALSE}

colnames(df)[colnames(df) == "shelf"] <- "distToShelf"
colnames(df)[colnames(df) == "dc"] <- "distToCoast"
colnames(df)[colnames(df) == "prof"] <- "depth"
colnames(df)[colnames(df) == "sst"] <- "seaSurfaceTemperature"
colnames(df)[colnames(df) == "sss"] <- "seaSurfaceSalinity"
colnames(df)[colnames(df) == "chl"] <- "chlorophyll"
colnames(df)[colnames(df) == "oxi"] <- "oxygen"
colnames(df)[colnames(df) == "oxiP"] <- "depthOxycline"

colnames(df)[colnames(df) == "anc"] <- "anchovy"
colnames(df)[colnames(df) == "ancp"] <- "smallAnchovy"
colnames(df)[colnames(df) == "ancg"] <- "largeAnchovy"
colnames(df)[colnames(df) == "sar"] <- "sardine"
colnames(df)[colnames(df) == "jur"] <- "jackMackerel"
colnames(df)[colnames(df) == "cab"] <- "chubMackerel"
colnames(df)[colnames(df) == "mun"] <- "redSquatLobster"
colnames(df)[colnames(df) == "sam"] <- "samasa"
colnames(df)[colnames(df) == "pot"] <- "HumboldtGiantSquid"
colnames(df)[colnames(df) == "vin"] <- "Vinciguerria"
colnames(df)[colnames(df) == "zoo"] <- "mesozooplankton"
colnames(df)[colnames(df) == "dem"] <- "demersalFish"
colnames(df)[colnames(df) == "mer"] <- "hake"
colnames(df)[colnames(df) == "otr"] <- "otherSpecies"
colnames(df)[colnames(df) == "cam"] <- "squid"



#Not for all variables the meaning is known.
#E.g. for bag, fvo, mic



```


##  Change to the right data type

Most variables are numeric anyway, so not much changes necessary. Only the date variable is changed to a lubridate data type.


```{r deal with time, echo=FALSE}

#sum(is.na(df$month))
#sum(is.na(df$day))
#sum(is.na(df$year))
#No missing values in month, day and year

#-------------------------------------

#create a datetime object out of year, month and day with lubridate
library(lubridate)

#test <- paste0(df$day, df$month, df$year)
#sum(is.na(test)) #0 NAs

df$date <- as.Date(paste0(df$year,"-", df$month, "-", df$day)) #as.Date expects data to be in the 2012-10-23 format.
#this is a base R method, we still have to convert to lubridate?

#check for NAs
#sum(is.na(df$date)) #works

#---------------------------------------


### later on we need to have a date, where just the time of the year is stored,but not the year itself ###


library(lubridate)
df$date2 <- as_date(format(df$date, "%m%d"), format = "%m%d")
#with as.Date it pretends that everything is 2023
#with as_date it sets year to 0. Okay?



```



## Missing values

Which variables have lot of missing values? Are missing values between two variables related? Generally missing values are not so mysterious in our dataset, because some species were not detected up to a certain point. This leads to there being NAs for that species up to that point.


###   Missing values disregarding time

```{r missing values - comment, echo=FALSE}

#Here is a gallery for different function froms the naniar package: https://cran.r-project.org/web/packages/naniar/vignettes/naniar-visualisation.html. There are also similar visualizations in the tidyverse, but they're mostly the same, but with more lines of code involved.

```



```{r missing values - gg_miss_var, echo=FALSE}

#--- with gg_miss_var
library(naniar)
gg_miss_var(df)

```
Small anchovy, large anchovy and other species have the largest amountof rows with missing values.



A sample of 5% is taken here, since the method cannot process the whole data. Given the sample is representative, we should still get a good impression.

```{r missing values - vis_miss, echo=FALSE}

#--- with vis_miss
library(naniar)
vis_miss(dplyr::slice_sample(df, prop = 0.05))
#unfortunately not all data can be processed at once

```


```{r missing values - gg_miss_upset, echo=FALSE}

#--- with gg_miss_upset
#you could also use the package upsetR, but this seems to be more convenient

library(naniar)
gg_miss_upset(df, order.by = "freq", nsets = 10, nintersects = 10)

```

That means, that there are 77.540 rows of data, where all data from the mentioned variables is missing. There are 54.302 rows of data, where just demersal fish, hake, toher species, small anchovy and large anchovy are missing.


```{r missing values - geom_miss_point, echo=FALSE}

#--- with geom_miss_point

library(naniar)
library(ggplot2)

# using  geom_miss_point()
ggplot(df,aes(x = sardine, y = anchovy)) +
 geom_miss_point()


```
If you go through several possible combinations of marine animals, most fishes do not seem to occur together very often, because the turquoise points form an L shape.

There are just missing sardines, if anchovy is at the 0 level. But the rows with sardine missing vary.


###   Missing values regarding time

Let's have a look, how the missing values are distributed in time. The red markings indicate missing data. (Hard to interprete somehow)

```{r missing values - time - comment, echo=FALSE}

#Here are some examples:
#https://cran.r-project.org/web/packages/imputeTS/vignettes/gallery_visualizations.html

```



First let's have a look at the missing value distribution of anchovy in time.

```{r missing values - ggplot_na_distribution - anchovy, echo=FALSE}

#--- with ggplot_na_distribution

library("imputeTS")
ggplot_na_distribution(x = df$anchovy, x_axis_labels = df$date)

#this function also works on df directly, i.e. without specifying the variable. But it will just work properly with univariate input.


```

Anchovy seems to have just some missing values at the end of the time series.


```{r missing values - ggplot_na_distribution - red Squat Lobster, echo=FALSE}

#--- with ggplot_na_distribution

library("imputeTS")
ggplot_na_distribution(x = df$redSquatLobster, x_axis_labels = df$date)

#this function also works on df directly, i.e. without specifying the variable. But it will just work properly with univariate input.


```

Red Squat Lobster seems to have no values up until 1998.

###   Missing values regarding time - gap size


```{r missing values - ggplot_na_gapsize - anchovy, echo=FALSE}

#--- with ggplot_na_gapsize

library(imputeTS)
ggplot_na_gapsize(df$anchovy)


```
For anchovy, there are about 10.000 gaps of size 1 in the data. Is there some kind of systematic behind it? There are also a lot of gaps with the size 5964. Why this specific number?


```{r missing values - ggplot_na_gapsize - redSquatLobster, echo=FALSE}

#--- with ggplot_na_gapsize

library(imputeTS)
ggplot_na_gapsize(df$redSquatLobster)


```
With red squat lobster there is one large gap at the beginning of the time series, with size of 74.170. Here is also a gap of 5964.

Does that mean the gaps between the cruises are really that regular?



## Deal with missing values

The gaps with missing values are quite large, as we will see later, so that might not be a good idea. Let's see how a naive imputation works for anchovy.


```{r deal missing values - anchovy, echo=FALSE, results='hide'}

library(imputeTS)
imp <- na_interpolation(df$anchovy)
ggplot_na_imputations(df$anchovy, imp)

rm(imp)

```

Since we have more than one value per day, one can hardly spot, if the imputation is plausible.


##  Distribution of data over the year

How do the data change over time? During which months was the data taken?

Unfortunately it is not possible to show all years at once, so we have to split the data into 5 years chunks.

```{r distr year 1983-2012, echo=FALSE}


#I'd like to plot a diagram that shows, which in which months each year
#the ship was driving around

#More interesting could be a histogram of time over each year

library(ggplot2)
library(dplyr)
plot <- df %>% 
    mutate(date = ymd(date)) %>% 
    ggplot(aes(date)) +
    ggtitle("Distribution of data in the year 2012") +
    geom_histogram(binwidth = 1) +
    #facet_grid(year ~., scales = "free", space = "free") #Can't really do 30 rows with facet_grid
    facet_wrap(year~.,  scales = "free", nrow = 5)

#plot all years at once
#print(plot)

rm(plot)

#plot is not really helpful with free x and y axis


```


```{r distr year 1983-1987, echo=FALSE}


#I'd like to plot a diagram that shows, which in which months each year
#the ship was driving around

#More interesting could be a histogram of time over each year

library(ggplot2)
library(dplyr)
library(scales)

df.temp <- subset(df, subset = year %in% c(1983,1984,1985,1986,1987))

plot <- df.temp %>% 
    mutate(date2 = ymd(date2)) %>% 
    ggplot(aes(date2)) +
    ggtitle("Distribution of data in the years 1983-1987") +
    geom_histogram(binwidth = 1) +
    scale_x_date(labels = date_format("%b")) +
    xlab("Time of the year") +
    facet_grid(year ~., scales = "free", space = "free") #Can't really do 30 rows with facet_grid

print(plot)

#clean up
rm(plot, df.temp)


```

```{r distr year 1988-1992, echo=FALSE}


#I'd like to plot a diagram that shows, which in which months each year
#the ship was driving around

#More interesting could be a histogram of time over each year

library(ggplot2)
library(dplyr)
library(scales)

df.temp <- subset(df, subset = year %in% c(1988,1989,1990,1991,1992))

plot <- df.temp %>% 
    mutate(date2 = ymd(date2)) %>% 
    ggplot(aes(date2)) +
    ggtitle("Distribution of data in the years 1988-1992") +
    geom_histogram(binwidth = 1) +
    scale_x_date(labels = date_format("%b")) +
    xlab("Time of the year") +
    facet_grid(year ~., scales = "free", space = "free") #Can't really do 30 rows with facet_grid

#plot all years at once
print(plot)

#clean up
rm(plot, df.temp)


```

```{r distr year 1993-1997, echo=FALSE}


#I'd like to plot a diagram that shows, which in which months each year
#the ship was driving around

#More interesting could be a histogram of time over each year

library(ggplot2)
library(dplyr)
library(scales)

df.temp <- subset(df, subset = year %in% c(1993,1994,1995,1996,1997))


plot <- df.temp %>% 
    mutate(date2 = ymd(date2)) %>% 
    ggplot(aes(date2)) +
    ggtitle("Distribution of data in the years 1993-1997") +
    geom_histogram(binwidth = 1) +
    scale_x_date(labels = date_format("%b")) +
    xlab("Time of the year") +
    facet_grid(year ~., scales = "free", space = "free") #Can't really do 30 rows with facet_grid

#plot all years at once
print(plot)

#clean up
rm(plot, df.temp)


```

```{r distr year 1998-2002, echo=FALSE}


#I'd like to plot a diagram that shows, which in which months each year
#the ship was driving around

#More interesting could be a histogram of time over each year

library(ggplot2)
library(dplyr)
library(scales)

df.temp <- subset(df, subset = year %in% c(1998,1999,2000,2001,2002))


plot <- df.temp %>% 
    mutate(date2 = ymd(date2)) %>% 
    ggplot(aes(date2)) +
    ggtitle("Distribution of data in the years 1998-2002") +
    geom_histogram(binwidth = 1) +
    scale_x_date(labels = date_format("%b")) +
    xlab("Time of the year") +
    facet_grid(year ~., scales = "free", space = "free") #Can't really do 30 rows with facet_grid

#plot all years at once
print(plot)

#clean up
rm(plot, df.temp)


```

```{r distr year 2003-2007, echo=FALSE}


#I'd like to plot a diagram that shows, which in which months each year
#the ship was driving around

#More interesting could be a histogram of time over each year

library(ggplot2)
library(dplyr)
library(scales)

df.temp <- subset(df, subset = year %in% c(2003,2004,2005,2006,2007))


plot <- df.temp %>% 
    mutate(date2 = ymd(date2)) %>% 
    ggplot(aes(date2)) +
    ggtitle("Distribution of data in the years 2003-2007") +
    geom_histogram(binwidth = 1) +
    scale_x_date(labels = date_format("%b")) +
    xlab("Time of the year") +
    facet_grid(year ~., scales = "free", space = "free") #Can't really do 30 rows with facet_grid

#plot all years at once
print(plot)

#clean up
rm(plot, df.temp)


```

```{r distr year 2008-2012, echo=FALSE}


#I'd like to plot a diagram that shows, which in which months each year
#the ship was driving around

#More interesting could be a histogram of time over each year

library(ggplot2)
library(dplyr)
library(lubridate)
library(scales)

df.temp <- subset(df, subset = year %in% c(2008,2009,2010,2011,2012))


plot <- df.temp %>% 
    mutate(date2 = ymd(date2)) %>% 
    ggplot(aes(date2)) +
    ggtitle("Distribution of data in the years 2008-2012") +
    geom_histogram(binwidth = 1) +
    scale_x_date(labels = date_format("%b")) +
    xlab("Time of the year") +
    facet_grid(year ~., scales = "free", space = "free") #Can't really do 30 rows with facet_grid

#plot all years at once
print(plot)

#clean up
rm(plot, df.temp)


```
One can see that there are huge gaps in our data and we cannot treat our data as a usual time series.


##    Spatial distribution of the data

Where on the world is the data actually from?

```{r total space, echo=FALSE}

library(sp)
library(leaflet)

df.temp <- dplyr::slice_sample(df, prop = 0.001)

df.temp <- data.frame(longitude = df.temp$lon, 
                 latitude = df.temp$lat)

coordinates(df.temp) <- ~longitude+latitude
leaflet(df.temp) %>% addMarkers() %>% addTiles()

rm(df.temp)

```

Data is in front of the coast of Peru

Can we divide the data by year? Let's create a map for every year.

```{r space per year, echo=FALSE, warning = FALSE}

#okay we must first sort out the unique spatial points in the correct order
df.temp <- df[!duplicated(df$lon), ]
#///that is a little brute force, since the same coordinate might show up later again

#Control for Years
year = unique(df$year)
#df <- df[df$year == 1983,]
MapList.year <- vector(mode = "list")

#remotes::install_github("Lchiffon/leafletCN")
library(leafletCN) #foraddTitle for Leaflet
library(leaflet)

for (i in year){

  map <-  df.temp[df.temp$year == i,] %>%
          leaflet() %>% 
          addTiles() %>% 
          addCircles() %>%             
          setView(lng = mean(df$lon), lat = mean(df$lat), zoom = 4) %>%
          #keep the original size of the original data, i.e. the Peru EEZ
          suspendScroll(sleepNote = FALSE, hoverToWake = FALSE)
          #you cannot scroll anymore
  map <- list(map)                #push the map one down in order
  MapList.year <- append(MapList.year, map) #so that a map here is its own list entry
}


library(leafsync)
latticeView(MapList.year, ncol = 5)

rm(df.temp, df.temp2, map, i, year)

```


In earlier years the the data points are going from a straight line away from the coast in rows.
In later years, the data points rather look like zick zacks.

In some years there seem to be a lot more data than in other years, maybe a contour plot per year would help here more.

##    Which route does the ship take?

```{r space route, echo=FALSE}

df.temp <- df[!duplicated(df$lon), ]
df.temp <- df.temp[df.temp$year == 2012,]
df.temp <- df.temp[1:400,] #play a little with the amount of points shown
map <- leaflet(df.temp) %>% addTiles() %>% addCircles()
map

rm(map, df.temp)

```


Playing around with the year and amount of data points, on notices that
the ship starts from the north in one year and from the south in another year.
You can also see how the zick zacks are built.

```{r space route 2, echo=FALSE}

df.temp <- df[!duplicated(df$lon), ]
df.temp <- df.temp[df.temp$year == 2012,]
#df.temp <- df.temp[1:400,] #play a little with the amount of points shown
map <- leaflet(df.temp) %>% 
                addTiles() %>% 
                addCircles() %>%
                addPolylines(lat=~lat, lng=~lon, color = "red", weight = 0.5) %>%
                setView(lng = mean(df$lon), lat = mean(df$lat), zoom = 8) 

map


```

If you zoom in, you can see, that the blue data point dots do not represent the
order, in which the data points were collected. Instead the ship goes zick zack 
in these blue data points. So probably lat lon represent the start or endpoint 
of a voyage nd the line between probably the way on which the data was taken.

If that is true, one should also consider, that these voyages have not the same length!!!


##    Change the size of the blue circles
n
The idea is to change the size of the blue circles according to the amount of
marine animals present

```{r chaging blue circles, echo=FALSE}

df.temp <- df[!duplicated(df$lon), ]
df.temp <- df.temp[df.temp$year == 2012,]

df.temp2 <- df.temp[df.temp$year == 2012,]
map <- leaflet(df.temp2) %>%       
              addTiles() %>% 
              addCircles(radius = df$redSquatLobster, weight = 5) %>%
              setView(lng = mean(df$lon), lat = mean(df$lat), zoom = 8) 
map

rm(map, df.temp, df.temp2)


```

This pattern is much more irregular than anticipated. Curious that neighboring
points of a point with large amounts of red squat lobsters partially have such
a low value. So even with relatively immobile red squat lobsters there might be
moving swarm, which can randomly occur or not.




##    Create scaled and minimal dataset versions for later

Here we create a function to scale the dataset. Better than a generic function as the scaled variables are explicitly stated.

```{r scaled dataset - create, echo=FALSE}

#just scale the variables, which have outliers
#colnames(df)
library(tidyr)

df.scale <- df %>%
mutate(seaSurfaceTemperature_scal = scale(seaSurfaceTemperature),
    seaSurfaceSalinity_scal = scale(seaSurfaceSalinity),
    chlorophyll_scal = scale(chlorophyll),
    oxygen_scal = scale(oxygen),
    depthOxycline_scal = scale(depthOxycline),
    masas_scal = scale(masas),
    anchovy_scal = scale(anchovy),
    smallAnchovy_scal = scale(smallAnchovy),
    largeAnchovy_scal = scale(largeAnchovy),
    sardine_scal = scale(sardine),
    jackMackerel_scal = scale(jackMackerel),
    chubMackerel_scal = scale(chubMackerel),
    redSquatLobster_scal = scale(redSquatLobster),
    samasa_scal = scale(samasa),
    bag_scal = scale(bag),
    fvo_scal = scale(fvo),
    HumboldtGiantSquid_scal = scale(HumboldtGiantSquid),
    Vinciguerria_scal = scale(Vinciguerria),
    mic_scal = scale(mic),
    mesozooplankton_scal = scale(mesozooplankton),
    demersalFish_scal = scale(demersalFish),
    hake_scal = scale(hake),
    otherSpecies_scal = scale(otherSpecies),
    squid_scal = scale(squid),
    unclassified_scal = scale(unclassified))%>%
select(-c(seaSurfaceTemperature, seaSurfaceSalinity, chlorophyll, oxygen, depthOxycline, masas, anchovy, smallAnchovy, largeAnchovy, sardine, jackMackerel, chubMackerel, redSquatLobster, samasa, bag, fvo, HumboldtGiantSquid, Vinciguerria, mic, mesozooplankton, demersalFish, hake, otherSpecies, squid, unclassified ))

   #correct the column names by inserting the original colnames
   colnames(df.scale) <- colnames(df)



```

Create this as a function.


```{r scaled dataset - comment, echo=FALSE}

#How does this work, if not all variables explicitly stated are in the dataset you hand over to the funtion?


```



```{r scaled dataset - function, echo=FALSE}

#just scale the variables, which have outliers
#colnames(df)
library(tidyr)

scale.df <- function(df) {
   df.scale <- df %>%
mutate(seaSurfaceTemperature_scal = scale(seaSurfaceTemperature),
    seaSurfaceSalinity_scal = scale(seaSurfaceSalinity),
    chlorophyll_scal = scale(chlorophyll),
    oxygen_scal = scale(oxygen),
    depthOxycline_scal = scale(depthOxycline),
    masas_scal = scale(masas),
    anchovy_scal = scale(anchovy),
    smallAnchovy_scal = scale(smallAnchovy),
    largeAnchovy_scal = scale(largeAnchovy),
    sardine_scal = scale(sardine),
    jackMackerel_scal = scale(jackMackerel),
    chubMackerel_scal = scale(chubMackerel),
    redSquatLobster_scal = scale(redSquatLobster),
    samasa_scal = scale(samasa),
    bag_scal = scale(bag),
    fvo_scal = scale(fvo),
    HumboldtGiantSquid_scal = scale(HumboldtGiantSquid),
    Vinciguerria_scal = scale(Vinciguerria),
    mic_scal = scale(mic),
    mesozooplankton_scal = scale(mesozooplankton),
    demersalFish_scal = scale(demersalFish),
    hake_scal = scale(hake),
    otherSpecies_scal = scale(otherSpecies),
    squid_scal = scale(squid),
    unclassified_scal = scale(unclassified))%>%
select(-c(seaSurfaceTemperature, seaSurfaceSalinity, chlorophyll, oxygen, depthOxycline, masas, anchovy, smallAnchovy, largeAnchovy, sardine, jackMackerel, chubMackerel, redSquatLobster, samasa, bag, fvo, HumboldtGiantSquid, Vinciguerria, mic, mesozooplankton, demersalFish, hake, otherSpecies, squid, unclassified ))
   
   #correct the column names by inserting the original colnames
   colnames(df.scale) <- colnames(df)
   
   return(df.scale)

}


```


```{r scaled dataset - function application, echo=FALSE}

df.scale <- scale.df(df)

rm(df.scale)


```

To create a dataset with the most relevant variables could be done in a
a similar fashion, but we have to leave additional columns in the future, like regions, which don't even exist yet, so this might not work here to make a generic subset function here.


##    Try to make a new dataset with aggregated data per cruise

###   Try to add a column with cruise number

Next we assign a cruise number to every data row, that exists. We say it stays the same cruise, as long as not more than 20 days between these two data rows. One has to correct this heuristic later on, so we will look through the assignment and change it, if necessary.



```{r add a cruise number to df, echo=FALSE}

#one could do that automatically by stating how much time should
#be empty before and after a cluster of data or do that by hand

#using "df$cruiseNo <- " really gets very slow, so work on a vector here

#df$cruiseNo <- 0

#-------------------------
#initialize start values
cruiseNo <- 0
cruiseNo[1] <- 1
timeBetween <- 20
#-------------------------


for (i in 2:dim(df)[1]){
  cruiseNo[i] <- if((df$date[i]-df$date[i-1])<timeBetween){
                      cruiseNo[i-1]
                    }else{
                      cruiseNo[i-1]+1}
} 


df$cruiseNo <- cruiseNo
df$cruiseNo <- as.factor(df$cruiseNo)
#class(df$cruiseNo)
nlevels(df$cruiseNo)


rm(timeBetween, i, cruiseNo)


```
The automatic cruise detector identified 68 different cruises. It could be beneficial to choose a lower time span to compare. I have gone for 20 days.



```{r add a cruise number to df - make it faster, echo=FALSE}

# this could make the function even faster by replacing 
# "df$date[i]-df$date[i-1]" by a diff vector, which is precalculated

#https://stackoverflow.com/questions/36895525/how-to-get-the-difference-between-next-and-previous-row-in-an-r-time-series

fdiff = function(x, lag = 1){
  # Number of obs
  n = length(x)

  # Trigger error to prevent subset
  if(n < 2+lag){stop("`x` must be greater than `2+lag`")}

  # X_(T+1) - X_(T-1)
  x[(2+lag):n] - x[1:(n-lag-1)]
}

diff <- fdiff(df$day)
#result is a vector of size 307.241 


rm(diff, fdiff) #not used, method above fast enough

```



###   Check if cruise assignment is correct

We could plot the courses with leaflet and check if the cruises are not cut in half spacewise. One cruiise could cover the north, another the south. Since they would complement each other, spacewise they would belong together.


```{r space per year - add title functions, echo=FALSE, warning = FALSE}

#this chunk should be used to set the title in the following leaflet.
#leaflet is based on html, or js, so we to make setting here.
#nevertheless setting titles doesn't work 100% until now

library(htmltools)

tag.map.title <- tags$style(HTML("
  .leaflet-control.map-title { 
    transform: translate(-50%,20%);
    position: fixed !important;
    left: 50%;
    text-align: center;
    padding-left: 10px; 
    padding-right: 10px; 
    background: rgba(255,255,255,0.75);
    font-weight: bold;
    font-size: 28px;
  }
"))

title <- tags$div(tag.map.title, HTML("Map title"))  


```




```{r space per year, echo=FALSE, warning = FALSE}

#okay we must first sort out the unique spatial points in the correct order
df.temp <- df[!duplicated(df$lon), ]
#///that is a little brute force, since the same coordinate might show up later again

#Control for Years
cruiseNo = unique(df$cruiseNo)
#df <- df[df$year == 1983,]
MapList.cruises <- vector(mode = "list")

#remotes::install_github("Lchiffon/leafletCN")
library(leafletCN) #foraddTitle for Leaflet
library(leaflet)
library(stringr)
library(leaflet.extras)



for (i in cruiseNo){

  #str_title <- str_glue("cruise no {i}")
  
  #title <- tags$div(tag.map.title, HTML(str_title)) 
  
  map <- df.temp[df.temp$cruiseNo == i,] %>% 
              leaflet() %>% 
              addTiles() %>% 
              addCircles() %>%
              #fitBounds(~min(df$lon), ~min(df$lat), ~max(df$lon), ~max(df$lat)) %>%
              #fitBounds zooms to far out, without giving you good controls to change that
              setView(lng = mean(df$lon), lat = mean(df$lat), zoom = 4) %>%
              #keep the original size of the original data, i.e. the Peru EEZ
              suspendScroll(sleepNote = FALSE, hoverToWake = FALSE) 
              #you cannot scroll anymore
  
  #%>%
              #addControl(title, position = "topleft", className="map-title")
  
              #addTitle(as.character(i), color = "black", fontSize = "20px", fontFamily = "Sans", leftPosition = 50, topPosition = 2)
    
    
  map <- list(map)                #push the map one down in order
  MapList.cruises <- append(MapList.cruises, map) #so that a map here is its own list entry
}


library(leafsync)
latticeView(MapList.cruises, ncol = 5)


rm(df.temp, map, cruiseNo, i, title, str_title, tag.map.title)

```

From the kind of graphics, it is really hard to spot, if the cruises were set correct, they still provide a lot of insight, especially about how different the cruises are spacewise. 

To make a label for every leaflet depicted in latticeview is really hard to do, since a latticeview object is an html object and you had to change the html file, rather than giving arguments to its creation.

Since there is no label for which cruise is depicted in the graphics for now, it is really hard to discern, which cruise is depicted. And thus which cruise separation might have to be changed.

Sometimes data is really missing in some areas for some cruises.

For showing that cruises should be separated, it might be helpful to find a method to find points to close together.



###   Check cruise assignment by coloring the time data

Here we take the plots, we already had in previous chapters and try to color them to show, which cruise they belong to.


```{r distr year 1983-1987 - colored, echo=FALSE}


#I'd like to plot a diagram that shows, which in which months each year
#the ship was driving around

#More interesting could be a histogram of time over each year

library(ggplot2)
library(dplyr)
library(lubridate)
library(scales)

df.temp <- subset(df, subset = year %in% c(1983,1984,1985,1986,1987))

plot <- df.temp %>% 
    mutate(date2 = ymd(date2)) %>% 
    ggplot(aes(date2,color = cruiseNo)) +
    ggtitle("Distribution of data in the years 1983-1987") +
    geom_histogram(binwidth = 1) +
    scale_x_date(labels = date_format("%b")) +
    xlab("Time of the year") +
    facet_grid(year ~., scales = "free", space = "free") 

#plot all years at once
print(plot)

#clean up
rm(plot, df.temp)


```
That looks quite optimally split timewise.

```{r distr year 1988-1992 - colored, echo=FALSE}


#I'd like to plot a diagram that shows, which in which months each year
#the ship was driving around

#More interesting could be a histogram of time over each year

library(ggplot2)
library(dplyr)
library(scales)


df.temp <- subset(df, subset = year %in% c(1988,1989,1990,1991,1992))

plot <- df.temp %>% 
    mutate(date2 = ymd(date2)) %>% 
    ggplot(aes(date2,color = cruiseNo)) +
    ggtitle("Distribution of data in the years 1988-1992") +
    geom_histogram(binwidth = 1) +
    scale_x_date(labels = date_format("%b")) +
    xlab("Time of the year") +
    facet_grid(year ~., scales = "free", space = "free") 

#plot all years at once
print(plot)

#clean up
rm(plot, df.temp)


```
14 might have to be split again.

```{r distr year 1988-1992 - split, echo=FALSE}

#add a new level
levels(df$cruiseNo) <- c(levels(df$cruiseNo), 'new_level')

for (i in 1:dim(df)[1]){
  if (df$cruiseNo[i] == "14" & df$year[i] == 1991){df$cruiseNo[i] <- "14"}
  if (df$cruiseNo[i] == "14" & df$year[i] == 1992){df$cruiseNo[i] <- "new_level"}
}

#keep 14 for the 1991 part, and attach new level for the 1992 part

rm(i)

```


```{r distr year 1993-1997 - colored, echo=FALSE}


#I'd like to plot a diagram that shows, which in which months each year
#the ship was driving around

#More interesting could be a histogram of time over each year

library(ggplot2)
library(dplyr)
library(scales)

df.temp <- subset(df, subset = year %in% c(1993,1994,1995,1996,1997))


plot <- df.temp %>% 
    mutate(date2 = ymd(date2)) %>% 
    ggplot(aes(date2,color = cruiseNo)) +
    ggtitle("Distribution of data in the years 1993-1997") +
    geom_histogram(binwidth = 1) +
    scale_x_date(labels = date_format("%b")) +
    xlab("Time of the year") +
    facet_grid(year ~., scales = "free", space = "free") 

#plot all years at once
print(plot)

#clean up
rm(plot, df.temp)


```
Cruises 20 till 23 should be probably merged. 18 and 19 also.

```{r distr year 1993-1997 - merge, echo=FALSE}

library(forcats)
df$cruiseNo <- fct_collapse(df$cruiseNo, "20" = c("20","21", "22", "23"))
#levels(df$cruiseNo) #worked
df$cruiseNo <- fct_collapse(df$cruiseNo, "18" = c("18","19"))

```

```{r distr year 1998-2002 - colored, echo=FALSE}


#I'd like to plot a diagram that shows, which in which months each year
#the ship was driving around

#More interesting could be a histogram of time over each year

library(ggplot2)
library(dplyr)

df.temp <- subset(df, subset = year %in% c(1998,1999,2000,2001,2002))


plot <- df.temp %>% 
    mutate(date2 = ymd(date2)) %>% 
    ggplot(aes(date2,color = cruiseNo)) +
    ggtitle("Distribution of data in the years 1998-2002") +
    geom_histogram(binwidth = 1) +
    scale_x_date(labels = date_format("%b")) +
    xlab("Time of the year") +
    facet_grid(year ~., scales = "free", space = "free") 

#plot all years at once
print(plot)


#clean up
rm(plot, df.temp)

```
Do 41 and 42 belong together? From the leaflet plots it looks like they should
stay separate.


```{r distr year 2003-2007 - colored, echo=FALSE}


#I'd like to plot a diagram that shows, which in which months each year
#the ship was driving around

#More interesting could be a histogram of time over each year

library(ggplot2)
library(dplyr)
library(scales)

df.temp <- subset(df, subset = year %in% c(2003,2004,2005,2006,2007))


plot <- df.temp %>% 
    mutate(date2 = ymd(date2)) %>% 
    ggplot(aes(date2,color = cruiseNo)) +
    ggtitle("Distribution of data in the years 2003-2007") +
    geom_histogram(binwidth = 1) +
    scale_x_date(labels = date_format("%b")) +
    xlab("Time of the year") +
    facet_grid(year ~., scales = "free", space = "free")

#plot all years at once
print(plot)

#clean up
rm(plot, df.temp)

```
Looks quite okay.

```{r distr year 2008-2012 - colored, echo=FALSE}


#I'd like to plot a diagram that shows, which in which months each year
#the ship was driving around

#More interesting could be a histogram of time over each year

library(ggplot2)
library(dplyr)
library(lubridate)
library(scales)

df.temp <- subset(df, subset = year %in% c(2008,2009,2010,2011,2012))


plot <- df.temp %>% 
    mutate(date2 = ymd(date2)) %>% 
    ggplot(aes(date2,color = cruiseNo)) +
    ggtitle("Distribution of data in the years 2008-2012") +
    geom_histogram(binwidth = 1) +
    scale_x_date(labels = date_format("%b")) +
    xlab("Time of the year") +
    facet_grid(year ~., scales = "free", space = "free") #Can't really do 30 rows with facet_grid

#plot all years at once
print(plot)

#clean up
rm(plot, df.temp)


```
Last picture looks perfect

At last we have to reenumerate.

```{r distr year 2008-2012 - reenumerate, echo=FALSE}

#Since we have merged and split factor levels here, we to enumerate
#the cruises again.

n  <- nlevels(df$cruiseNo)
levels(df$cruiseNo) <- 1:n
#worked

nlevels(df$cruiseNo)

rm(n)
```
So we have 65 cruises for now.

It seems to be that different splits time and spacewise might be considered. It seems to be that some cruises were done in the south and then some month later the rest in the north was taken. Timewise that should be two different cruises. But spacewise they should belong together, because there is one part north an one part south. They complement each other. 

###   Aggregate the data by cruise number

Here we try to aggregate all values within a cruise, so that we reduce the data to 65 rows.
We will just keep alls numeric variables, which have enough data.

```{r per cruise - no columns with many NA, echo=FALSE}

#colnames(df)

#c(3:6, 17:18, 30:32, 34:39) #which has to be excluded
#Omit date variables and species with few data, and boolean values.

#df.min <- df[,c(7:16, 19:29, 33, 40)] 
#40 is the CruiseNo and should stay

#str(df.min)

df.min <- subset(df,select = c('distToShelf',
                               'distToCoast',
                               'depth',
                               'seaSurfaceTemperature',
                               'seaSurfaceSalinity',
                               'chlorophyll',
                               'oxygen',
                               'depthOxycline',
                               'masas',
                               'anchovy',
                               'sardine',
                               'jackMackerel',
                               'chubMackerel',
                               'redSquatLobster',
                               'samasa',                               
                                'bag',
                               'fvo',
                               'HumboldtGiantSquid',
                               'Vinciguerria',
                               'mic',
                               'mesozooplankton',
                               'squid',
                               'cruiseNo'
                               ))



```

```{r per cruise - actual aggregation, echo=FALSE, warnings = FALSE}

mean_wo_NA = function(x){
  mean(x,na.rm = TRUE)
}


agg.cruise = aggregate(df.min,
                by = list(df.min$cruiseNo),
                FUN = mean_wo_NA)

#even though there are NAN left, I think it's correct,
#because in that case all values are NAs

rm(mean_wo_NA, df.min)

#why so many warnings????

```

```{r per cruise - clean the aggregation, echo=FALSE}

agg.cruise <- agg.cruise[,1:23] #delete the last column
colnames(agg.cruise)[1] <- "cruiseNo"
#class(agg.cruise)

#replace NAN with NA
for(i in 1:dim(agg.cruise)[2]){
  agg.cruise[,i][is.nan(agg.cruise[,i])]<-NA
}
#yeah worked

#There are actually a lot of "not available" entries in the first years of the dataset. So yes the many NAs in the aggregated version, might be justified.

```



```{r per cruise - add time information, echo=FALSE}

#maybe add the start date and last date of the cruise

library(lubridate)
agg.cruise$startCruise <- 0
agg.cruise$startCruise <- as_date(agg.cruise$startCruise)
agg.cruise$stopCruise <- 0
agg.cruise$stopCruise <- as_date(agg.cruise$stopCruise)
agg.cruise$midtimeCruise <- 0
agg.cruise$midtimeCruise <- as_date(agg.cruise$midtimeCruise)



for(i in 1:dim(agg.cruise)[1]){
  df.temp <- df[df$cruiseNo == i,] #take all rows from i-th cruise
  firstRow <- df.temp[1,]  #store first row
  agg.cruise$startCruise[i] <- firstRow$date
  lastRow <- tail(df.temp, n=1)
  agg.cruise$stopCruise[i]  <- lastRow$date
  agg.cruise$midtimeCruise[i] <- firstRow$date + (lastRow$date - firstRow$date)/2
}

rm(df.temp, firstRow, lastRow)
#worked

```

We might also want to add the geographical center of the cruise. That is the mean longitude and latitude of all geometric points of a single cruise.

```{r per cruise - add space information, echo=FALSE}

#maybe add the start date and last date of the cruise


agg.cruise$geoCenterLon <- 0
agg.cruise$geoCenterLat <- 0


for(i in 1:dim(agg.cruise)[1]){
  df.temp <- df[df$cruiseNo == i,] #take all rows from i-th cruise
  meanLon <- mean(df.temp$lon)
  meanLat <- mean(df.temp$lat)
  agg.cruise$geoCenterLon[i]  <- meanLon
  agg.cruise$geoCenterLat[i]  <- meanLat
}

rm(df.temp, meanLon, meanLat)
#worked

```

```{r per cruise - plot geocenters, echo=FALSE}

library(sp)
library(leaflet)


df.temp <- data.frame(longitude = agg.cruise$geoCenterLon, 
                       latitude = agg.cruise$geoCenterLat)

coordinates(df.temp) <- ~longitude+latitude
leaflet(df.temp) %>% addCircles() %>% addTiles()

rm(df.temp,i)

```

Most geocenters are in the middle of the EEZ, but a little closer to the coast as expected. There are also outliers. Maybe it's better the choose the partition into cruises in such a way, that the geocenters are about the same.

One should also check the equidistance in time.

```{r per cruise - equidistance in time, echo=FALSE}

plot(agg.cruise$midtimeCruise)


```
The data is not completely equidistant. Unfortunately, the newly created level was added as the last level
and not as a level in between, where it belongs.



The actual work on the aggregated dataset is done further down the line.

```{r per cruise - export the aggregation, echo=FALSE}

#write.csv(agg, "aggregatedPerCruise.csv", row.names=FALSE)
#worked, it was written in the workspace


```


##    Aggregate by year

```{r per year - no columns with many NA, echo=FALSE}

#colnames(df)

#c(3:6, 17:18, 30:32, 34:39) #which has to be excluded
#Omit date variables and species with few data, and boolean values.

#df.min <- df[,c(7:16, 19:29, 33, 40)] 
#40 is the CruiseNo and should stay

#str(df.min)

df.min <- subset(df,select = c('year',
                               'distToShelf',
                               'distToCoast',
                               'depth',
                               'seaSurfaceTemperature',
                               'seaSurfaceSalinity',
                               'chlorophyll',
                               'oxygen',
                               'depthOxycline',
                               'masas',
                               'anchovy',
                               'sardine',
                               'jackMackerel',
                               'chubMackerel',
                               'redSquatLobster',
                               'samasa',                               
                                'bag',
                               'fvo',
                               'HumboldtGiantSquid',
                               'Vinciguerria',
                               'mic',
                               'mesozooplankton',
                               'squid'
                               ))



```

```{r per year - actual aggregation, echo=FALSE, warnings = FALSE}

mean_wo_NA = function(x){
  mean(x,na.rm = TRUE)
}


agg.year = aggregate(df.min,
                by = list(df.min$year),
                FUN = mean_wo_NA)

#even though there are NAN left, I think it's correct,
#because in that case all values are NAs

rm(mean_wo_NA, df.min)

#why so many warnings????

```

```{r per cruise - clean the aggregation, echo=FALSE}


#replace NAN with NA
for(i in 1:dim(agg.year)[2]){
  agg.year[,i][is.nan(agg.year[,i])]<-NA
}
#yeah worked

#There are actually a lot of "not available" entries in the first years of the dataset. So yes the many NAs in the aggregated version, might be justified.

```


##    Try to make a new dataset with aggregated data per region

In this section we try to make a separation of the Peru exclusive economic zone
into different regions. Since administrative regions are not present in the ocean,
we will orient ourself at the concentration of data. Regions shall be smaller,
where more data is present and larger, where less data is present.

Alternatively, there are already shapefiles in the internet, where a separation
of the peruvian ocean was conducted.

###  Load shapefiles with boundaries of the Peru Ocean region

There are even maps of the whole peruvian natural zones present in the internet.


####    Shapefile of nature habitat

```{r spatial map of Peru, echo=FALSE}

#source
#https://mapcruzin.com/free-peru-country-city-place-gis-shapefiles.htm

library(sf)
boundaries.peru <- st_read("Shape-Files/natural/natural.shp")
#str(boundaries.peru)


plot(st_geometry(boundaries.peru), col = sf.colors(12, categorical = TRUE), border = 'grey', axes = TRUE)

#there seem to be coastal areas as well

```
What kind of landscape types are there and what are the relations?

```{r spatial map of Peru 2, echo=FALSE}

str(boundaries.peru$type)

library("cleaner") #for freq func
freqType <- boundaries.peru %>% cleaner::freq(type)

#Do a pie chart to determine the frequency of water, river spaces etc

ggplot(freqType, aes(x="", y=percent, fill=item))+
  geom_bar(stat="identity", color="black")+
  coord_polar("y")

rm(freqType)

```

Maybe it's best to delete all habitation types except water.

```{r spatial map of Peru 3, echo=FALSE}

#str(boundaries.peru$type)
boundaries.peru.water <- dplyr::filter(boundaries.peru, boundaries.peru$type == "water")

#5887 from 12149 objects left

plot(st_geometry(boundaries.peru.water), col = sf.colors(12, categorical = TRUE), border = 'grey', axes = TRUE)

#there seem to be still inland water areas

rm(boundaries.peru, boundaries.peru_2, boundaries.peru.water)


```
There seems to be a lot of inland water area left and the buckets are too small.
Even after you delete some of the about 6000 buckets, there are might be 3000 left.
That is still too many.

This approach might not work.


####    Shapefile by using world map data


One could also use a world map and cut it down to Peru. There are
bathymetric data present, which can tell you how the ocean floor is
shaped. Maybe you could indentify regions by looking how the Peruvian ocean
floor is shaped.


```{r spatial map of Peru 3, echo=FALSE}

#https://www.geogpsperu.com/2016/02/shapefile-de-todo-el-mundo-fisico-y.html

library(sf)
boundaries.peru_2 <- st_read("Shape-Files/10m_physical/ne_10m_bathymetry_G_4000.shp")
#str(boundaries.peru)

plot(st_geometry(boundaries.peru_2), col = sf.colors(12, categorical = TRUE), border = 'grey', axes = TRUE)

#Shape data sets of the oceans seem to be called bathymetric
#Could be beneficial to look for further

#in the dataset there is also a partition of all oceans on earth, but this is very rough

rm(boundaries.peru_2)

```
Here you can see a depiction of the ocean. Notice Africa in the middle and South
America to the left.

This approach is not pursued any further here.

For further bathymetry data see https://www.gebco.net/.

####    Shapefile by splitting a frame

Try to split the data top down 


```{r spatial map of Peru 4, echo=FALSE}

#https://www.marineregions.org/gazetteer.php?p=details&id=8432

library(sf)
boundaries.peru_3 <- st_read("Shape-Files/eez/eez.shp")
#str(boundaries.peru)

plot(st_geometry(boundaries.peru_3), col = sf.colors(12, categorical = TRUE), border = 'grey', axes = TRUE)

#this is just the shape of the exclusive economic zone (EEZ) of Peru without any partitioning

```

###  Partition the shapefile of Peruvian Exclusive Economic Zone (EEZ)

Now we want to partition the loaded silhouette of the exclusive economic zone
of Peru.


####    Partition by grid

First we want to make a simple grid over this area.

```{r spatial map of Peru 4 - check bucket size, echo=FALSE}


#further partitioning------------------------------------------------------

#check how many long and lat bins are needed
max(df$lon, na.rm = TRUE)-min(df$lon, na.rm = TRUE) #18 degrees difference for longitude
max(df$lat, na.rm = TRUE)-min(df$lat, na.rm = TRUE) #15 degrees difference for latitude
#one could simply choose quadratic buckets in 1 x 1 shape


#add grid to partition the EEZ shapefile
grid <- st_make_grid(boundaries.peru_3, cellsize = c(0.5,0.5))
plot(st_geometry(boundaries.peru_3), col = sf.colors(12, categorical = TRUE), border = 'grey', axes = TRUE)
plot(grid, add = T)

#Maybe use 0.5 by 0.5 instead, because a higher resolution at the coast might lead to better result


```
Okay, now we have a grid, but the grid should be limited to the silhouette of
the EEZ of Peru.


```{r spatial map of Peru 4 - partition EEZ shapefile, echo=FALSE}


#change data according to this link
#https://rpubs.com/huanfaChen/grid_from_polygon

library(sf)
require(rgdal)
require(raster)

# note the order of resolution #how to choose it properly?
grid <- raster(extent(boundaries.peru_3), resolution = c(0.5,0.5))
#class(grid)

# convert to SpatialPolygonsDataFrame
gridPolygon <- rasterToPolygons(grid)
#class(gridPolygon)

plot(st_geometry(boundaries.peru_3))
plot(gridPolygon, add = T)

#//raster seems to work in a similar fashion like st_make_grid
#//cellsize and resolution seem to be similar

#now clip the grid to the EZZ shapefile
#class(boundaries.peru_3)
#class(as(boundaries.peru_3, 'Spatial'))

intersectGridClipped <- raster::intersect(gridPolygon, as(boundaries.peru_3, 'Spatial'))
#//boundaries.peru_3 is a shapefile sf. I have to convert it to 

plot(intersectGridClipped)

#one can make all halved buckets full if thats wanted

boundaries.peruOcean.grid <- st_as_sf(intersectGridClipped)
plot(st_geometry(boundaries.peruOcean.grid))
#probably shapefile sf more useful than spatial polynom (sp)


rm(grid, gridPolygon, intersectGridClipped)
#boundaries.peru_3 is used later.

```
Now we have clipped the grid to the EEZ silhouette.



####   Another partition by grid


```{r generate spatial map by hexagon, echo=FALSE}

#https://urbandatapalette.com/post/2021-08-tessellation-sf/
#This seems to measure density/intensity, but you could use it anyway.

library(sf)
library(dplyr)
library(mapview)
library(tmap)


#https://epsg.io/?q=Peru%20deprecated%3A0%20%20kind%3APROJCRS
#According to this source, the crs of west Peru is: 1201

test  <- st_as_sf(df, coords = c("lon", "lat"), crs = 1201, remove = FALSE)


area_fishnet_grid = st_make_grid(test, c(1, 1), what = "polygons", square = TRUE)

# To sf and add grid ID
fishnet_grid_sf = st_sf(area_fishnet_grid) %>%
  # add grid ID
  mutate(grid_id = 1:length(lengths(area_fishnet_grid)))

#------------------------------

# count number of points in each grid
# https://gis.stackexchange.com/questions/323698/counting-points-in-polygons-with-sf-package-of-r
fishnet_grid_sf$n_colli = lengths(st_intersects(fishnet_grid_sf, test))

# remove grid without value of 0 (i.e. no points in side that grid)
fishnet_count = filter(fishnet_grid_sf, n_colli > 0)


#---------------------------------

tmap_mode("view")

map_fishnet = tm_shape(fishnet_count) +
  tm_fill(
    col = "n_colli",
    palette = "Reds",
    style = "cont",
    title = "Number of data points",
    id = "grid_id",
    showNA = FALSE,
    alpha = 0.5,
    popup.vars = c(
      "Number of collisions: " = "n_colli"
    ),
    popup.format = list(
      n_colli = list(format = "f", digits = 0)
    )
  ) +
  tm_borders(col = "grey40", lwd = 0.7)

map_fishnet





```


Huge advantage to the method above is that the squares are not cut off and squares
are just set, if there is data at all. 

This might be a wrong projection of the data since there are squares on land. 
I think, just the square in Bolivia seems to be really off, the rest of the
squares has some water in it.

####    Partition by hexagon

We use the same approach as before, instead hexagons are used.

```{r generate spatial map by hexagon, echo=FALSE}

area_honeycomb_grid = st_make_grid(test, c(1, 1), what = "polygons", square = FALSE)

# To sf and add grid ID
honeycomb_grid_sf = st_sf(area_honeycomb_grid) %>%
  # add grid ID
  mutate(grid_id = 1:length(lengths(area_honeycomb_grid)))

# count number of points in each grid
# https://gis.stackexchange.com/questions/323698/counting-points-in-polygons-with-sf-package-of-r
honeycomb_grid_sf$n_colli = lengths(st_intersects(honeycomb_grid_sf, test))

# remove grid without value of 0 (i.e. no points in side that grid)
honeycomb_count = filter(honeycomb_grid_sf, n_colli > 0)


#-----------------------------------------------


tmap_mode("view")

map_honeycomb = tm_shape(honeycomb_count) +
  tm_fill(
    col = "n_colli",
    palette = "Reds",
    style = "cont",
    title = "Number of data points",
    id = "grid_id",
    showNA = FALSE,
    alpha = 0.6,
    popup.vars = c(
      "Number of collisions: " = "n_colli"
    ),
    popup.format = list(
      n_colli = list(format = "f", digits = 0)
    )
  ) +
  tm_borders(col = "grey40", lwd = 0.7)

map_honeycomb





```

Somehow this looks much nicer. 

What can be seen is, that there is more data in the second row from the coast, 
since in the hexagons closer to the coast, there is just a part of the hexagon 
in the water.


```{r generate spatial map - remove, echo=FALSE}


rm(area_fishnet_grid, area_honeycomb_grid, fishnet_count, fishnet_grid_sf, honeycomb_count, honeycomb_grid_sf, mapview_test_points, test, map_fishnet, map_honeycomb)

```


####    Partition by using Voronoi diagram

The approach here is to use a clustering approach to set the centers for a Voronoi
diagram. We choose  the centers of longitude and latitude to be the centers
of the Voronoi diagram, because there is more data to the coast and more centers
to the coast will lead to a Voronoi diagram, which reflect that.

One could also draw a Voronoi diagram using centers, which reflect not only
longitude and latitude, but also time.



```{r generate spatial map by Voronoi - use k-means,warning=FALSE, echo=FALSE}

#use k means to get some impression of possible Voronoi centers

#https://www.guru99.com/r-k-means-clustering.html

df.min <- df[,c(7:16, 19:29, 33, 40)] 


kmean_withinss <- function(k) {
    cluster <- kmeans(df.temp, k)
    return (cluster$tot.withinss)
}

#algorithm doesnt work with NAs, so replace NAs with 0 
#df.temp <- df.scale[,c(1:2, 7:16, 19:29, 33)] 
#df.temp[is.na(df.temp)] <- 0

#try just with coordinates
df.temp <- df[,1:2]

# Set maximum cluster 
max_k <-20 
# Run algorithm over a range of k 
wss <- sapply(2:max_k, kmean_withinss)

#Warnung: keine Konvergenz nach 10 Schritten
#Warnung: Anzahl Schritte in der Quick-TRANSfer Stufe Ã¼berschreitet Maximum (= 15362150)


# Create a data frame to plot the graph
elbow <-data.frame(2:max_k, wss)

# Plot the graph with gglop
library(ggplot2)
ggplot(elbow, aes(x = X2.max_k, y = wss)) +
    geom_point() +
    geom_line() +
    scale_x_continuous(breaks = seq(1, 20, by = 1))


rm(max_k, wss, elbow, df.min, df.temp, kmean_withinss)

```

This the k means clustering elbow curve for all relevant data.This is just done 
just for test purposes, because we actually want to split the data by space.



```{r generate spatial map by Voronoi - use k-means 2,warning=FALSE, echo=FALSE}



k = 15

#try just with coordinates
kmeans_test <- df[,1:2] %>%
                kmeans(k)

# plot the clusters

library(FactoMineR)
library(factoextra)

fviz_cluster(kmeans_test, data = df.temp, geom = c("point"),ellipse.type = "euclid")
#with more than 15 clusters in space, you cannot really decipher the plot anymore


rm(kmeans_test, k) #df.temp is needed in the next chunk.

```
Result of k means just with latitude and longitude data. Plots with larger k not 
really reasonable to display here. Nevertheless, more than 15 regions are
needed.

We choose to extract geocenters with 40, 80, 120 and 200 clusters.
The amount of clusters are chosen arbitrary. For calculations it could be
better to choose a low number, since a high region number might lead to
the creation of missing values, especially if we span the data also in time.
For visualization it might be better to have a higher amount of regions,
so you can see in more detail, where marine animal species are located.

```{r generate spatial map by Voronoi - use k-means,warning=FALSE, echo=FALSE}

#use k means to get some impression of possible Voronoi centers

#since k-means sets the statrting clusters randomly, we should choose
#a seed to make the results reproducable.
set.seed(123)



#store the centers for the voronoi tesselation

kmeans_40  <- kmeans(df[,1:2], 40)
kmeans_80  <- kmeans(df[,1:2], 80)
kmeans_120 <- kmeans(df[,1:2], 120) 
kmeans_200 <- kmeans(df[,1:2], 200)


geo.centers.voronoi.k40  <- kmeans_40$centers
geo.centers.voronoi.k80  <- kmeans_80$centers
geo.centers.voronoi.k120 <- kmeans_120$centers
geo.centers.voronoi.k200 <- kmeans_200$centers

geo.centers.voronoi.k40  <- as.data.frame(geo.centers.voronoi.k40)
geo.centers.voronoi.k80  <- as.data.frame(geo.centers.voronoi.k80)
geo.centers.voronoi.k120 <- as.data.frame(geo.centers.voronoi.k120)
geo.centers.voronoi.k200 <- as.data.frame(geo.centers.voronoi.k200)

#geo.centers.voronoi
#str(geo.centers.voronoi)
#looks like num with attributes

rm(kmeans_40, kmeans_80, kmeans_120, kmeans_200)

```



```{r generate spatial map by Voronoi - function, echo=FALSE}

#https://gis.stackexchange.com/questions/220347/partition-an-area-into-polygons-so-that-each-polygon-will-contain-at-least-one-p

#There is a guideline in words
# and hint to package {deldir}

#https://gis.stackexchange.com/questions/293659/match-spatialpoints-to-delaunay-triangles-in-r
#might work

#https://stackoverflow.com/questions/12156475/combine-voronoi-polygons-and-maps
#there is a good example code, but threat is 10 years old

voronoipolygons <- function(x,poly) {
  require(deldir)
  if (.hasSlot(x, 'coords')) {
    crds <- x@coords  
  } else crds <- x
  bb = bbox(poly)
  rw = as.numeric(t(bb))
  z <- deldir(crds[,1], crds[,2],rw=rw)
  w <- tile.list(z)
  polys <- vector(mode='list', length=length(w))
  require(sp)
  for (i in seq(along=polys)) {
    pcrds <- cbind(w[[i]]$x, w[[i]]$y)
    pcrds <- rbind(pcrds, pcrds[1,])
    polys[[i]] <- Polygons(list(Polygon(pcrds)), ID=as.character(i))
  }
  SP <- SpatialPolygons(polys)
  
  index <- points_outside_polynom(crds, poly)
  crds <- crds[-index,]

  voronoi <- SpatialPolygonsDataFrame(SP, data=data.frame(x=crds[,1], y=crds[,2], row.names=sapply(slot(SP, 'polygons'), function(x) slot(x, 'ID'))))

  return(voronoi)

}


points_outside_polynom <- function(crds, poly){
  
  library(sf)
  points <- as.data.frame(crds[,1])
  points$'crds[,2]' <- crds[,2]
  colnames(points) <- c("lon", "lat")
  points <- SpatialPointsDataFrame(coords = points[,1:2], data = points)
  
  points <- st_as_sf(points)
  
  st_crs(points) #has no crs
  points <- st_set_crs(points, 5378) 
  
  poly <- st_as_sf(poly)
  poly <- st_set_crs(poly, 5378) 

  #check if both are sf
  str(points)
  str(poly)
  
  isInside <- st_within(points, poly) #92 is not within the poly
  #1 if within the polynom, 0 if outside
  #find position of the entry outside of the polynom
  
  #the problem is that the entry, which is not inside poly has no entry, instead of a 0
  isInside.logi <- as.logical(isInside) #converts empty entry into NA, very helpful
  index <- which(is.na(isInside.logi))
  #finds correct index
  
  
  #we must assume that the deldir function has just eliminated the point 92
  #all following polynoms in deldir shuold be transported one step to the front
  
  return(index)

  
}




#---------------------------------------------

#test for the deldir function

#library(deldir)
#test <- deldir(geo.centers.voronoi)
#plot(test)

#Doesn't look pretty, but it works

#---------------------------------------------



```


```{r generate spatial map by Voronoi - debugging the function, echo=FALSE, include = FALSE, eval = FALSE}

#k120 and k200 do not run through at the moment
#so try to debug it here

x <- geo.centers.voronoi.k120


  require(deldir)
  if (.hasSlot(x, 'coords')) {
    crds <- x@coords  
  } else crds <- x
  #works, coordinated now in crds

  dim(unique(crds))[1] #120 unique entries

  bb = bbox(poly) #creates coordinates of a rectangle, bottom left to upper right, as a bounding box object
  rw = as.numeric(t(bb))
  z <- deldir(crds[,1], crds[,2],rw=rw)
  #this creates the Voronoi diagram
  #this uses a bounding box rw, which spans a rectangle around the
  #cluster centers. This is not the silhouette of the Peruvian EEZ.
  
  #In the documentation it says that points outside of the bounding box are discarded.
  #Maybe we could make the bounding box a little bit larger.
  
  #As we have seen above, some of the points may lie outside of the EEZ.
  #So if a cluster is created for this area, it will never lie within
  #the EEZ and shall be discarded.In this case, just create 121 clusters,
  #and after discarding one excess, we have 120 regions.
  
  w <- tile.list(z)
  #extract information of the Voronoi tesselation
  #just 119 entries created, instead of 120.
  #one would expect that as a result of cutting the voronoi rectangle.
  #this is not even done here.

  
  polys <- vector(mode='list', length=length(w)) #stores the polynoms created by the Voronoi tesselation
  require(sp)
  for (i in seq(along=polys)) {
    pcrds <- cbind(w[[i]]$x, w[[i]]$y)
    pcrds <- rbind(pcrds, pcrds[1,])
    polys[[i]] <- Polygons(list(Polygon(pcrds)), ID=as.character(i))
  }
  SP <- SpatialPolygons(polys)
  
  
  #-----------------------------------------
  
  #Which of the 120 coords was not used
  #check if all 120 coords lie within the bounding box
  
  #both objects must be a sf object or similar
  
  library(sf)
  points <- as.data.frame(crds[,1])
  points$'crds[,2]' <- crds[,2]
  colnames(points) <- c("lon", "lat")
  points <- SpatialPointsDataFrame(coords = points[,1:2], data = points)
  
  points <- st_as_sf(points)
  
  st_crs(points) #has no crs
  points <- st_set_crs(points, 5378) 
  
  poly <- st_as_sf(poly)
  poly <- st_set_crs(poly, 5378) 

  #check if both are sf
  str(points)
  str(poly)
  
  isInside <- st_within(points, poly) #92 is not within the poly
  #1 if within the polynom, 0 if outside
  #find position of the entry outside of the polynom
  
  #the problem is that the entry, which is not inside poly has no entry, instead of a 0
  isInside.logi <- as.logical(isInside) #converts empty entry into NA, very helpful
  index <- which(is.na(isInside.logi))
  #finds correct index
  
  
  #we must assume that the deldir function has just eliminated the point 92
  #all following polynoms in deldir shuold be transported one step to the front
  
  points <- points[-index,]
  crds <- crds[-index,]


  #---------------------------------------
  
  voronoi <- SpatialPolygonsDataFrame(SP, data=data.frame(x=crds[,1], y=crds[,2], row.names=sapply(slot(SP, 'polygons'), function(x) slot(x, 'ID'))))


  #---------------------------------------
  
  #not relevant anymore
  
  # create a sf bounding box
  #library(pgirmess)
  ##var in bbox2sf n,s,w,e
  #bb.sf <- pgirmess::bbox2sf(rw[1],rw[2],rw[3],rw[4],bbox=NA,crs=5378)
  
  
  #try with rgeos
  #points <- as(points, "Spatial")
  #bb.sf <- as(bb.sf, "Spatial")
  
  #library(rgeos)
  #rgeos::overGeomGeom(bb.sf, points)
  
  
  #crs seems to be either 1201 or 5378
  
  
  rm(bb,bb.sf,crds,isInside,pcrds,points,polys,SP,voronoi,x, w,z,i,index,isInside.logi,rw)
  
  
```




```{r generate spatial map by Voronoi - create poly, echo=FALSE}


#test <- unique(geo.centers.voronoi.k80) # all are unique
#rm(test)

library(sf)
poly <-  sf:::as_Spatial(boundaries.peru_3$geom)
# we lose information with this, but we just need the geom information
voronoi.poly.k40 <- voronoipolygons(geo.centers.voronoi.k40, poly)
voronoi.poly.k80 <- voronoipolygons(geo.centers.voronoi.k80, poly)
voronoi.poly.k120 <- voronoipolygons(geo.centers.voronoi.k120, poly)
voronoi.poly.k200 <- voronoipolygons(geo.centers.voronoi.k200, poly)

#this does not seem to work for larger values, e.g. k=100.
#Values lower than k = 100 might also cause problems, depending on the seed
#for the k-means algorithm above.

#the geo.centers.voronoi is okay,. but the polygon is not okay
str(boundaries.peru_3)



library(rgeos)
tess.peruOcean.voronoi.k40 = gIntersection(poly,voronoi.poly.k40,byid=TRUE)

tess.peruOcean.voronoi.k80 = gIntersection(poly,voronoi.poly.k80,byid=TRUE)

tess.peruOcean.voronoi.k120 = gIntersection(poly,voronoi.poly.k120,byid=TRUE)

tess.peruOcean.voronoi.k200 = gIntersection(poly,voronoi.poly.k200,byid=TRUE)

#-------------------------------------------------
#plot the different tesselations in one graphic

layout(matrix(c(1,2,3,4), nrow = 2, ncol = 2, byrow = TRUE),   widths=c(1,1), heights=c(25,25))
plot(tess.peruOcean.voronoi.k40, main = 40)
plot(tess.peruOcean.voronoi.k80, main = 80)
plot(tess.peruOcean.voronoi.k120, main = 120)
plot(tess.peruOcean.voronoi.k200, main = 200)

#total plot not helpful, but single plot can also be inspected

#--------------------------------------------------

rm(voronoi.poly.k40, voronoi.poly.k80, voronoi.poly.k120, voronoi.poly.k200)
rm(geo.centers.voronoi.k40, geo.centers.voronoi.k80, geo.centers.voronoi.k120, geo.centers.voronoi.k200)
rm(poly)

```


Here we can check, how many clusters might be justified. I think 40 looks better than 30. So maybe 40 might be already a good idea.

With 80 there seem to be a lot more clusters at the coast, but the cluster size further out in the ocean stays the same.




```{r generate spatial map by quadtree, echo=FALSE}


#https://gis.stackexchange.com/questions/31236/how-can-i-generate-irregular-grid-containing-minimum-n-points

#this quadtree methods looks also cool

quadtree <- function(xy, k=1) {
  d = dim(xy)[2]
  quad <- function(xy, i, id=1) {
    if (length(xy) < 2*k*d) {
      rv = list(id=id, value=xy)
      class(rv) <- "quadtree.leaf"
    }
    else {
      q0 <- (1 + runif(1,min=-1/2,max=1/2)/dim(xy)[1])/2 # Random quantile near the median
      x0 <- quantile(xy[,i], q0)
      j <- i %% d + 1 # (Works for octrees, too...)
      rv <- list(index=i, threshold=x0, 
                 lower=quad(xy[xy[,i] <= x0, ], j, id*2), 
                 upper=quad(xy[xy[,i] > x0, ], j, id*2+1))
      class(rv) <- "quadtree"
    }
    return(rv)
  }
  quad(xy, 1)
}

test <- quadtree(df, k=1)

#how to apply that to a shapefile object?

#--------------------------------------------

points.quadtree <- function(q, ...) {
  points(q$lower, ...); points(q$upper, ...)
}
points.quadtree.leaf <- function(q, ...) {
  points(q$value, col=hsv(q$id), ...)
}

#substituted q$id by q$index
#there is not always a id created. Why?

#plot.new()
#points.quadtree(test)
#doesn't work


#--------------------------------------------

lines.quadtree <- function(q, xylim, ...) {
  i <- q$index
  j <- 3 - q$index
  clip <- function(xylim.clip, i, upper) {
    if (upper) xylim.clip[1, i] <- max(q$threshold, xylim.clip[1,i]) else 
      xylim.clip[2,i] <- min(q$threshold, xylim.clip[2,i])
    xylim.clip
  } 
  if(q$threshold > xylim[1,i]) lines(q$lower, clip(xylim, i, FALSE), ...)
  if(q$threshold < xylim[2,i]) lines(q$upper, clip(xylim, i, TRUE), ...)
  xlim <- xylim[, j]
  xy <- cbind(c(q$threshold, q$threshold), xlim)
  lines(xy[, order(i:j)],  ...)
}
lines.quadtree.leaf <- function(q, xylim, ...) {} # Nothing to do at leaves!

rm(test)

```


First we plot a quadtree based on artificial data

```{r generate spatial map by quadtree - test, echo=FALSE}


#https://gis.stackexchange.com/questions/31236/how-can-i-generate-irregular-grid-containing-minimum-n-points

#go through the example

#generate the artificial data-----------------------------
n <- 25000       # Points per cluster
n.centers <- 40  # Number of cluster centers
sd <- 1/2        # Standard deviation of each cluster
set.seed(17)
centers <- matrix(runif(n.centers*2, min=c(-90, 30), max=c(-75, 40)), ncol=2, byrow=TRUE)
xy <- matrix(apply(centers, 1, function(x) rnorm(n*2, mean=x, sd=sd)), ncol=2, byrow=TRUE)
#plot(xy)

#test the function-------------------------------------

k <- 5
system.time(qt <- quadtree(xy, k))
#
# Set up to map the full extent of the quadtree.
#
xylim <- cbind(x=c(min(xy[,1]), max(xy[,1])), y=c(min(xy[,2]), max(xy[,2])))


#plot the quadtree--------------------------------------------

#we have defined the functions above, these are not the standard functions

plot(xylim, type="n", xlab="x", ylab="y", main="Quadtree")
#
# This is all the code needed for the plot!
#
lines(qt, xylim, col="Gray")
#points(qt, pch = '.')

#mostly worked
#unfortunately points doesn't accept another col argument, 
#but on the other hand it needs one
#A colour argument was assigned in the function above by using hsv




rm(n, n.centers, sd, centers, xy, xylim, k)

```

Now we plot a quadtree of a subsample of our true data.

```{r generate spatial map by quadtree - Peru Ocean data, echo=FALSE}


#https://gis.stackexchange.com/questions/31236/how-can-i-generate-irregular-grid-containing-minimum-n-points

#apply it to our data

k <- 5
#probably quadtree() expects a matrix, because it uses length
#n

xy.index <- sample(1:dim(df)[1], size = 10000)
xy  <- df[xy.index,1:2]

system.time(qt <- quadtree(as.matrix(xy), k))
#does the function relate the center coordinates to every point in df?
#Error: C stack usage  15927056 is too close to the limit


# Set up to map the full extent of the quadtree.
xylim <- cbind(x=c(min(xy[,1]), max(xy[,1])), y=c(min(xy[,2]), max(xy[,2])))



#plot the quadtree--------------------------------

plot(xylim, type="n", xlab="x", ylab="y", main="Quadtree")
#
# This is all the code needed for the plot!
#
lines(qt, xylim, col="Gray")
#points(qt, pch=".")

rm(k, xy.index, qt, xylim)

```

Looks quite nice, but maybe the partition is too fine grained. On the positive 
side, the size of the polynoms varies more than is the case in  the Voronoi 
tesselation. That might be a hint that the quadtree method can fit the data better.



```{r generate spatial map by quadtree - appl, echo=FALSE}

#https://gis.stackexchange.com/questions/31236/how-can-i-generate-irregular-grid-containing-minimum-n-points

#transform into spatial object function

cell <- function(q, xylim, ...) {
  if (class(q)=="quadtree") f <- cell.quadtree else f <- cell.quadtree.leaf
  f(q, xylim, ...)
}
cell.quadtree <- function(q, xylim, ...) {
  i <- q$index
  j <- 3 - q$index
  clip <- function(xylim.clip, i, upper) {
    if (upper) xylim.clip[1, i] <- max(q$threshold, xylim.clip[1,i]) else 
      xylim.clip[2,i] <- min(q$threshold, xylim.clip[2,i])
    xylim.clip
  } 
  d <- data.frame(id=NULL, x=NULL, y=NULL)
  if(q$threshold > xylim[1,i]) d <- cell(q$lower, clip(xylim, i, FALSE), ...)
  if(q$threshold < xylim[2,i]) d <- rbind(d, cell(q$upper, clip(xylim, i, TRUE), ...))
  d
}
cell.quadtree.leaf <- function(q, xylim) {
  data.frame(id = q$id, 
             x = c(xylim[1,1], xylim[2,1], xylim[2,1], xylim[1,1], xylim[1,1]),
             y = c(xylim[1,2], xylim[1,2], xylim[2,2], xylim[2,2], xylim[1,2]))
}

```

```{r generate spatial map by quadtree - appl 2, echo=FALSE}

#https://gis.stackexchange.com/questions/31236/how-can-i-generate-irregular-grid-containing-minimum-n-points

#transform into spatial object

qt <- quadtree(as.matrix(xy), k)
xylim <- cbind(x=c(min(xy[,1]), max(xy[,1])), y=c(min(xy[,2]), max(xy[,2])))


polys <- cell(qt, xylim)
polys.attr <- data.frame(id=unique(polys$id))


library(shapefiles)
polys.shapefile <- convert.to.shapefile(polys, polys.attr, "id", 5)
write.shapefile(polys.shapefile, "G:/Benny/Lernkurse ohne Studium/Open Campus SH/SS 2023/Time Series/Project/quadtree_shapefile", arcgis=TRUE)

class(polys.shapefile)

library(rgdal)
shape <- rgdal::readOGR(dsn = ".", layer = "quadtree_shapefile")


#Create an intersection of Peru boundaries with tesselation-------------------

poly <-  sf:::as_Spatial(boundaries.peru_3$geom)

tess.peruOcean.quadtree.k5 = gIntersection(poly,shape,byid=TRUE)



plot(tess.peruOcean.quadtree.k5)

rm(shape, poly, polys.shapefile, polys, polys.attr, qt, xylim, k, xy)

```
Very irregular shape where there is few data, that is in the West of the Peru Ocean
zone. In contrast to the Voronoi approach, there are not 40 till 200 polynoms 
created, but 1800. Also the amount of polynoms cannot be regulated directly.
So, all in all, the Voronoi method seems to be superior.


###    Create a column with the area number              

We want to add a column containing the number of the region.

```{r spatial add area column - test, echo=FALSE, include = FALSE, eval = FALSE}

#this is just a test section

#https://gis.stackexchange.com/questions/282750/identify-polygon-containing-point-with-r-sf-package
#last entry in the top answer

pnts <- df[,1:2]
tt.k40 <- st_as_sf(tess.peruOcean.voronoi.k40)
class(tt.k40)


# create a points collection
pnts_sf <- do.call("st_sfc",c(lapply(1:nrow(pnts), function(i) {st_point(as.numeric(pnts[i, ]))}), list("crs" = 4326))) 


#possible projections
#sf_proj_info

pnts_trans <- st_transform(pnts_sf, 2163)       # apply transformation to pnts sf
tt_trans.k40 <- st_transform(tt.k40, 2163)      # apply transformation to polygons sf
  


#name the areas
tt_trans.k40$NAME_1 <- 1:dim(tt_trans.k40)[1]


# intersect and extract state name
pnts$region.k40 <- apply(st_intersects(tt_trans.k40, pnts_trans, sparse = FALSE), 2, 
               function(col) { 
                  tt_trans.k40[which(col), ]$NAME_1
               })



#the result is a list, not a vector
str(pnts[,3])
str(pnts[,4])

#---------------------------------

#add the region number to the original dataframe
    #this can't be done at the beginning, because the tesselation was necessary

df$region.k40 <- pnts$region.k40


#--------------------------------

#region must be unlisted before adding it to the df
#problem is that unlisting creates a shorter vector than the rows of the dataframe

df$region.k40[sapply(df$region.k40, is.null)] <- NA


#sum(is.na(df$region))
#sum(is.null(df$region))

#df$region.k40 <- unlist(df$region.k40)
#df$region.k80 <- unlist(df$region.k80)

#Unlist seems to output a different dimension(?) than the original vector

summary(df$region.k40)

#--------------------------------

library(tidyr)
df.min <- unnest(df, c(region.k40, region.k80))
#class(df.min$region.k40)




dim(df.min)
#if you use unnest, then you have just 306.995 datarows.
#In the original you had 307.243. Why is that?

#That's why the column is stored as df.min here and not df. 

str(df.min)

#------------------------------
#troubleshooting section

#first time, the function ran through, but no column region added to
#pnts. Problem seems to be that there are no names for the regions.


#Are there rows without an region.k80 entry?
#sum(is.na(df.min$region.k80))
#sum(is.null(df.min$region.k80))
#Doesn't seem like it, maybe it is a display error.

```

the unnest or unlist function removes the amount of data rows.
Why is that? Is something major wrong?


```{r spatial add area column - function, echo=FALSE}

#https://gis.stackexchange.com/questions/282750/identify-polygon-containing-point-with-r-sf-package
#last entry in the top answer

#we enter a dataframe into the function
#the result shall be the same dataframe, but with a column added with region id

addRegionID <- function(df, tess, region_name){
  #input is the dataframe, the tesselation and the column name for region IDs
  
  pnts <- df[,1:2]
  tess <- st_as_sf(tess)
  
  # create a points collection
  pnts_sf <- do.call("st_sfc",c(lapply(1:nrow(pnts), function(i) {st_point(as.numeric(pnts[i, ]))}), list("crs" = 4326))) 
  
  #possible projections
  #sf_proj_info
  
  #change crs projection
  pnts_trans <- st_transform(pnts_sf, 2163)       # apply transformation to pnts sf
  tess       <- st_transform(tess, 2163)      # apply transformation to polygons sf
  
  #name the areas
  tess$NAME_1 <- 1:dim(tess)[1]
  
  # intersect and extract state name
  pnts$region <- apply(st_intersects(tess, pnts_trans, sparse = FALSE), 2, 
               function(col) { 
                  tess[which(col), ]$NAME_1
               })
  
  #add the region number to the original dataframe
    #this can't be done at the beginning, because the tesselation was necessary

  df$region_name <- pnts$region 
  #if you o this one by one, the region will be named region_name

  #region must be unlisted before adding it to the df
  #problem is that unlisting creates a shorter vector than the rows of the dataframe

  df$region_name[sapply(df$region_name, is.null)] <- NA
  
  library(tidyr)
  df.min <- unnest(df, region_name)
  #unnest reduces the amount of datarows
  
  
  return(df.min)
  
}

#this is still very inefficient, since you probably don't need to

```


```{r spatial add area column - create dataframe for aggregation, echo=FALSE}

#https://gis.stackexchange.com/questions/282750/identify-polygon-containing-point-with-r-sf-package
#last entry in the top answer

#All region information shall be added to the same dataframe

df.region.k40 <- addRegionID(df, tess.peruOcean.voronoi.k40, region.Voro.k40)
df.region.k80 <- addRegionID(df, tess.peruOcean.voronoi.k80, region.Voro.k80)
df.region.k119 <- addRegionID(df, tess.peruOcean.voronoi.k120, region.Voro.k120)
df.region.k199 <- addRegionID(df, tess.peruOcean.voronoi.k120, region.Voro.k200)
df.region.quad5 <- addRegionID(df, tess.peruOcean.quadtree.k5, region.quad.k5)

#unlist leads always to the same decrease in data rows, which is quite handy
#that is not the case for the quadtree method

#Since the unnest function creates dataframes of different length, we do not
#unify the region indeces to one dataframe here

```



###    aggregate by region

```{r spatial - aggregate by region, echo=FALSE}

#maybe the resulting aggregation have to be added to tess.PeruOcean.voronoi
#and then converted into a spatial dataframe?

#c(3:6, 17:18, 30:32, 34:39) #which has to be excluded
#column 41 is the region and should stay

df.region.k40.min <- df.region.k40[,c(7:16, 19:29, 33, 41)] 
df.region.k80.min <- df.region.k80[,c(7:16, 19:29, 33, 41)] 
df.region.k119.min <- df.region.k119[,c(7:16, 19:29, 33, 41)] 
df.region.k199.min <- df.region.k199[,c(7:16, 19:29, 33, 41)] 
df.region.quad5.min <- df.region.quad5[,c(7:16, 19:29, 33, 41)] 



```

```{r spatial - aggregate by region - actual aggregation, echo=FALSE, warnings = FALSE}

mean_wo_NA = function(x){
  mean(x,na.rm = TRUE)
}


agg.region.k40 = aggregate(df.region.k40.min,
                by = list(df.region.k40.min$region_name),
                FUN = mean_wo_NA)

agg.region.k80 = aggregate(df.region.k80.min,
                by = list(df.region.k80.min$region_name),
                FUN = mean_wo_NA)

agg.region.k119 = aggregate(df.region.k119.min,
                by = list(df.region.k119.min$region_name),
                FUN = mean_wo_NA)

agg.region.k199 = aggregate(df.region.k199.min,
                by = list(df.region.k199.min$region_name),
                FUN = mean_wo_NA)

agg.region.quad5 = aggregate(df.region.quad5.min,
                  by = list(df.region.quad5.min$region_name),
                  FUN = mean_wo_NA)

#even though there are NAN left, I think it's correct,
#because in that case all values are NAs

rm(mean_wo_NA)

#Troubleshooting---------------------------

#class(df.min$region.k40)


```

```{r spatial - aggregate by region - clean up, echo=FALSE}

cleanAggregation <- function(agg){
  
  #delete the last entry
  agg <- agg[,1:(ncol(agg)-1)]
  
  #rename first column
  colnames(agg)[1] <- "region"

  #replace NAN with NA
  for(i in 1:dim(agg)[2]){
      agg[,i][is.nan(agg[,i])]<-NA
  }
  
  return(agg)
}



#-----------------------------------

agg.region.k40 <- cleanAggregation(agg.region.k40)
agg.region.k80 <- cleanAggregation(agg.region.k80)
agg.region.k119 <- cleanAggregation(agg.region.k119)
agg.region.k199 <- cleanAggregation(agg.region.k199)
agg.region.quad5 <- cleanAggregation(agg.region.quad5)


```

```{r spatial - aggregate by region - add to per Ocean tess, echo=FALSE}

#How to combine the tess.peruOcean.voronoi and agg.region information?
tess.peruOcean.voronoi.k40 <- st_as_sf(tess.peruOcean.voronoi.k40)
tess.peruOcean.voronoi.k40$region <- 1:dim(tess.peruOcean.voronoi.k40)[1]

agg.region.sf.k40 <- merge(tess.peruOcean.voronoi.k40, agg.region.k40, by.x="region", by.y="region")


#---------------------------------------------

tess.peruOcean.voronoi.k80 <- st_as_sf(tess.peruOcean.voronoi.k80)
tess.peruOcean.voronoi.k80$region <- 1:dim(tess.peruOcean.voronoi.k80)[1]

agg.region.sf.k80 <- merge(tess.peruOcean.voronoi.k80, agg.region.k80, by.x="region", by.y="region")

#----------------------------------------------

tess.peruOcean.voronoi.k120 <- st_as_sf(tess.peruOcean.voronoi.k120)
tess.peruOcean.voronoi.k120$region <- 1:dim(tess.peruOcean.voronoi.k120)[1]

agg.region.sf.k119 <- merge(tess.peruOcean.voronoi.k120, agg.region.k119, by.x="region", by.y="region")

#------------------------------------------------

tess.peruOcean.voronoi.k200 <- st_as_sf(tess.peruOcean.voronoi.k200)
tess.peruOcean.voronoi.k200$region <- 1:dim(tess.peruOcean.voronoi.k200)[1]

agg.region.sf.k199 <- merge(tess.peruOcean.voronoi.k200, agg.region.k199, by.x="region", by.y="region")

#-----------------------------------------------

tess.peruOcean.quadtree.k5 <- st_as_sf(tess.peruOcean.quadtree.k5)
tess.peruOcean.quadtree.k5$region <- 1:dim(tess.peruOcean.quadtree.k5)[1]

agg.region.sf.quad5 <- merge(tess.peruOcean.quadtree.k5, agg.region.quad5, by.x="region", by.y="region")


```

####    Quick Chloropeth map

Could you do a chloropeth map here? Try different ways to plot a chloropeth map
This chunk is supposed for playing around. It is not to be executed as a chunk.

```{r spatial - aggregate by region - chloropeth, echo=FALSE}

#simple plot

plot(agg.region.sf.quad5["redSquatLobster"])

#-------------------------------------- 

#plot with deliberately choosing breaks and colours

library(RColorBrewer)
pal <- brewer.pal(7, "OrRd") # we select 7 colors from the palette
class(pal)
    
plot(agg.region.sf.k80["redSquatLobster"], 
     main = "Lobsters", 
     breaks = "quantile", nbreaks = 7,
     pal = pal)

#--------------------------------------

library(tmap)
tm_shape(agg.region.sf.k80) + tm_fill(col="redSquatLobster", style="quantile", n=8, palette="Greens") +
              tm_legend(outside=TRUE)

#-------------------------------------

library(sp)
#agg.region.sf is an sf not sp object
spplot(as(agg.region.sf.k199, 'Spatial'), "redSquatLobster") 

#looks similar to standard plot() output
#more breaks here actually important

```
To choose a region size of 200 doesn't even seem to be that much if the marine
animals are clustered in a certain area. If there is just information in a small
area, then it helps that that area has a higher resolution.


###   aggregate by region and year

Year might be better than cruiseNo, because some cruises are rather empty.
It doesn't matter, if we choose cruiseNo or year with regard to seasonality.

Probably it's best to aggregate over the years, since cruises might be in autumn, 
spring etc and by taking the year, we eliminate seasonal effects, if ww have 
cruises throughout the year (which is not always the case).

```{r spatial - aggregate space and time - min, echo=FALSE}

df.region.k40.min <- df.region.k40[,c(3, 7:16, 19:29, 33, 41)] 
df.region.k80.min <- df.region.k80[,c(3, 7:16, 19:29, 33, 41)] 
df.region.k119.min <- df.region.k119[,c(3, 7:16, 19:29, 33, 41)] 
df.region.k199.min <- df.region.k199[,c(3, 7:16, 19:29, 33, 41)] 
df.region.quad5.min <- df.region.quad5[,c(3, 7:16, 19:29, 33, 41)] 

```



```{r spatial - aggregate space and time - aggregate, echo=FALSE}

#since we also need year, we have to create a new df.min
#keep also year, which is on the 3rd position


mean_wo_NA = function(x){
  mean(x,na.rm = TRUE)
}


agg.region.k40.year = aggregate(df.region.k40.min,
                    by = list(df.region.k40.min$region_name, df.region.k40.min$year),
                    FUN = mean_wo_NA)
#created 1.128 rows, where this should be 40 x 30 = 1.200 rows.

agg.region.k80.year = aggregate(df.region.k80.min,
                    by = list(df.region.k80.min$region_name, df.region.k80.min$year),
                    FUN = mean_wo_NA)
#created 2.164 rows, where this should be 80 x 30 = 2.400 rows.

agg.region.k119.year = aggregate(df.region.k119.min,
                   by = list(df.region.k119.min$region_name, df.region.k119.min$year),
                    FUN = mean_wo_NA)
#created 3.125 rows, where this should be 119 x 30 = 3.570 rows.


agg.region.k199.year = aggregate(df.region.k199.min,
                    by = list(df.region.k199.min$region_name, df.region.k199.min$year),
                    FUN = mean_wo_NA)
#created 5.079 rows, where this should be 199 x 30 = 5.970 rows.


agg.region.quad5.year = aggregate(df.region.quad5.min,
                      by = list(df.region.quad5.min$region_name, df.region.quad5.min$year),
                      FUN = mean_wo_NA)
#created 39.056 rows, where this should be 1.802 x 30 = 54.060 rows.


#even though there are NAN left, I think it's correct,
#because in that case all values are NAs

rm(mean_wo_NA)

```


```{r spatial - aggregate space and time - clean, echo=FALSE}

#the function is redefined here, to also handle year and region

cleanAggregation <- function(agg){
  
  #delete year and region columns
  agg <- subset(agg, select = -c(year,region_name))

  
  #rename first column
  colnames(agg)[1] <- "region"
  colnames(agg)[2] <- "year"

  #replace NAN with NA
  for(i in 1:dim(agg)[2]){
      agg[,i][is.nan(agg[,i])]<-NA
  }
  
  return(agg)
}

#-----------------------------------

agg.region.k40.year   <- cleanAggregation(agg.region.k40.year)
agg.region.k80.year   <- cleanAggregation(agg.region.k80.year)
agg.region.k119.year  <- cleanAggregation(agg.region.k119.year)
agg.region.k199.year  <- cleanAggregation(agg.region.k199.year)
agg.region.quad5.year <- cleanAggregation(agg.region.quad5.year)



```


```{r spatial - aggregate space and time - merge fail, echo=FALSE}

#------------------------------------------------------

#keep all entries from agg.region.k40.year, add geometry information

#problem is merging with duplicates here

#library(data.table)

#agg.region.sf.k40.year <- merge(x = setDT(agg.region.k40.year), y = setDT(tess.peruOcean.voronoi.k40), by = "region", all = TRUE, no.dups = FALSE)

#agg.region.sf.k80.year <- merge(x = setDT(tess.peruOcean.voronoi.k80), y = setDT(agg.region.k80.year), by="region", by.y="region", all = TRUE)

#does this work, even if the amount of rows will not be the same?

#-----------------------------------------------

#https://stackoverflow.com/questions/73571142/how-to-merge-multiple-data-frames-into-panel-data-frame
#try to trick the merge function

#lst1 <- list(agg.region.k40.year, tess.peruOcean.voronoi.k40)
#commonIds <- Reduce(intersect, lapply(lst1, `[[`, "region"))
#out <- do.call(rbind, lapply(lst1, subset, subset = region %in% commonIds))
#out <- out[order(out$region),]
#row.names(out) <- NULL

#polygon was added successfully, but just as a separate entry before each new region starts
#counting up



```


```{r spatial - aggregate space and time - merge success, echo=FALSE}


#build a simple merge myself, where the geometry is added to the right region

simpleMerge <- function(agg, tess){
  nrows = dim(agg)[1]
  agg$geometry <- NA

  noRegions <- dim(tess)[1]
  
  
  for(i in 1:nrows){
    for(j in 1:noRegions){
      if(agg$region[i] == j){agg$geometry[i] <- tess$geometry[j]}
    }
  }
  
  return(agg)

}

agg.region.k40.year   <- simpleMerge(agg.region.k40.year, tess.peruOcean.voronoi.k40)
agg.region.k80.year   <- simpleMerge(agg.region.k80.year, tess.peruOcean.voronoi.k80)
agg.region.k119.year  <- simpleMerge(agg.region.k119.year, tess.peruOcean.voronoi.k120)
agg.region.k199.year  <- simpleMerge(agg.region.k199.year, tess.peruOcean.voronoi.k200)
agg.region.quad5.year <- simpleMerge(agg.region.quad5.year, tess.peruOcean.quadtree.k5)


#-------------------------------------------------

#somehow the geometry column was converted into a list of lists instead of a multipolygon?
#still a multipolygon it seems, it just looks differently

#we can fix that
#https://stackoverflow.com/questions/56258655/convert-data-frame-containing-coordinates-of-polygons-to-sf


#Troubleshooting-----------------------------------

#class(agg.region.k40.year)
#class(tess.peruOcean.voronoi.k40)
#both dataframes

#str(agg.region.k80.year$region)
#str(tess.peruOcean.voronoi.k80$region)
#both integer

#class(tess.peruOcean.voronoi.k40$geometry)
#class(tess.peruOcean.voronoi.k80$geometry)
#both the same class

#test <- unique(agg.region.k80.year$region) #80 different integers
#print(sort(test))
#test <- tess.peruOcean.voronoi.k80$region
#print(test) #also 80 integers


```




###   create balanced datasets for yearly data


In our current datasets there were no rows created, when e.g. in a north region
there were no animals in that year. If we want balanced panels, where the
amount of rows is year times region, we have to complete  the dataset.

```{r  spatial - aggregate space and time - balance the data , echo=FALSE}

library(plm)

agg.region.k40.year.balanced    <- agg.region.k40.year %>% complete(nesting(region), year = full_seq(year, period = 1))
agg.region.k80.year.balanced    <- agg.region.k80.year %>% complete(nesting(region), year = full_seq(year, period = 1))
agg.region.k119.year.balanced   <- agg.region.k119.year %>% complete(nesting(region), year = full_seq(year, period = 1))
agg.region.k199.year.balanced   <- agg.region.k199.year %>% complete(nesting(region), year = full_seq(year, period = 1))
agg.region.quad5.year.balanced  <- agg.region.quad5.year %>% complete(nesting(region), year = full_seq(year, period = 1))
#this just adds additional rows with NAs to balance the panel data set

punbalancedness(agg.region.k40.year.balanced)
punbalancedness(agg.region.k80.year.balanced)
punbalancedness(agg.region.k119.year.balanced)
punbalancedness(agg.region.k199.year.balanced)
punbalancedness(agg.region.quad5.year.balanced)
#no unbalancedness any more


#The newly build rows have no geometry information in some rows
agg.region.k40.year.balanced   <- simpleMerge(agg.region.k40.year.balanced, tess.peruOcean.voronoi.k40)
agg.region.k80.year.balanced   <- simpleMerge(agg.region.k80.year.balanced, tess.peruOcean.voronoi.k80)
agg.region.k119.year.balanced  <- simpleMerge(agg.region.k119.year.balanced, tess.peruOcean.voronoi.k120)
agg.region.k199.year.balanced  <- simpleMerge(agg.region.k199.year.balanced, tess.peruOcean.voronoi.k200)
agg.region.quad5.year.balanced <- simpleMerge(agg.region.quad5.year.balanced, tess.peruOcean.quadtree.k5)




```
```{r  spatial - aggregate space and time - distribution NAs , echo=FALSE}

library(mice)

#Are the NAs randomly distributed or systematically
#md.pattern(agg.region.k40.year.balanced)

#----------------

library(VIM)
mice_plot <- aggr(agg.region.k40.year.balanced, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(agg.region.k40.year.balanced), cex.axis=.7,
                    gap=3, ylab=c("Missing data","Pattern"))


```

How to interprete the right graphic?

```{r  spatial - aggregate space and time - imputation , echo=FALSE}

library(mice)
set.seed(666)


#mice doesn't accept the geometry column as input
imp  <- mice(agg.region.k40.year.balanced[,1:24], maxit = 5, method = "cart", print =  FALSE)
imp2 <- mice(agg.region.k80.year.balanced[,1:24], maxit = 5, method = "cart", print =  FALSE)
imp3 <- mice(agg.region.k119.year.balanced[,1:24], maxit = 5, method = "cart", print =  FALSE)
imp4 <- mice(agg.region.k199.year.balanced[,1:24], maxit = 5, method = "cart", print =  FALSE)
#imp5 <- mice(agg.region.quad5.year.balanced[,1:24], maxit = 5, method = "cart", print =  FALSE)

#imputation with quadtree takes too long, maybe another strategy is necessary


#5 different imputations are created, you can choose one, or pool the existing imputations

agg.region.k40.year.balanced.imp   <- complete(imp,2) #choose 2nd imputation version
agg.region.k80.year.balanced.imp   <- complete(imp2,2) #choose 2nd imputation version
agg.region.k119.year.balanced.imp  <- complete(imp3,2) #choose 2nd imputation version
agg.region.k199.year.balanced.imp  <- complete(imp4,2) #choose 2nd imputation version
#agg.region.quad5.year.balanced.imp <- complete(imp5,2) #choose 2nd imputation version




sum(is.na(agg.region.k40.year.balanced.imp))
sum(is.na(agg.region.k80.year.balanced.imp))
sum(is.na(agg.region.k119.year.balanced.imp))
sum(is.na(agg.region.k199.year.balanced.imp))
#sum(is.na(agg.region.quad5.year.balanced.imp))

#no NAs left


#-----------------------------------------------
#re-add the geometry information

agg.region.k40.year.balanced.imp$geometry   <- agg.region.k40.year.balanced$geometry
agg.region.k80.year.balanced.imp$geometry   <- agg.region.k80.year.balanced$geometry
agg.region.k119.year.balanced.imp$geometry  <- agg.region.k119.year.balanced$geometry
agg.region.k199.year.balanced.imp$geometry  <- agg.region.k199.year.balanced$geometry
#agg.region.quad5.year.balanced.imp$geometry <- agg.region.quad5.year.balanced$geometry


#---------------------------------------------


agg.region.k40.year.balanced.imp  <- st_as_sf(agg.region.k40.year.balanced.imp)
agg.region.k80.year.balanced.imp  <- st_as_sf(agg.region.k80.year.balanced.imp)
agg.region.k119.year.balanced.imp <- st_as_sf(agg.region.k119.year.balanced.imp)
agg.region.k199.year.balanced.imp <- st_as_sf(agg.region.k199.year.balanced.imp)



```
How can you be sure, that the imputation is correct? an analysis is omitted here.





###     Creation of animated chlorpeth maps

Is done later one.

##    Creation of monthly data aggregation

Maybe it makes just sense to choose a low amount of regions if you go for
monthly data, since creating monthly data might create a lot of NAs.

```{r spatial - aggregate space and time - monthly - min, echo=FALSE}

df.region.k40.min <- df.region.k40[,c(3, 4, 7:16, 19:29, 33, 41)] 
df.region.k80.min <- df.region.k80[,c(3, 4, 7:16, 19:29, 33, 41)] 


```

```{r spatial - aggregate space and time - monthly - aggregate, echo=FALSE}

#since we also need year, we have to create a new df.min
#keep also year, which is on the 3rd position


mean_wo_NA = function(x){
  mean(x,na.rm = TRUE)
}


agg.region.k40.month = aggregate(df.region.k40.min,
                    by = list(df.region.k40.min$region_name, df.region.k40.min$year, df.region.k40.min$month),
                    FUN = mean_wo_NA)
#created 2.262 rows, where this should be ? rows.

agg.region.k80.month = aggregate(df.region.k80.min,
                    by = list(df.region.k80.min$region_name, df.region.k80.min$year, df.region.k80.min$month),
                    FUN = mean_wo_NA)
#created 4.068 rows, where this should be ? rows.

```

```{r spatial - aggregate space and time - clean, echo=FALSE}

#the function is redefined here, to also handle year and region

# reuse the cleanAggregation function from above


agg.region.k40.month   <- cleanAggregation(agg.region.k40.month)
agg.region.k80.month   <- cleanAggregation(agg.region.k80.month)


```



```{r spatial - aggregate space and time - month - merge success, echo=FALSE}


#build a simple merge myself, where the geometry is added to the right region

# reuse the simpleMerge function from above 

agg.region.k40.month   <- simpleMerge(agg.region.k40.month, tess.peruOcean.voronoi.k40)
agg.region.k80.month   <- simpleMerge(agg.region.k80.month, tess.peruOcean.voronoi.k80)


```


###   create balanced datasets for monthly data




```{r  spatial - aggregate space and time - month - balance the data , echo=FALSE}

library(plm)

agg.region.k40.month.balanced    <- agg.region.k40.month %>% complete(nesting(region), year = full_seq(year, period = 1))
agg.region.k80.month.balanced    <- agg.region.k80.month %>% complete(nesting(region), year = full_seq(year, period = 1))
#this just adds additional rows with NAs to balance the panel data set

punbalancedness(agg.region.k40.month.balanced) 
#still incomplete, could be since the data don't start in January and don't end in Decembre
punbalancedness(agg.region.k80.month.balanced)
#also unbalanced


#The newly build rows have no geometry information in some rows
agg.region.k40.month.balanced   <- simpleMerge(agg.region.k40.month.balanced, tess.peruOcean.voronoi.k40)
agg.region.k80.month.balanced   <- simpleMerge(agg.region.k80.month.balanced, tess.peruOcean.voronoi.k80)


```


```{r  spatial - aggregate space and time - month - distribution NAs , echo=FALSE}

library(mice)

#Are the NAs randomly distributed or systematically
#md.pattern(agg.region.k40.year.balanced)

#----------------

library(VIM)
mice_plot <- aggr(agg.region.k40.month.balanced, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(agg.region.k40.month.balanced), cex.axis=.7,
                    gap=3, ylab=c("Missing data","Pattern"))


```

How to interprete the right graphic?

```{r  spatial - aggregate space and time - month - imputation , echo=FALSE}

library(mice)
set.seed(666)


#mice doesn't accept the geometry column as input
imp6  <- mice(agg.region.k40.month.balanced[,1:24], maxit = 5, method = "cart", print =  FALSE)
imp7  <- mice(agg.region.k80.month.balanced[,1:24], maxit = 5, method = "cart", print =  FALSE)


#5 different imputations are created, you can choose one, or pool the existing imputations

agg.region.k40.month.balanced.imp   <- complete(imp6,2) #choose 2nd imputation version
agg.region.k80.month.balanced.imp   <- complete(imp7,2) #choose 2nd imputation version
#there seem to be logged events. That means ...?


sum(is.na(agg.region.k40.month.balanced.imp))
sum(is.na(agg.region.k80.month.balanced.imp))
#still NAs left. Why?


#library(tidyr)
#agg.region.k40.month.balanced.imp <- replace_na(agg.region.k40.month.balanced.imp, 0)
#agg.region.k80.month.balanced.imp <- replace_na(agg.region.k80.month.balanced.imp, 0)
#with this function you have to set the replacement by column

agg.region.k40.month.balanced.imp[is.na(agg.region.k40.month.balanced.imp)] <- 0
agg.region.k80.month.balanced.imp[is.na(agg.region.k80.month.balanced.imp)] <- 0
#worked

#-----------------------------------------------
#re-add the geometry information

agg.region.k40.month.balanced.imp$geometry   <- agg.region.k40.month.balanced$geometry
agg.region.k80.month.balanced.imp$geometry   <- agg.region.k80.month.balanced$geometry

#---------------------------------------------


agg.region.k40.month.balanced.imp  <- st_as_sf(agg.region.k40.month.balanced.imp)
agg.region.k80.month.balanced.imp  <- st_as_sf(agg.region.k80.month.balanced.imp)


```

#   analysis without considering space and time

##    check correlations

###   correlations of explaining variables

```{r correlations explaining variables, echo=FALSE}

cordata <- df[,c(7:14)]
cordata <- na.omit(cordata)

corr <- round(cor(cordata), 2)

library(ggcorrplot)

ggcorrplot(corr, hc.order = TRUE, type = "lower", lab = TRUE, lab_size = 3, method="circle", colors = c("blue", "white", "red"), outline.color = "gray", show.legend = TRUE, show.diag = FALSE, title="Correlogram of explaining variables")

rm(cordata, corr)

```

The correlogram of the explaining variables indicate multicollinearity, so one should select or adjust these explaining variables before using linear regression. Especially distance to coast, distance to shelf and depth are heavily correlated.

###   correlations between all variables

```{r correlations all variables, echo=FALSE}

#unfortunately I was not able to put all the explaining variables on one axis and all animal population variables to the other axis. I get a quadratic MatrixError

#since we have to omit NAs not all marine animals are viable here

cordata <- df[,c(7:14,19:27)]
cordata <- na.omit(cordata)

corr <- round(cor(cordata), 2)

library(ggcorrplot)

ggcorrplot(corr, type = "lower", lab = TRUE, lab_size = 1.7, method="circle", colors = c("blue", "white", "red"), outline.color = "gray", show.legend = TRUE, show.diag = FALSE, title="Correlogram of variables")

rm(cordata, corr)

```





##    check relations between variables on (non)linearity

###   linear regression

Maybe start with linear regression with all explaining variables

```{r linear redSquatLobster allExp, echo=FALSE}

#normalization

#library(caret)
#df.temp <- preProcess(df[,c(2,8:14,22)], method=c("center", "scale"))
#df.temp <- as.data.frame(df.temp)

df.temp <- as.data.frame(scale(df[,c(2,8:14,22)]))

#---------------------------------------------------------

#linear regression
lmRSL = lm(redSquatLobster ~  lat + distToCoast + depth + seaSurfaceTemperature + seaSurfaceSalinity + chlorophyll + oxygen + depthOxycline, data = df.temp) 
summary(lmRSL)

library('car') 
vif(lmRSL)

rm(df.temp, lmRSL)

```
Very low RÂ² of 0.01489. So this model has not really much explanatory power.
But most relations are significant.

The correlation plot between the explanatory variables suggested, that there might be multicollinearity.
But since the variation inflation factors (VIF) are quite low, that is not the case. We get higher VIF if we
both include distance to coast and to shelf. So that should be avoided.

###   linear or non-linear relationships between variables

Study the relationship between the population of red squat lobsters and the distance to the coast.

```{r linear redSquatLobster distCoast, echo=FALSE}

#linear regression
lmCoast = lm(redSquatLobster~distToCoast, data = df) 
#summary(lmCoast)

# filtering out zeroes
library(dplyr)
df.temp <- df %>%  filter(redSquatLobster != 0)

#remove outliers
quartiles <- quantile(df.temp$redSquatLobster, probs=c(.25, .75), na.rm = TRUE)
IQR <- IQR(df.temp$redSquatLobster)

Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR 

data_no_outlier <- subset(df.temp, df.temp$redSquatLobster > Lower & df.temp$redSquatLobster < Upper)

#plot
plot(data_no_outlier[ , c("distToCoast", "redSquatLobster")], pch = 16, cex = 0.1, col = "blue")
abline(lmCoast)


```

Obviously, the relation between red squat lobster and distance to coast is not
linear, but rather a hyperbole.

```{r non-linear redSquatLobster distCoast with nls, echo=FALSE}

nlmCoast <- nls(formula = redSquatLobster ~ SSlogis(log(distToCoast), Asym, xmid, scal),data = data_no_outlier)

plot(data_no_outlier[ , c("distToCoast", "redSquatLobster")], pch = 16, cex = 0.1, col = "blue")
lines(data_no_outlier$distToCoast, predict(nlmCoast),lty=2,col="red",lwd=3)

rm(nlmCoast)

```
Not much more information here than above.


```{r non-linear redSquatLobster distCoast with nlcor, echo=FALSE}

#install.packages("devtools") 
library(devtools)
#install_github("ProcessMiner/nlcor")
library(nlcor)

nlcor(data_no_outlier$distToCoast,data_no_outlier$redSquatLobster, plt = T)


```

The function here should display several lines, which depict the slop in the 
appropriate section of the x-axis. Still a completely linear line is depicted. 
So maybe the assumption of a linear relationship is not so wrong at all.

###    decision tree

Maybe the relation of the variable is more complex than a linear one.
Decision trees might help to identify other relationships.

```{r decision tree - first try, echo=FALSE}

# Load the party package. It will automatically load other
# dependent packages.
library(party)

index <- sample(1:dim(df)[1], size = 10000)
df.sample  <- df[index,]

#remove rows with just NAs for red squat lobster
df.sample  <- filter(df.sample, !is.na(df.sample$redSquatLobster))

# Create the tree.
output.tree <- ctree(redSquatLobster ~ distToCoast + depth + seaSurfaceTemperature + seaSurfaceSalinity + chlorophyll + oxygen + depthOxycline, data = df.sample)

# Plot the tree.
plot(output.tree)

#computation takes ages
#could be that the method is rather used for classification
#in that case every different value of redSquatLobster pop could lead to another leaf
#that's why computation takes so long

```
According to this graphic depth and chlorophyll seem to be the most important variables
when you want to determine if there is red squat lobster there or not.


We could also use random forest to make a model, which can fit the data already
better. This model might be already useful for predictions.

```{r random forest, echo=FALSE}

require(randomForest)

set.seed(101)

#delete all rows with NAs in the used variables

df.temp <- df[ ,c(2,7:14,22)] #just take the variables of interest
df.temp <- na.omit(df.temp) #delete NAs

#sum(is.na(df.temp)) #is zero
#sum(is.na(df.temp$redSquatLobster)) #is zero


train = sample(1:nrow(df), 1000) # gives out indexes

rf.lobster = randomForest(redSquatLobster ~ lat + distToCoast + depth + seaSurfaceTemperature + seaSurfaceSalinity + chlorophyll + oxygen + depthOxycline, data = df.temp, subset = train, mtry = 8, importance=TRUE, na.action=na.roughfix)
rf.lobster


```

The percentage of explained variance is 8.19%, which is quite low.

```{r random forest - importance of variables, echo=FALSE}

#https://hackernoon.com/random-forest-regression-in-r-code-and-interpretation

# Get variable importance from the model fit
ImpData <- as.data.frame(importance(rf.lobster))
ImpData$Var.Names <- row.names(ImpData)

library(ggplot2)
ggplot(ImpData, aes(x=Var.Names, y=`%IncMSE`)) +
  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color="skyblue") +
  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )


```

That means splits with the variable depth have the largest impact on MSE. 
What is the meaning of the negative impact of seasurface temperature on MSE?

##result

If you don't apply a treatment for space and time, regression models
seem to perform poorly. Nevertheless, the correlations with explanatory
variables seem to be already quite clear.

#     Spatial Analysis
 
##    Create a heatmap

## creating heatmaps to show geographic density

Create a heatmap to show, where the data points were taken.

```{r heatmap - geographic density, echo=FALSE}

library(ggplot2)
ggplot() +
  stat_density2d(data = df, aes(x = lon, y = lat, fill = ..density..), geom = 'tile', contour = F)
#just looks at the amount of data points, not on the level of any variable


```

Not optimal maybe another approach works better.

```{r heatmap heatmap - geographic density - leaflet, echo=FALSE}

#https://gis.stackexchange.com/questions/168886/r-how-to-build-heatmap-with-the-leaflet-package

## INITIALIZE
library("leaflet")
library("data.table")
library("sp")
library("rgdal")
# library("maptools")
library("KernSmooth")
library("magrittr")


df.temp <- as.data.frame(df)[ ,1:2]
#change tibble to standard data.frame



## MAKE CONTOUR LINES
## Note, bandwidth choice is based on MASS::bandwidth.nrd()
kde <- bkde2D(df.temp, bandwidth=c(.1, .1), gridsize = c(200,200))
CL <- contourLines(kde$x1 , kde$x2 , kde$fhat)



## EXTRACT CONTOUR LINE LEVELS
LEVS <- as.factor(sapply(CL, `[[`, "level"))
NLEV <- length(levels(LEVS))

## CONVERT CONTOUR LINES TO POLYGONS
pgons <- lapply(1:length(CL), function(i)
    Polygons(list(Polygon(cbind(CL[[i]]$x, CL[[i]]$y))), ID=i))
spgons = SpatialPolygons(pgons)

## Leaflet map with polygons
leaflet(spgons) %>% addTiles() %>% 
    addPolygons(color = heat.colors(NLEV, NULL)[LEVS])

rm(df.temp, kde, NLEV, pgons, spgons)
 
```

This is a nice plot. We can see that there is more data along the coastal area.


###   Create a heatmap with leaflet

Let's focus still on the variable red squat lobster. Where are larger, 
where smaller populations?


```{r heatmap - with leaflet, echo=FALSE, include = FALSE, eval = FALSE}

#probably NAs are zeros!!!
df.temp  <- df
df.temp$redSquatLobster[is.na(df.temp$redSquatLobster)] <- 0

#test sum(is.na(df$redSquatLobster)) #worked

#from https://www.supplychaindataanalytics.com/leaflet-heatmaps-in-r/

# importing leaflet and leaflet.extras will enable me to make a heatmap
library(leaflet)
library(leaflet.extras)
library(magrittr)


# creating a heat map for the burger search intensity 
viz_map <- df.temp %>%
  leaflet() %>% 
  addTiles() %>% 
  #addProviderTiles(providers$OpenStreetMap.DE) %>% 
  setView(lng = mean(df$lon), lat = mean(df$lat), zoom = 5) %>%
  addHeatmap(lng=~lon,lat=~lat,intensity = log(df.temp$redSquatLobster),max=0.8,radius=50,blur=10, cellSize = 25)

viz_map


rm(df.temp, viz_map)

#unfortunately doesn't work
```

Unfortunately, the leaflet Heatmap doesn't function properly.


```{r heatmap 1 - with raster, echo=FALSE, include = FALSE, eval = FALSE}

#https://rstudio.github.io/leaflet/raster.html

#addRasterImage from raster package

library(raster)

r <-  rasterFromXYZ(xyz = df[,c(1:2, 22)])
#data is supposed to be on a raster

#crs(r) <- "+proj=longlat +datum=WGS84"

getValues(r) #it is empty and has 100 entries #getvalues is the same as values
values(r) <- 1:100

pal <- colorNumeric(c("#0C2C84", "#41B6C4", "#FFFFCC"),domain = values(r),na.color = "transparent")

leaflet() %>% addTiles() %>%
  addRasterImage(r, colors = pal, opacity = 0.8) %>%
  addLegend(pal = pal, values = values(r),
    title = "Surface temp")

#Error in if (x@srs != "") { : missing value where TRUE/FALSE needed
#That part of the raster object r is empty

```

I think the raster function expects a raster object. Since we create a grid object down below,
we could use those.



### create a heatmap with Voronoi diagrams

create a dataframe with log10 data from df.min

```{r heatmap 1 - create df.log, echo=FALSE, include = FALSE, eval = FALSE}

df.log <- df[,1:22]

for(i in 3:dim(df.log)[2]){   #keep lon and lat
  df.log[,i]= log(df.log[,i])
}

#Nan are okay, -inf maybe not

#replace -inf by zeroes
df.log <- replace(df.log, df.log == -Inf , 0)

#NANs produced

summary(df.log$redSquatLobster) 

```
This seem to have created some negative numbers. Some might be lower than 0.

```{r heatmap 1 - with voronoi, echo=FALSE, include = FALSE, eval = FALSE}

#https://cran.r-project.org/web/packages/ggvoronoi/vignettes/ggvoronoi.html

library(ggplot2)
library(sf)

cali_map <-
  ggplot(data=boundaries.peru_3) +
      scale_fill_gradientn("Lobsters", 
          colors=c("seagreen","darkgreen","green1","yellow","gold4", "sienna"),
          values=scales::rescale(c(-6,0,2,5,8,10))) + 
      scale_color_gradientn("Lobsters", 
          colors=c("seagreen","darkgreen","green1","yellow","gold4", "sienna"),
          values=scales::rescale(c(-6,0,2,5,8,10))) + 
      coord_quickmap() + 
      theme_minimal() +
      theme(axis.text=element_blank(),
            axis.title=element_blank())

cali_map +
      geom_sf() +
      geom_point(data = df.log, aes(x=lon,y=lat,color=redSquatLobster),size=.01) 
 

library(ggvoronoi)     
cali_map +
  geom_voronoi(data = df.log, aes(x=lon,y=lat,fill=redSquatLobster))
#problem with, outline=boundaries.peru_3 object
#geom_voronoi seems to just work with a dataframe, where
#the outline of the area is depicted as a path in the first two
#columns of a dataframe
#it's hard to convert a sf object into such an object

#okay, cannot compute, if wwe leave outline

#--------------------------
#Trouble shooting section

#this is the file used in the example for the outline
library(dplyr)
california <- map_data("state") %>% filter(region == "california")
class(california)

rm(california, cali_map)
#boundaries.peru_3 is an sf file so we needed geom_sf for ggplot

#class(as.data.frame(scale(df.min)))
```

###   Heatmaps with geom_point

```{r heatmap 1 - lobster with ggplot2 and geompoint, echo=FALSE}

#https://stackoverflow.com/questions/71546183/spatial-heatmap-with-given-value-for-colour

summary(df.log$redSquatLobster)
#normal: from 0 to 83.000 -- mean 50
#normal: from -6 to 11 -- mean 0.4


library(RColorBrewer) 

ggplot(df.log, aes(x = lon, y = lat)) + 
  geom_point(aes(color = redSquatLobster, alpha = redSquatLobster)) +
  scale_colour_gradientn(colours=rev(brewer.pal(8,"Spectral")),
                         breaks = log(c(0.1,0.5,1,2,3,4,5,6)),
                         labels = c(0.1,0.5,1,2,3,4,5,6),
                         name = "Amount of Lobsters")+
  guides(alpha = guide_none()) +
  coord_fixed()

#looks similar to the example above

#can't really use normal version here, instead of brackets
#the breaks still seem to be a continuum


```

Still doesn't look very proper. The scale does work differently than with contour plots below. Maybe the logarithmisation has to be "stronger".

```{r heatmap 1 - lobster with ggplot2 and geompoint - new try, echo=FALSE}

#https://stackoverflow.com/questions/71546183/spatial-heatmap-with-given-value-for-colour

#https://towardsdatascience.com/how-to-create-animated-plots-in-r-adf53a775961

summary(df.log$redSquatLobster)
#normal: from 0 to 83.000 -- mean 50
#normal: from -6 to 11 -- mean 0.4

library(gganimate)
library(RColorBrewer) 

p1 <- ggplot(df.log, aes(x = lon, y = lat)) + 
  geom_point(aes(color = redSquatLobster, alpha = redSquatLobster)) +
  scale_colour_gradientn(colours=rev(brewer.pal(8,"Spectral")),
                         breaks = log(c(0.1,0.5,1,2,3,4,5,6)),
                         labels = c(0.1,0.5,1,2,3,4,5,6),
                         name = "Amount of Lobsters")+
  guides(alpha = guide_none()) +
  coord_fixed() +
  labs(title = 'Year: {frame_time}', x = 'longitude', y = 'latitude') +
  transition_time(year) +
  ease_aes('linear')

animate(p1)
anim_save('plot_gdpPercap_lifeExp.gif')


```


Works, but not doesn't look really well.


###   Heatmaps with contourplot for marine animals

####    contour plot including all years

#####   contour plot including all years for red squat lobster

Instead of plotting with points, one could also plot with contours.

```{r heatmap 1 - lobster with ggplot2 and geom_contour_filled, echo=FALSE}

#continuing from above

summary(df$redSquatLobster)
#from 0 to 83.000 --- mean 50

z <- tapply(df$redSquatLobster, list(cut(df$lon, 200), 
                               cut(df$lat, 200)), mean, na.rm = TRUE)

df.tmp <- expand.grid(x = seq(min(df$lon), max(df$lon), length = 200),
                  y = seq(min(df$lat), max(df$lat), length = 200))


df.tmp$z <- c(tapply(df$redSquatLobster, list(cut(df$lon, 200), 
                  cut(df$lat, 200)), mean, na.rm = TRUE))

df.tmp$z[is.na(df.tmp$z)] <- 0

ggplot(df.tmp, aes(x, y)) + 
 geom_contour_filled(aes(z = z), breaks = c(0.1,10,30,70,150,500,1000,5000,10000,25000,50000,100000)) + 
 scale_fill_manual(values = rev(brewer.pal(12, "Spectral")))

 #one has to exclude the 0 here. Otherwise the graphic will make no sense.
#one could add a leaflet map or the outline of the EEZ by using geom_sf 

#library(plotly)
#ggplotly(p)

#unfortunately the result of using plotly is
#geom_GeomContourFilled() has yet to be implemented in plotly.If you'd like to see this geom implemented.
#got multiple warnings like that

```
Lobsters seem to be concentrated in the middle of the map to the coast.

#####   contour plot including all years for jack Mackerel


```{r heatmap 1 - jackMackerel with ggplot2 and geom_contour_filled, echo=FALSE}

#continuing from above

summary(df$jackMackerel)
#from 0 to 53.000 --- mean 17

z <- tapply(df$jackMackerel, list(cut(df$lon, 200), 
                               cut(df$lat, 200)), mean, na.rm = TRUE)

df.tmp <- expand.grid(x = seq(min(df$lon), max(df$lon), length = 200),
                  y = seq(min(df$lat), max(df$lat), length = 200))


df.tmp$z <- c(tapply(df$jackMackerel, list(cut(df$lon, 200), 
                  cut(df$lat, 200)), mean, na.rm = TRUE))

df.tmp$z[is.na(df.tmp$z)] <- 0

ggplot(df.tmp, aes(x, y)) + 
 geom_contour_filled(aes(z = z), breaks = c(0.1,10,30,70,150,500,1000,5000,10000,25000,50000,100000)) + 
 scale_fill_manual(values = rev(brewer.pal(12, "Spectral")))

 #one has to exclude the 0 here. Otherwise the graphic will make no sense.
#one could add a leaflet map or the outline of the EEZ by using geom_sf 

#library(plotly)
#ggplotly(p)

#unfortunately the result of using plotly is
#geom_GeomContourFilled() has yet to be implemented in plotly.If you'd like to see this geom implemented.
#got multiple warnings like that

```

Jack Mackerel seem to live mostly in a midrange area.

#####   contour plot including all years for Humboldt Giant Squid

```{r heatmap 1 - HumboldtGiantSquid with ggplot2 and geom_contour_filled, echo=FALSE}

#continuing from above

summary(df$HumboldtGiantSquid)
#from 0 to 22.000 --- mean 21

z <- tapply(df$HumboldtGiantSquid, list(cut(df$lon, 200), 
                               cut(df$lat, 200)), mean, na.rm = TRUE)

df.tmp <- expand.grid(x = seq(min(df$lon), max(df$lon), length = 200),
                  y = seq(min(df$lat), max(df$lat), length = 200))


df.tmp$z <- c(tapply(df$HumboldtGiantSquid, list(cut(df$lon, 200), 
                  cut(df$lat, 200)), mean, na.rm = TRUE))

df.tmp$z[is.na(df.tmp$z)] <- 0

ggplot(df.tmp, aes(x, y)) + 
 geom_contour_filled(aes(z = z), breaks = c(0.1,10,30,70,150,500,1000,5000,10000,25000,50000)) + 
 scale_fill_manual(values = rev(brewer.pal(11, "Spectral")))

 #one has to exclude the 0 here. Otherwise the graphic will make no sense.
#one could add a leaflet map or the outline of the EEZ by using geom_sf 

#library(plotly)
#ggplotly(p)

#unfortunately the result of using plotly is
#geom_GeomContourFilled() has yet to be implemented in plotly.If you'd like to see this geom implemented.
#got multiple warnings like that

```
More clustered in the north than other marine animals. Otherwise also rather mid range.

#####   contour plot including all years for squid


```{r heatmap 1 - squid with ggplot2 and geom_contour_filled, echo=FALSE}

#continuing from above

summary(df$squid)
#from 0 to 24.000 --- mean 2

z <- tapply(df$squid, list(cut(df$lon, 200), 
                               cut(df$lat, 200)), mean, na.rm = TRUE)

df.tmp <- expand.grid(x = seq(min(df$lon), max(df$lon), length = 200),
                  y = seq(min(df$lat), max(df$lat), length = 200))


df.tmp$z <- c(tapply(df$squid, list(cut(df$lon, 200), 
                  cut(df$lat, 200)), mean, na.rm = TRUE))

df.tmp$z[is.na(df.tmp$z)] <- 0

ggplot(df.tmp, aes(x, y)) + 
 geom_contour_filled(aes(z = z), breaks = c(0.1,10,30,70,150,500,1000,5000,10000,25000)) + 
 scale_fill_manual(values = rev(brewer.pal(10, "Spectral")))

#not that many findings

```
Not many squid data

#####   contour plot of all years for Vinciguerria


```{r heatmap 1 - Vinciguerria with ggplot2 and geom_contour_filled, echo=FALSE}

#continuing from above

summary(df$Vinciguerria)
#from 0 to 63.040.054 --- mean 420

z <- tapply(df$Vinciguerria, list(cut(df$lon, 200), 
                               cut(df$lat, 200)), mean, na.rm = TRUE)

df.tmp <- expand.grid(x = seq(min(df$lon), max(df$lon), length = 200),
                  y = seq(min(df$lat), max(df$lat), length = 200))


df.tmp$z <- c(tapply(df$Vinciguerria, list(cut(df$lon, 200), 
                  cut(df$lat, 200)), mean, na.rm = TRUE))

df.tmp$z[is.na(df.tmp$z)] <- 0

ggplot(df.tmp, aes(x, y)) + 
 geom_contour_filled(aes(z = z), breaks = c(0.1,10,30,70,150,500,1000,5000,10000,100000,10000000)) + 
 scale_fill_manual(values = rev(brewer.pal(11, "Spectral")))

#not that many findings

```

The distance to the coast of Vinciguerria concentration varies a lot here. Highest concentrations are farther away from the coast.

###   Heatmaps with contourplot for explanatory variables

#####   contour plot including all years for depth


```{r heatmap 1 - depth with ggplot2 and geom_contour_filled, echo=FALSE}

summary(df.log$depth)

z <- tapply(df.log$depth, list(cut(df.log$lon, 200), 
                               cut(df.log$lat, 200)), mean, na.rm = TRUE)

df.tmp <- expand.grid(x = seq(min(df.log$lon), max(df.log$lon), length = 200),
                  y = seq(min(df.log$lat), max(df.log$lat), length = 200))


df.tmp$z <- c(tapply(df.log$depth, list(cut(df.log$lon, 200), 
                  cut(df.log$lat, 200)), mean, na.rm = TRUE))

df.tmp$z[is.na(df.tmp$z)] <- 0

ggplot(df.tmp, aes(x, y)) + 
 geom_contour_filled(aes(z = z), breaks = c(0.1,3,4,5,6,7,8,9)) +
 scale_fill_manual(values = rev(brewer.pal(8, "Spectral")))



```

#####   contour plot including all years for oxygen


```{r heatmap 1 - oxygen with ggplot2 and geom_contour_filled, echo=FALSE}

#log obscures too much here, so take normal data

summary(df$oxygen)
#from 5 to 150

z <- tapply(df$oxygen, list(cut(df$lon, 200), 
                               cut(df$lat, 200)), mean, na.rm = TRUE)

df.tmp <- expand.grid(x = seq(min(df$lon), max(df$lon), length = 200),
                  y = seq(min(df$lat), max(df$lat), length = 200))


df.tmp$z <- c(tapply(df$oxygen, list(cut(df$lon, 200), 
                  cut(df$lat, 200)), mean, na.rm = TRUE))

df.tmp$z[is.na(df.tmp$z)] <- 0

ggplot(df.tmp, aes(x, y)) + 
 geom_contour_filled(aes(z = z), breaks = c(5,25,45,65,85,105,125,145,165)) +
 scale_fill_manual(values = rev(brewer.pal(9, "Spectral")))



```
Low oxygen levels to the coast, higher levels father out at sea.

#####   contour plot including all years for depth oxycline


```{r heatmap 1 - depthOxycline with ggplot2 and geom_contour_filled, echo=FALSE}

#log obscures too much here, so take normal data

summary(df.log$depthOxycline)
#median deviates a lot from mean, so maybe log justified
#1.5 till 7

z <- tapply(df.log$depthOxycline, list(cut(df.log$lon, 200), 
                               cut(df.log$lat, 200)), mean, na.rm = TRUE)

df.tmp <- expand.grid(x = seq(min(df.log$lon), max(df.log$lon), length = 200),
                  y = seq(min(df.log$lat), max(df.log$lat), length = 200))


df.tmp$z <- c(tapply(df.log$depthOxycline, list(cut(df.log$lon, 200), 
                  cut(df.log$lat, 200)), mean, na.rm = TRUE))

df.tmp$z[is.na(df.tmp$z)] <- 0

ggplot(df.tmp, aes(x, y)) + 
 geom_contour_filled(aes(z = z), breaks = c(0.1,0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5)) +
 scale_fill_manual(values = rev(brewer.pal(9, "Spectral")))



```

Looks very similar to the oxygen map.

#####   contour plot including all years for chlorophyll


```{r heatmap 1 - chlorophyll with ggplot2 and geom_contour_filled, echo=FALSE}

#log obscures too much here, so take normal data

summary(df$chlorophyll)
#median deviates a lot from mean, so maybe log justified
#-3.5 till 5

z <- tapply(df$chlorophyll, list(cut(df$lon, 200), 
                               cut(df$lat, 200)), mean, na.rm = TRUE)

df.tmp <- expand.grid(x = seq(min(df$lon), max(df$lon), length = 200),
                  y = seq(min(df$lat), max(df$lat), length = 200))


df.tmp$z <- c(tapply(df$chlorophyll, list(cut(df$lon, 200), 
                  cut(df$lat, 200)), mean, na.rm = TRUE))

df.tmp$z[is.na(df.tmp$z)] <- 0

ggplot(df.tmp, aes(x, y)) + 
 geom_contour_filled(aes(z = z), breaks = c(0.1,0.5,1.5,2.5,3.5,4.5,5.5,10,30,60,90,120)) +
 scale_fill_manual(values = rev(brewer.pal(9, "Spectral")))

#It looks much nicer, if you set the intervalls in a logarithmic form, instead of using the log on the data and choose a linear interval

```

More chlorophyll closer to the coast. There seem to be chlorophyll hotspots.

#####   contour plot including all years for seasurface temperature


```{r heatmap 1 - seaSurfaceTemperature with ggplot2 and geom_contour_filled, echo=FALSE}

#log obscures too much here, so take normal data

summary(df$seaSurfaceTemperature)
#10 till 30

z <- tapply(df$seaSurfaceTemperature, list(cut(df$lon, 200), 
                               cut(df$lat, 200)), mean, na.rm = TRUE)

df.tmp <- expand.grid(x = seq(min(df$lon), max(df$lon), length = 200),
                  y = seq(min(df$lat), max(df$lat), length = 200))


df.tmp$z <- c(tapply(df$seaSurfaceTemperature, list(cut(df$lon, 200), 
                  cut(df$lat, 200)), mean, na.rm = TRUE))

df.tmp$z[is.na(df.tmp$z)] <- 0

ggplot(df.tmp, aes(x, y)) + 
 geom_contour_filled(aes(z = z), breaks = c(10,12.5,15,17.5,20,22.5,25,27.5,30,32.5,35)) +
 scale_fill_manual(values = rev(brewer.pal(12, "Spectral")))

#It looks much nicer, if you set the intervalls in a logarithmic form, instead of using the log on the data and choose a linear interval

```
Temperature does not deviate that much. Surprisingly, temperature at the coast is colder at the surface than father out.

#####   contour plot including all years for seasurface salinity


```{r heatmap 1 - seaSurfaceSalinity with ggplot2 and geom_contour_filled, echo=FALSE}

#log obscures too much here, so take normal data

summary(df$seaSurfaceSalinity)
#32 till 36

z <- tapply(df$seaSurfaceSalinity, list(cut(df$lon, 200), 
                               cut(df$lat, 200)), mean, na.rm = TRUE)

df.tmp <- expand.grid(x = seq(min(df$lon), max(df$lon), length = 200),
                  y = seq(min(df$lat), max(df$lat), length = 200))


df.tmp$z <- c(tapply(df$seaSurfaceSalinity, list(cut(df$lon, 200), 
                  cut(df$lat, 200)), mean, na.rm = TRUE))

df.tmp$z[is.na(df.tmp$z)] <- 0

ggplot(df.tmp, aes(x, y)) + 
 geom_contour_filled(aes(z = z), breaks = c(32,32.5,33,33.5,34,34.5,35,35.5,36)) +
 scale_fill_manual(values = rev(brewer.pal(9, "Spectral")))

#It looks much nicer, if you set the intervalls in a logarithmic form, instead of using the log on the data and choose a linear interval

```

Less salinity at the coast, more salinity further out. Very little variation.



####    yearly contour plot

#####   yearly contour plot for red squat lobster

Now try to plot a yearwise or cruisewise heatmap for lobsters

```{r heatmap 1 - lobster with ggplot2 and geom_contour_filled split by year, noCruise, echo=FALSE, include = FALSE, eval = FALSE}

MapList2 <- vector(mode = "list")

#add year and cruise information back to the dataset
df.temp <- cbind(df.log, df$cruiseNo, df$year)
colnames(df.temp)[23:24] <- c("cruiseNo", "year")


for (i in 1:nlevels(df.temp$cruiseNo)){
    df.temp2 <- df.temp[df.temp$cruiseNo == i,]
z <- tapply(df.temp2$redSquatLobster, list(cut(df.temp2$lon, 200), 
                               cut(df.temp2$lat, 200)), mean, na.rm = TRUE)

df.tmp <- expand.grid(x = seq(min(df.temp2$lon), max(df.temp2$lon), length = 200),
                  y = seq(min(df.temp2$lat), max(df.temp2$lat), length = 200))


df.tmp$z <- c(tapply(df.temp2$redSquatLobster, list(cut(df.temp2$lon, 200), cut(df.temp2$lat, 200)), mean, na.rm = TRUE))

df.tmp$z[is.na(df.tmp$z)] <- 0

map <- ggplot(df.tmp, aes(x, y)) + 
 geom_contour_filled(aes(z = z), breaks = c(0.1, 1,2, 3,4, 5,6,7,8,9, 10, 15)) +
 scale_fill_manual(values = rev(brewer.pal(8, "Spectral")))

  map <- list(map)                #push the map one down in order
  MapList2 <- append(MapList2, map) #so that a map here is its own list entry

  }

#Warnung: [38;5;232m`stat_contour()`: Zero contours were generated[39m
#Warnung: no non-missing arguments to min; returning Inf
#Warnung: no non-missing arguments to max; returning -Inf


#information like year was dropped before, so we can't really
#use facet_grid here

#we have to use a for loop here

MapList2[3]

```

Doesn't seem to work.


```{r heatmap 1 - lobster with ggplot2 and geom_contour_filled split by year - another attempt, echo=FALSE}

library(RColorBrewer) 

#there are lobsters beginning with 1998


for(i in 1998:2012){
  df.temp <- df[df$year == i, ] 

z <- tapply(df.temp$redSquatLobster, list(cut(df.temp$lon, 200), 
                               cut(df.temp$lat, 200)), mean, na.rm = TRUE)

df.tmp <- expand.grid(x = seq(min(df.temp$lon), max(df.temp$lon), length = 200),
                  y = seq(min(df.temp$lat), max(df.temp$lat), length = 200))


df.tmp$z <- c(tapply(df.temp$redSquatLobster, list(cut(df.temp$lon, 200), 
                  cut(df.temp$lat, 200)), mean, na.rm = TRUE))

df.tmp$z[is.na(df.tmp$z)] <- 0

p1 <- ggplot(df.tmp, aes(x, y)) + 
 geom_contour_filled(aes(z = z), breaks = c(0.1,10,30,70,150,500,1000,5000,10000,25000,50000)) + 
 scale_fill_manual(values = rev(brewer.pal(11, "Spectral")))
  print(p1)
}


```
Plots are not that nice. Since you always start a new plot, the axis are different.
At list the buckets stay the same. It is hard to spot movements or changes in
time this way.



#####   yearly contour plot for sardines

```{r heatmap 1 - sardine with ggplot2 and geom_contour_filled split by year - another attempt 2, echo=FALSE}

library(RColorBrewer) 

#sardines exist from 1983 until 2002
#summary(df$sardine)

for(i in 1983:2002){
  df.temp <- df[df$year == i, ] 

z <- tapply(df.temp$sardine, list(cut(df.temp$lon, 200), 
                               cut(df.temp$lat, 200)), mean, na.rm = TRUE)

df.tmp <- expand.grid(x = seq(min(df.temp$lon), max(df.temp$lon), length = 200),
                  y = seq(min(df.temp$lat), max(df.temp$lat), length = 200))


df.tmp$z <- c(tapply(df.temp$redSquatLobster, list(cut(df.temp$lon, 200), 
                  cut(df.temp$lat, 200)), mean, na.rm = TRUE))

df.tmp$z[is.na(df.tmp$z)] <- 0

p1 <- ggplot(df.tmp, aes(x, y)) + 
 geom_contour_filled(aes(z = z), breaks = c(0.1,5,10,20,70,150,500,1000,5000,10000,20000)) + 
 scale_fill_manual(values = rev(brewer.pal(11, "Spectral")))
  print(p1)
}


```

not better with other sardines. Why do earlier years not work.


```{r heatmap - remove, echo=FALSE}

rm(CL, df.temp, df.temp2, p1, z, i, inurl, LEVS)


```



### plot chloropeth maps

####    chloropeth maps including all years


```{r spatial - aggregate by region - chloropeth, echo=FALSE}

#simple plot

plot(agg.region.sf.quad5["redSquatLobster"])

#-------------------------------------- 

#plot with deliberately choosing breaks and colours

library(RColorBrewer)
pal <- brewer.pal(7, "OrRd") # we select 7 colors from the palette
class(pal)
    
plot(agg.region.sf.k80["redSquatLobster"], 
     main = "Lobsters", 
     breaks = "quantile", nbreaks = 7,
     pal = pal)

#--------------------------------------

library(tmap)
tm_shape(agg.region.sf.k80) + tm_fill(col="redSquatLobster", style="quantile", n=8, palette="Greens") +
              tm_legend(outside=TRUE)

#-------------------------------------

library(sp)
#agg.region.sf is an sf not sp object
spplot(as(agg.region.sf.k199, 'Spatial'), "redSquatLobster") 

#looks similar to standard plot() output
#more breaks here actually important

```

####    yearly chloropeth maps

Here we want to create Chloropeth maps. 

A problem could be, that the created areas have a different size by tesselation. 
A bigger area might have more marine animals than a smaller one, just by being bigger.

I would like to try and animate a chloropeth map, which goes through the different years.

#####   animated chloropeth map of red squat lobster


```{r  plot chloropeth map - red squat lobster , echo=FALSE}



#-------------------------------------------------
#https://stackoverflow.com/questions/52007878/using-ggplot-to-plot-shapefile-and-gganimate-for-animation

library(gganimate)
library(gifski)

test <- agg.region.k199.year.balanced.imp %>% 
  ggplot() +
  geom_sf(aes(fill = redSquatLobster)) +
  scale_fill_viridis_b() +
  theme_void() +
  coord_sf(datum = NA) +
  labs(title = 'Year: {current_frame}') +
  transition_manual(year) 

animate(test)

#it works, the animate() function writes PNGs into the working directory
#not animating anything within RStudio though

#it shows movements of the red squat lobster
#still very far away from being nice in any way.

rm(test)

```

Red squat lobsters seem to be also far into the ocean. This is highly
likely not correct.




```{r  plot dataframes with region and year - red squat lobster , echo=FALSE}

#tutorial taken from
#https://community.rstudio.com/t/animate-a-map-based-on-year-values/73578/2

library(raster)
library(sf)
library(rgeos)

library(tidyverse)
library(gganimate)
library(transformr)
library(gifski)
library(rmapshaper) #to display sf objects faster, it is required to downsize them



lobster_animated <- ggplot(agg.region.k119.year.balanced.imp) +
  geom_sf(col = NA, aes(fill=(redSquatLobster),geometry=geometry,group=year)) +
  scale_fill_continuous() + 
  coord_sf(datum = NA) +
  theme_minimal()  + 
  geom_text(mapping=aes(x=-65,y=-20,label=year, size = 30))

anim <- lobster_animated + transition_states(year,
                                    transition_length = .5,
                                    state_length = .5)

gganimate::animate(anim, renderer=gifski_renderer(),width = 800, height = 600,
                   nframes=50,fps=5)


```
No data for earlier years, in later years closer to the coast, but also some population
far out in the ocean.

#####   animated chloropeth map of anchovy


```{r  plot dataframes with region and year - anchovy , echo=FALSE}


anchovy_animated <- ggplot(agg.region.k119.year.balanced.imp) +
  geom_sf(col = NA, aes(fill=(anchovy),geometry=geometry,group=year)) +
  scale_fill_continuous() + 
  coord_sf(datum = NA) +
  theme_minimal()  + 
  geom_text(mapping=aes(x=-65,y=-20,label=year, size = 30))

anchovy_anim <- anchovy_animated + transition_states(year,
                                    transition_length = .5,
                                    state_length = .5)

gganimate::animate(anchovy_anim, renderer=gifski_renderer(),width = 800, height = 600,
                   nframes=50,fps=5)


```

It's hard to see any trends in the graphic. There seem to be several highs in
population in the years 1988, 1997, 2012. There seems to be also a shift away from
the coast during the years.

```{r  plot dataframes with region and year - anchovy , echo=FALSE}

#create a consecutive month id, a combination of year and month seems to be harder to do
#there seems to be something wrong with the orignal dataset

anchovy_month_animated <- ggplot(agg.region.k80.month.balanced.imp.test) +
  geom_sf(col = NA, aes(fill=(anchovy),geometry=geometry,group=id)) +
  scale_fill_continuous() + 
  coord_sf(datum = NA) +
  theme_minimal()  + 
  geom_text(mapping=aes(x=-65,y=-20,label=year, size = 30))

anchovy__month_anim <- anchovy_month_animated + transition_states(id,
                                    transition_length = .5,
                                    state_length = .5)

gganimate::animate(anchovy__month_anim , renderer=gifski_renderer(),width = 800, height = 600,
                   nframes=50,fps=5)


```

The created monthly spatial balanced panel data set seems to be not right.
I probably have to fix it upstream.


#####   animated chloropeth map of sardine


```{r  plot dataframes with region and year - sardine , echo=FALSE}


sardine_animated <- ggplot(agg.region.k119.year.balanced.imp) +
  geom_sf(col = NA, aes(fill=(sardine),geometry=geometry,group=year)) +
  scale_fill_continuous() + 
  coord_sf(datum = NA) +
  theme_minimal()  + 
  geom_text(mapping=aes(x=-65,y=-20,label=year, size = 30))

sardine_anim <- sardine_animated + transition_states(year,
                                    transition_length = .5,
                                    state_length = .5)

gganimate::animate(sardine_anim, renderer=gifski_renderer(),width = 800, height = 600,
                   nframes=50,fps=5)


```

There seem to be a high in the 90ies and then after 200 there seem to be no sardines 
any more. That is in line with the information we had beforehand, that the 
population of sardines has inclined, because of temperature change due to the
El Nino phenomenon.

#####   animated chloropeth map of sardine


```{r  plot dataframes with region and year - temperature , echo=FALSE}


seaSurfaceTemperature_animated <- ggplot(agg.region.k119.year.balanced.imp) +
  geom_sf(col = NA, aes(fill=(seaSurfaceTemperature),geometry=geometry,group=year)) +
  scale_fill_continuous() + 
  coord_sf(datum = NA) +
  theme_minimal()  + 
  geom_text(mapping=aes(x=-65,y=-20,label=year, size = 30))

seaSurfaceTemperature_anim <- seaSurfaceTemperature_animated + transition_states(year,
                                    transition_length = .5,
                                    state_length = .5)

gganimate::animate(seaSurfaceTemperature_anim, renderer=gifski_renderer(),width = 800, height = 600,
                   nframes=50,fps=5)


```

Very interesting graphic. There seem to be a trend in temperature. The ocean seems
to get cooler over the years as you can see by the darker colours.
The regional homogeneity in temperature also seems to vary.


###    Additional preparation for spatial methods

We prepare some essential additional information to handle Spatial regression.
One key variable is a list of neighbours, that each area has. The basic idea of
Spatial Regression is that neighbouring areas influence each other.

Here we take the Voronoi tesselation with 119 created regions.

```{r  create a neighbourhood list, echo=FALSE}

#https://mgimond.github.io/simple_moransI_example/

library(spdep)

nb <- poly2nb(agg.region.sf.k119, queen=TRUE)
lw <- nb2listw(nb, style="W", zero.policy=TRUE)
inc.lag <- lag.listw(lw, agg.region.sf.k119$redSquatLobster)

#-----------------------
is.symmetric.nb(nb) #yes, it is symmetric

#------------------------

plot(inc.lag ~ agg.region.sf.k119$redSquatLobster, pch=16, asp=1)
M1 <- lm(inc.lag ~ agg.region.sf.k119$redSquatLobster)
abline(M1, col="blue")

#------------------------

coef(M1)[2]

```
As you can see, the slope is positive. The positive slope is an indication of positive
spatial autocorrelation.The height of the slope is Morans I.



## Calculation of Spatial dependence

###   Morans I - first
```{r  spatial - calculate Morans I, echo=FALSE}

library(spdep)
I <- moran(agg.region.sf.k119$redSquatLobster, lw, length(nb), Szero(lw))[1]
I

moran.mc(as.vector(agg.region.sf.k119$redSquatLobster), lw, nsim=99999)


#------------------------

#Analytical method
moran.test(agg.region.sf.k119$redSquatLobster,lw, alternative="greater")

#------------------------

#library(ape)

#Moran.I(agg.region.sf.k119$redSquatLobster, lw) 
#different weight expected?!
#Error in if (dim(weight)[1] != dim(weight)[2]) stop("'weight' must be a square matrix")

#-------------------------


MC<- moran.mc(agg.region.sf.k119$redSquatLobster, lw, nsim=999, alternative="greater")

# View results (including p-value)
MC

plot(MC)



```
For Morans I, no matter the method, the result is always the same: 
It is 0.3152 for k = 80.
It is 0.5511 for k = 119

This might be a hint that Morans I is increasing in the amount of regions created.

The curve shows the distribution of Moran I values we could expect had the red squat lobster been randomly distributed across the regions. Note that our observed statistic, 0.315, falls way to the right of the distribution suggesting that the income values are clustered (a positive Moranâs I value suggests clustering whereas a negative Moranâs I value suggests dispersion).



###   Local Moran I

```{r Morans I - Local Moran I , echo=FALSE}

lisaRslt <- spdep::localmoran(agg.region.sf.k119$redSquatLobster, lw, 
                              zero.policy = TRUE, na.action = na.omit)

#dim(lisaRslt); dim(agg.region.sf);

# Now we can derive the cluster/outlier types (COType in ArcGIS term) for each spatial feature in the data
significanceLevel <- 0.05; # 95% confidence
meanVal <- mean(agg.region.sf.k119$redSquatLobster);

lisaRslt %<>% tibble::as_tibble() %>%
  magrittr::set_colnames(c("Ii","E.Ii","Var.Ii","Z.Ii","Pr(z > 0)")) %>%
  dplyr::mutate(coType = dplyr::case_when(
  `Pr(z > 0)` > 0.05 ~ "Insignificant",
  `Pr(z > 0)` <= 0.05 & Ii >= 0 & agg.region.sf.k119$redSquatLobster >= meanVal ~ "HH",
  `Pr(z > 0)` <= 0.05 & Ii >= 0 & agg.region.sf.k119$redSquatLobster < meanVal ~ "LL",
  `Pr(z > 0)` <= 0.05 & Ii < 0 & agg.region.sf.k119$redSquatLobster >= meanVal ~ "HL",
  `Pr(z > 0)` <= 0.05 & Ii < 0 & agg.region.sf.k119$redSquatLobster < meanVal ~ "LH"
))

# Now add this coType to original sf data
agg.region.sf.k119$coType <- lisaRslt$coType %>% tidyr::replace_na("Insignificant")

ggplot(agg.region.sf.k119) +
  geom_sf(aes(fill=coType),color = 'lightgrey') +
  scale_fill_manual(values = c('red','brown','NA','blue','cyan'), name='Clusters & \nOutliers') +
  labs(title = "Amount of lobsters")


```
With lower amount of regions not that many high-high,
but with a higher amount of regions, there are clear high-highs at the coast,
as expected. That also means that a higher resolution of regions
close to the coast seem to be important.


###   Bivariate Moran I

This checks the local relation between two variables. Might be interesting for us,
if two different marine animal species live in similar locations.

Let's try sardine and jack mackerel. In the per cruise correloplots they most of the
time share some correlation.

```{r Morans I - Bivariate Moran I , echo=FALSE}

#http://www.geo.hunter.cuny.edu/~ssun/R-Spatial/spregression.html
#Chapter 4.3

moranBVboot <- spdep::moran_bv(x = datX <- agg.region.sf.k119$sardine, 
                y = datY <- agg.region.sf.k119$jackMackerel, 
                listw = lw, 
                nsim = 500)

# Tend to overestimate the association
moranBVboot$t0
boot::boot.ci(moranBVboot, conf=c(0.99, 0.95, 0.9), type="basic")
plot(moranBVboot)


```



```{r Morans I - Bivariate Moran I - 2, echo=FALSE}

#http://www.geo.hunter.cuny.edu/~ssun/R-Spatial/spregression.html
#Chapter 4.3

localMoranBV <- spdep::localmoran_bv(x = datX, 
                y = datY, 
                listw = lw, 
                nsim = 500) %>% as.data.frame()

# To plot as HH, HL, LH, LL
# refer 
# listw2mat, nb2mat functions to get a matrix mat
# apply the matrix to scaled y  mat%*%scale(y) > 0 high < 0 low
meanVal <- lw %>% listw2mat() %*% scale(datY) %>% as.vector()
significanceLevel <- 0.05; # 95% confidence

#localMoranBV$datY <- datY
#localMoranBV$meanVal <- meanVal

localMoranBV %<>% tibble::as_tibble() %>%
  magrittr::set_colnames(c("Ibvi","E.Ibvi","Var.Ibvi","Z.Ibvi","P", "PSim", "PFoldedSim")) %>%
  dplyr::mutate(lmbvType = dplyr::case_when(
  `PSim` > 0.05 ~ "Insignificant",
  `PSim` <= 0.05 & Ibvi >= 0 & datY >= meanVal ~ "HH",
  `PSim` <= 0.05 & Ibvi >= 0 & datY < meanVal ~ "LL",
  `PSim` <= 0.05 & Ibvi < 0 & datY >= meanVal ~ "HL",
  `PSim` <= 0.05 & Ibvi < 0 & datY < meanVal ~ "LH"
))
  
# Now add this lmbvType to original sf data
agg.region.sf.k119$lmbvType <- localMoranBV$lmbvType %>% tidyr::replace_na("Insignificant")


ggplot(agg.region.sf.k119) +
  geom_sf(aes(fill=lmbvType),color = 'lightgrey') +
  scale_fill_manual(values = c('red','brown','NA','blue','cyan')[1:5], name='Clusters & \nOutliers') +
  labs(title = "Local Bivariate Moran's I - sardine & jack mackerel")


```

In the top region there are high jack mackerels and sardines. In the lower
area there are regions, where there are many sardines but few jack mackerels or vice versa.

Hard to see any significant spatial correlation between sardine and jack mackerel
in this graphic.


```{r Morans I - Bivariate Moran I - 3, echo=FALSE}

#http://www.geo.hunter.cuny.edu/~ssun/R-Spatial/spregression.html
#Chapter 4.3

mapview::mapview(x = agg.region.sf.k119, 
                 zcol='lmbvType', 
                 col.regions = c('red','brown','NA','blue','cyan')[1:5])
#the results can be showwn on a leaflet map


```

Same graphic, just with leaflet

###    Spatial regression

First do a standard regression.

```{r spatial regression - spatial, echo=FALSE}

#https://pages.cms.hu-berlin.de/EOL/gcg_quantitative-methods/Lab15_SpatialRegression.html

#colnames(agg.region.sf)

formula <- redSquatLobster ~ distToCoast + seaSurfaceTemperature + seaSurfaceSalinity + chlorophyll + oxygen + depthOxycline

lobster.lm <- lm(formula, data = agg.region.sf.k119)
summary(lobster.lm)

```
Very high RÂ² of 0.65 already. With k = 80 it is 0.76. So the more regions,
the lower the RÂ². 
But just two betas significant?! How does that go together?



```{r spatial regression - spatial, echo=FALSE}

#https://pages.cms.hu-berlin.de/EOL/gcg_quantitative-methods/Lab15_SpatialRegression.html

test <- agg.region.sf.k119

test$residuals <- residuals(lobster.lm)
test$fitted <- fitted(lobster.lm)

# We can plot the location dataset using geom_sf() instead of geom_point()
ggplot(test, aes(col = residuals, size = residuals)) +
  geom_sf() +
  scale_color_gradient2()


```

[!!!] Here all geometrys are filled, which are the lines of the region borders
      Can one do a better plot?

So in the middle of the coast the residuals were positive. That means there were more lobsters
than the model anticipated. In the north there were less lobsters than anticipated by the model.
So yes, spatial lag is likely.

```{r spatial regression - spatial 2, echo=FALSE}

#https://pages.cms.hu-berlin.de/EOL/gcg_quantitative-methods/Lab15_SpatialRegression.html

moran.plot(test$residuals, lw, zero.policy = TRUE)


```
Moranâs I is the slope of the regression line between the spatially lagged values 
and the observed values (0.55). 

The regions with the cross symbol are the most significant outliers.


###   actual spatial regression

```{r spatial regression - spatial 3, echo=FALSE}

#https://pages.cms.hu-berlin.de/EOL/gcg_quantitative-methods/Lab15_SpatialRegression.html

# fit spatial lag moddel
library(spatialreg)
lobster.slm <- spatialreg::lagsarlm(formula, data = agg.region.sf.k119, listw = lw, zero.policy = TRUE)

#lobster.slm <- errorsarlm(formula, data = agg.region.sf, listw = lw, zero.policy = TRUE)
summary(lobster.slm)

#[!!!] lagsarlm and errorsarlm are not anymore in spdep, but in spatialreg

```

Are residuals still related? If yes, then we might have a spatial error model instead.

```{r spatial regression - spatial 4, echo=FALSE}

#http://www.geo.hunter.cuny.edu/~ssun/R-Spatial/spregression.html#spatial-error-and-lag-models
#Chapter 4.2.2

# Derive the residuals from the regression. Need to handle those missed values
slResiduals <- rep(0, length(agg.region.sf.k119$redSquatLobster))
resIndex <- lobster.slm$residuals %>% names() %>% as.integer();
slResiduals[resIndex] <- lobster.slm$residuals

# Test if there is spatial autocorrelation in the regression residuals (errors).
lw %>%
  spdep::moran.test(slResiduals, ., zero.policy = TRUE)



```
There is no spatial dependence between residuals after applying a lag model anymore. 
The p value is 0.31 for k = 80 and is 0.07 for k = 119, so we stay with the spatial lag model.


```{r spatial regression - lag or error graphic, echo=FALSE}

#https://rpubs.com/quarcs-lab/tutorial-spatial-regression


#![alt text here](path-to-image-here)

#add a graphic here, when you should choose lag or error

```


```{r spatial regression - spatial lag or error, echo=FALSE}

#https://rpubs.com/quarcs-lab/tutorial-spatial-regression

lmLMtests <- lm.LMtests(lobster.lm, lw, test=c("LMerr", "LMlag", "RLMerr", "RLMlag", "SARMA"))
lmLMtests


```

The lowest value is RLMlag, so we choose a lag modell. We ignore the very high p-values?



```{r spatial regression - regression - spatial lag, echo=FALSE}

#https://rpubs.com/quarcs-lab/tutorial-spatial-regression

library(spatialreg)
lobster.lm2 = lmSLX(formula, data = agg.region.sf.k119, lw)
summary(lobster.lm2)

```
Again good RÂ², but low significance. Multicollinearity or nonlinear relationships here?


####    Direct, indirect and total impacts

Usually a spatial regression measures the direct impact of a variable x_i on
a variable y in the same. But there is also an indirect effect. When the x_i
influences the other x and y of other regions, some of this impact is transferred back
to the original region. This is the indirect effect.

Direct plus indirect effect make the total effect. That means the spatial regression
results underestimate the impact.


```{r spatial regression - regression - impact, echo=FALSE}

#https://rpubs.com/quarcs-lab/tutorial-spatial-regression

impacts(lobster.lm2, listw = lw)

#---------------------------------

summary(impacts(lobster.lm2, listw=lw, R=500), zstats = TRUE) 

```

Sometimes direct and indirect effect have a different sign.


###    Geographically Weighted Regression

You can have different betas in different regions with this regression type.
By going into this method, we will also learn if there is spatial heterogeneity.
Different betas in different regions could be seen as a statistic for
spatial heterogeneity.

Larger spatial heterogeneity could be a sign for omitted variables. Not
unlikely that we will se that here.

```{r spatial regression - GWG - bandwidth, echo=FALSE}

#http://www.geo.hunter.cuny.edu/~ssun/R-Spatial/spregression.html#spatial-error-and-lag-models
#Chapter 4.3

#as far as I see, it is done unaggregated, so I imitate that

# Remove all the NAs in the dataset
df.NoNA <- df.log %>% tidyr::drop_na()
#if we make it that simple, a lot of incomplete data will be dropped.
#okay, we have 0 with df and 84162 with df.min

library("GWmodel")
# Estimate an optimal bandwidth
bwVal <- GWmodel::bw.gwr(formula,
                         data = agg.region.sf.k119 %>% sf::as_Spatial(), 
                         approach = 'AICc', kernel = 'bisquare', 
                         adaptive = TRUE)

#as data input an sf or sp is expected and will be converted to a Spatial Data frame.

#agg.region.sf is already in a suitable format, so I try this first


```

```{r spatial regression - GWG - actual regression, echo=FALSE}

#http://www.geo.hunter.cuny.edu/~ssun/R-Spatial/spregression.html#spatial-error-and-lag-models
#Chapter 4.3

# Perform a basic GWR
gwr.res <- gwr.basic(formula,
                     data = agg.region.sf.k119 %>% sf::as_Spatial(), 
                     bw = bwVal, kernel = "bisquare", adaptive = TRUE)
# Show the results
print(gwr.res)


```

The difference of GWR coefficients from min to max is quite large.
Does that mean we have high heterogeneity? These differences also
exist in the example.

I personally think, yes this suggests high heterogeneity.



Furthermore, one can calculate a local RÂ²

```{r spatial regression - GWG - local RÂ², echo=FALSE}

#http://www.geo.hunter.cuny.edu/~ssun/R-Spatial/spregression.html#spatial-error-and-lag-models
#Chapter 4.3

#names(gwr.res$SDF)

# Because the spplot cannot handle those unconventional column names, we need to do extra works.
spGWRData <- gwr.res$SDF@data;
spGWRData$coefChlorophyll <- spGWRData$chlorophyll;
# Calculate the p value from (student) t value
spGWRData$pChlorophyll <- 2*pt(-abs(spGWRData$`chlorophyll_TV`), df = dim(spGWRData)[1] -1)

spGWRData$coefOxygen <- spGWRData$oxygen; 
spGWRData$coefSeaSurfaceTemperature <- spGWRData$seaSurfaceTemperature

spGWR <- gwr.res$SDF; 
spGWR@data <- spGWRData;

# This show how the coefficients of the master degree on unemployment change over space. 
spplot(spGWR, 'Local_R2', main="Local R Squared")


```



```{r spatial regression - GWG - effect chlorophyll, echo=FALSE}

#http://www.geo.hunter.cuny.edu/~ssun/R-Spatial/spregression.html#spatial-error-and-lag-models
#Chapter 4.3

# This show how the coefficients of the master degree on unemplyment change over space. 
spplot(spGWR, 'coefChlorophyll', main="Estimated Coefficients of Chlorophyll on Lobster")


```


```{r spatial regression - GWG - effect chlorophyll, echo=FALSE}

#http://www.geo.hunter.cuny.edu/~ssun/R-Spatial/spregression.html#spatial-error-and-lag-models
#Chapter 4.3

# But this variable is not signficant everywhere.
spplot(spGWR, 'pChlorophyll', main="p-value of Chlorophyll on Lobster")


```

```{r spatial regression - GWG - effect chlorophyll, echo=FALSE}

#http://www.geo.hunter.cuny.edu/~ssun/R-Spatial/spregression.html#spatial-error-and-lag-models
#Chapter 4.3

# Another factor of highschool
spplot(spGWR, 'coefOxygen', main="Estimated Coefficients of Oxygen on Lobster")


```
There are higher values in the middle at the coast. Maybe this alone leads to higher
betas in that region. Probably you have to scale your data, if you want to
do GWR with this results?






#   analysis focusing on time

##    determine general trends

We want to find out general trends by using the averages per year.

First look at the explaining variables sea surface temperature, sea surface salinity,
chlorophyll, oxygen, depth oxycline.

```{r timeseries - trends - exp var, echo=FALSE}

df.temp <- subset(agg.year,select = c('year',
                               'seaSurfaceTemperature',
                               'seaSurfaceSalinity',
                               'chlorophyll',
                               'oxygen',
                               'depthOxycline'
                               ))



library("reshape2") 
data_long <- melt(df.temp, id.vars = "year")

ggplot(data_long,                            # Draw ggplot2 time series plot
       aes(x = year,
           y = value,
           col = variable)) +
  geom_line()
 

```
At the beginning of the 1990ies the temperature was higher than in other years.
In the end of the 1990ies the amount of chlorophyll was higher than in other years.
Maybe the higher temperature also lead to an increase in chlorophyll.
With oxygen and depthOxycline there seems to be a seasonal effect over the years
with a 5 year cycle. Why?


Now for the trends of marine animals

```{r timeseries - trends - marine animals  1, echo=FALSE}

df.temp <- subset(agg.year,select = c('year',
                               'masas',
                               'anchovy',
                               'sardine',
                               'jackMackerel',
                               'chubMackerel',
                               'redSquatLobster'
                               ))



library("reshape2") 
data_long <- melt(df.temp, id.vars = "year")

ggplot(data_long,                            # Draw ggplot2 time series plot
       aes(x = year,
           y = value,
           col = variable)) +
  geom_line()

rm(data_long, df.temp)
 

```
One can clearly see that there are more anchovy over the years. Also the
decline of sardines is clearly visible. The same is true for jack and chub 
mackerel. Red squat lobster just seems to have a seasonality, with no clear trend
over the years.

Sardine and mackerel decline is highly likely related to the temperature change
due to El nino, but it might also be related to fishing. It might be interesting
to check how fishing quotas have evolved during this period of time.


```{r timeseries - trends - marine animals  1, echo=FALSE}

df.temp <- subset(agg.year,select = c('year',
                               'samasa',                               
                                'bag',
                               'fvo',
                               'HumboldtGiantSquid',
                               'Vinciguerria',
                               'mic',
                               'mesozooplankton',
                               'squid'
                               ))



library("reshape2") 
data_long <- melt(df.temp, id.vars = "year")

ggplot(data_long,                            # Draw ggplot2 time series plot
       aes(x = year,
           y = value,
           col = variable)) +
  geom_line()

rm(data_long, df.temp)
 

```
There seems to be no data for these marine animals before 1998.
Also, mesozooplankton and Vinciguerria seem to have a lot more population
in 2010 than in other year. What could explain this? Maybe both of these animals
live further out in the Ocean and just in 2010 the ship was going so far out?



##    determine within a year seasonal effects

###   Go through lobster example

We just look at lobster here, make some learnings and then apply them 
to other time series

####    Create time series averaging per time of the year 

Take the variable mun, add all years, and just look at the distribution during the year.

```{r timeseries seasons redSquatLobster, echo=FALSE}

library(dplyr)

df.temp <- df %>% 
    group_by(date2) %>% 
    summarise(redSquatLobster=mean(redSquatLobster, na.rm =TRUE))

# Most basic bubble plot
p <- ggplot(df.temp, aes(x=date2, y=redSquatLobster)) +
  geom_line() + 
  xlab("")
p

```
Please ignore the 0000 here.

Okay, even summing up the data over specific datas of all the years 
does not lead to having a complete time series. There is a  gap in the months
May and June.


####   Quick imputation of red squat lobster time series - based only on timeseries itself

For further analysis, the gap in the middle has to be imputed. There are methods
in the forecast and tsimpute package in R for that. First use interpolation from
forecast package

```{r timeseries seasons redSquatLobster - impute, echo=FALSE}

#https://stats.stackexchange.com/questions/224130/missing-data-imputation-in-time-series-in-r

ts.lobster <- ts(df.temp[,2], frequency = 1) 
#frequency is the amount of data points per period
#to state 7 means that we look at the ts from a week angle
#frequency matters, what is the right value?

library(forecast)
ts.lobster.noNA <- na.interp(ts.lobster)
plot.ts(ts.lobster.noNA)


```
Wow, that does not look good. na.interp doesn't have that many parameters.
So maybe choose another approach.

```{r timeseries seasons redSquatLobster - impute 2, echo=FALSE}

#https://stats.stackexchange.com/questions/224130/missing-data-imputation-in-time-series-in-r

library(zoo)
ts.lobster.noNA <- na.approx(ts.lobster)
plot(ts.lobster.noNA)

```

```{r timeseries seasons redSquatLobster - impute 3, echo=FALSE}

#https://stats.stackexchange.com/questions/224130/missing-data-imputation-in-time-series-in-r

library(imputeTS)
#ts.lobster.noNA <- na_kalman(ts.lobster)
ts.lobster.noNA <- na_kalman(ts.lobster, model ="auto.arima", smooth = FALSE)
plot(ts.lobster.noNA)

```
Similar results.


We try to show the trend with simple moving average of order 25.

```{r timeseries seasons redSquatLobster - smoothed, echo=FALSE}

library("TTR")

ts.lobster.SMA <- SMA(ts.lobster.noNA,n=10)
plot.ts(ts.lobster.SMA)

```

####    Try to decompose the imputed time series



```{r timeseries seasons redSquatLobster - decompose, echo=FALSE}

df.temp2 <- decompose(ts.lobster.noNA)
plot(df.temp2)

```

Decompose does not work.

#https://community.rstudio.com/t/decomposition-of-time-series-yields-error/86081

decompose needs 2 observations for each period.
Periods can be defined in the ts() method.

```{r timeseries seasons redSquatLobster - auto.arima, echo=FALSE}

library(forecast)
auto.arima(ts.lobster.noNA)

#okay, auto.arima really has troubles here too

```
log likelihood is terrible, no seasonal effects here?

###   seasonal effects explanatory variables

First we have to change the Date2 Variable to keep it from showing the year 2023.
It should be corrected further above. It was done so.




```{r convert Date 2, echo=FALSE}

#df <- df %>% mutate(date2 = format(as.Date(date2), "%d-%m"))
#str(df$date2)
#this converts a lubridate object into a character
#df$date2 <- dmy(df$date2)
#library(lubridate)
#df$date2 <- as_date(df$date2)

df$date2 <- as.Date(df$date2) #function scale_x_date doesn't work with lubridate format
str(df$date2)

```

####    seasonality - depth

```{r timeseries seasons expvar depth, echo=FALSE}

library(dplyr)
df.depth <- df %>% group_by(date2) %>% summarise(depth=mean(depth, na.rm =TRUE))
#df.depth <- df.depth %>% mutate(date2 = format(as.Date(date2), "%m-%d"))

library(scales)  #to show just the month

# Most basic bubble plot
library(ggplot2)
p <- ggplot(df.depth, aes(x=date2, y=depth)) +
  geom_line() +
  scale_x_date(labels = date_format("%b"))+
  xlab("")
p

rm(p)

```
Here the criss cross movement of the boat might still be seen. It seems to be that on
average the boat went further out in the autumn months.

Probably distance to coast and distance to shelf will show a similar result, so
omitted for now.

```{r timeseries seasons expvar depth - smooth, echo=FALSE}

library("TTR")

ts.depth <- ts(df.depth$depth)
ts.depth.SMA <- SMA(ts.depth,n=10)
plot.ts(ts.depth.SMA, xlab = "Day of the year", ylab = "Depth")


```






####    Seasonality - Sea Surface Temperature

```{r timeseries seasons expvar seaSurfaceTemperature, echo=FALSE}

library(dplyr)
df.seaSurfaceTemperature <- df %>% 
    group_by(date2) %>% 
    summarise(seaSurfaceTemperature=mean(seaSurfaceTemperature, na.rm =TRUE))

# Most basic bubble plot
library(ggplot2)
p <- ggplot(df.seaSurfaceTemperature, aes(x=date2, y=seaSurfaceTemperature)) +
  geom_line() + 
  scale_x_date(labels = date_format("%b"))+
  xlab("")
p

rm(p)

```
Oh, unexpectedly the average temperature in all of the years is lower in summer.


```{r timeseries seasons expvar seaSurfaceTemperature - smooth, echo=FALSE}

library("TTR")

ts.seaSurfaceTemperature <- ts(df.seaSurfaceTemperature$seaSurfaceTemperature)

#Check for NAs
#sum(is.na(ts.seaSurfaceTemperature)) # 1NA
library(imputeTS)
ts.seaSurfaceTemperature <- na_ma(ts.seaSurfaceTemperature, k = 4, weighting = "exponential", maxgap = Inf)


ts.seaSurfaceTemperature.SMA <- SMA(ts.seaSurfaceTemperature,n=10)
plot.ts(ts.seaSurfaceTemperature.SMA, xlab = "Day of the year", ylab = "Sea Surface Temperature")

rm(df.seaSurfaceTemperature)

```



####    seasonality - chlorophyll

```{r timeseries seasons expvar chlorophyll, echo=FALSE}

library(dplyr)
df.chlorophyll <- df %>% 
    group_by(date2) %>% 
    summarise(chlorophyll=mean(chlorophyll, na.rm =TRUE))

# Most basic bubble plot
library(ggplot2)
p <- ggplot(df.chlorophyll, aes(x=date2, y=chlorophyll)) +
  geom_line() + 
  scale_x_date(labels = date_format("%b"))+
  xlab("")
p


```
Little bit lower during the summer months.

```{r timeseries seasons expvar chlorophyll - smooth, echo=FALSE}

library("TTR")

ts.chlorophyll <- ts(df.chlorophyll$chlorophyll)

#Check for NAs
#sum(is.na(ts.chlorophyll)) # 1NA
library(imputeTS)
ts.chlorophyll <- na_ma(ts.chlorophyll, k = 4, weighting = "exponential", maxgap = Inf)


ts.chlorophyll.SMA <- SMA(ts.chlorophyll,n=10)
plot.ts(ts.chlorophyll.SMA, xlab = "Day of the year", ylab = "Chlorophyll")

rm(p, df.chlorophyll)

```


####    Time Series Analysis with Chlorophyll

Here we want to decompose the chlorophyll time series into trend, season and noise.
We experiment with different frequencies.

```{r timeseries seasons expvar chlorophyll - make ts longer, echo=FALSE}

ts.chlorophyll.longer <- c(ts.chlorophyll, ts.chlorophyll, ts.chlorophyll, ts.chlorophyll)
ts.chlorophyll.longer <- ts(ts.chlorophyll.longer)

#-------------------
#Don't take the mean, but it be as is


ts.chlorophyll.moreData <- df.chlorophyll <- df %>% group_by(date2)
ts.chlorophyll.moreData <- ts(ts.chlorophyll.moreData$chlorophyll, frequency = 30)

library(imputeTS)
ts.chlorophyll.moreData <- na_ma(ts.chlorophyll.moreData, k = 4, weighting = "exponential", maxgap = Inf)

#imputation takes too much time

#---------------------

ts.chlorophyll.weekly <- ts(ts.chlorophyll  , frequency = 7)


```



```{r timeseries seasons expvar chlorophyll - decompose, echo=FALSE}

ts.chlorophyll.components <- decompose(ts.chlorophyll.weekly)

plot(ts.chlorophyll.components)

```
The decomposition doesn't look good, the trend still contains a seasonality.


```{r timeseries seasons expvar chlorophyll - stl, echo=FALSE}


ts.chlorophyll.components <- stl(ts.chlorophyll.weekly, t.window = 90, s.window = "periodic", robust = TRUE)


plot(ts.chlorophyll.components)

```
This looks much better. Here the trend seems to be void of within year 
seasonalities. The chlorophyll is lower in the southern hemisphere winter, 
which is during May to July and higher in southern hemisphere summer.

The remainder still seems to contain a seasonality, so there is still room
for improvement.

```{r timeseries seasons expvar chlorophyll - mstl, echo=FALSE}

library(forecast)

ts.chlorophyll.components.multi <- mstl(ts.chlorophyll.weekly, s.window = "periodic", robust = TRUE)


plot(ts.chlorophyll.components.multi)

```
This looks worse than the attempt before.

```{r timeseries seasons expvar chlorophyll - auto.arima, echo=FALSE}


t <- auto.arima(ts.chlorophyll)
t
#plot(t$fitted) # fitted plot looks quite nice. Why so low log likelihood?

rm(df.chlorophyll, p, t)


```
Still not a good results, but way better than with red squat lobster.

```{r timeseries seasons expvar oxygen, echo=FALSE}

library(dplyr)
df.oxygen <- df %>% 
    group_by(date2) %>% 
    summarise(oxygen=mean(oxygen, na.rm =TRUE))

# Most basic bubble plot
library(ggplot2)
p <- ggplot(df.oxygen, aes(x=date2, y=oxygen)) +
  geom_line() + 
  scale_x_date(labels = date_format("%b"))+
  xlab("")
p

rm( p)

```
The within year time series of oxygen already looks quite stationary.
Could it be that the oxygen level has a seasonal span of a month and
is linked to the moon (lunar cycle)?



```{r timeseries seasons expvar oxygen - smooth, echo=FALSE}

library("TTR")

ts.oxygen <- ts(df.oxygen$oxygen)

#Check for NAs
#sum(is.na(ts.chlorophyll)) # 1NA
library(imputeTS)
ts.oxygen <- na_ma(ts.oxygen, k = 4, weighting = "exponential", maxgap = Inf)


ts.oxygen.SMA <- SMA(ts.oxygen,n=10)
plot.ts(ts.oxygen.SMA, xlab = "Day of the year", ylab = "oxygen")

rm(df.oxygen)

```

The amount of oxygen could  vary with the lunar cycle. The local maxima
seem to be 30 days apart.


###   seasonal effects other marine animals

####    seasonality of red squat lobster


```{r timeseries seasons  redSquatLobster, echo=FALSE}

library(dplyr)
df.redSquatLobster <- df %>% 
    group_by(date2) %>% 
    summarise(redSquatLobster=mean(redSquatLobster, na.rm =TRUE))

# Most basic bubble plot
library(ggplot2)
p <- ggplot(df.redSquatLobster, aes(x=date2, y=redSquatLobster)) +
  geom_line() +
  scale_x_date(labels = date_format("%b"))+
  xlab("")
p

rm(p)

```
This marine animal also has a trend of there being more of it in spring and less
in autumn.


```{r timeseries seasons redSquatLobster - smooth, echo=FALSE}

library("TTR")

ts.redSquatLobster <- ts(df.redSquatLobster$redSquatLobster)

#Check for NAs
#sum(is.na(ts.chlorophyll)) # 1NA
library(imputeTS)
ts.redSquatLobster <- na_ma(ts.redSquatLobster, k = 4, weighting = "exponential", maxgap = Inf)


ts.redSquatLobster.SMA <- SMA(ts.redSquatLobster,n=10)
plot.ts(ts.redSquatLobster.SMA, xlab = "Day of the year", ylab = "redSquatLobster")


```

####    seasonality of jack mackerel

```{r timeseries seasons  jackMackerel, echo=FALSE}

library(dplyr)
df.jackMackerel <- df %>% 
    group_by(date2) %>% 
    summarise(jackMackerel=mean(jackMackerel, na.rm =TRUE))

# Most basic bubble plot
library(ggplot2)
p <- ggplot(df.jackMackerel, aes(x=date2, y=jackMackerel)) +
  geom_line() +
  scale_x_date(labels = date_format("%b"))+
  xlab("")
p

rm(p)

```
This marine animal also has a trend of there being more of it in spring and less
in autumn.


```{r timeseries seasons redSquatLobster - smooth, echo=FALSE}

library("TTR")

ts.jackMackerel <- ts(df.jackMackerel$jackMackerel)

#Check for NAs
#sum(is.na(ts.chlorophyll)) # 1NA
library(imputeTS)
ts.jackMackerel <- na_ma(ts.jackMackerel, k = 4, weighting = "exponential", maxgap = Inf)


ts.jackMackerel.SMA <- SMA(ts.jackMackerel,n=10)
plot.ts(ts.jackMackerel.SMA, xlab = "Day of the year", ylab = "jackMackerel")

rm(p, ts.jack.mackerel,  df.jackMackerel)

```



####    seasonality of sardine


```{r timeseries seasons expvar sardine, echo=FALSE}

library(dplyr)
df.sardine <- df %>% 
    group_by(date2) %>% 
    summarise(sardine=mean(sardine, na.rm =TRUE))

# Most basic bubble plot
library(ggplot2)
p <- ggplot(df.sardine, aes(x=date2, y=sardine)) +
  geom_line() +
  scale_x_date(labels = date_format("%b"))+
  xlab("")
p

rm(p)

```
Most sardines in May, fewest in the autumn months. Might be related to seaSurfaceTemperature then?


```{r timeseries seasons sardine - smooth, echo=FALSE}

library("TTR")

ts.sardine <- ts(df.sardine$sardine)

#Check for NAs
#sum(is.na(ts.chlorophyll)) # 1NA
library(imputeTS)
ts.sardine <- na_ma(ts.sardine, k = 4, weighting = "exponential", maxgap = Inf)


ts.sardine.SMA <- SMA(ts.sardine,n=10)
plot.ts(ts.sardine.SMA, xlab = "Day of the year", ylab = "sardine")


rm(p, df.sardine, ts.sardine)

```


####    seasonality of anchovy



```{r timeseries seasons expvar anchovy, echo=FALSE}

library(dplyr)
df.anchovy <- df %>% 
    group_by(date2) %>% 
    summarise(anchovy=mean(anchovy, na.rm =TRUE))

# Most basic bubble plot
library(ggplot2)
p <- ggplot(df.anchovy, aes(x=date2, y=anchovy)) +
  geom_line() + 
  scale_x_date(labels = date_format("%b"))+
  xlab("")
p

rm(p)

```
It almost  looks like a cycle, which is 2 or 3 month long.


```{r timeseries seasons sardine - smooth, echo=FALSE}

library("TTR")

ts.anchovy <- ts(df.anchovy$anchovy)

#Check for NAs
#sum(is.na(ts.chlorophyll)) # 1NA
library(imputeTS)
ts.anchovy <- na_ma(ts.anchovy, k = 4, weighting = "exponential", maxgap = Inf)


ts.anchovy.SMA <- SMA(ts.anchovy,n=10)
plot.ts(ts.anchovy.SMA, xlab = "Day of the year", ylab = "anchovy")


rm(p, df.anchovy, ts.anchovy)

```



###   quick analysis of correlation between time series

By now, I haven't found any special statistics for the correlation of time series
variables. Usually, standard methods for correlation of numeric variables are used.
This could be e.g. the Spearman rank correlation coefficient.

See: https://www.r-bloggers.com/2020/03/testing-the-correlation-between-time-series-variables/


##    time series for for the 2012

We had a look at the general trend and the seasonality. The question is now, can
we use the data as it is without aggregation as a daily time series.

```{r timeseries redSquatLobster 2012, echo=FALSE}

df.temp <- dplyr::filter(df, year == 2012) #6944 entries
df.temp <- ts(df.temp)

library("TTR")

redSquatLobster_2012 <- SMA(df.temp[,22],n=5)
plot.ts(redSquatLobster_2012)


```
What you see here is the 6944 observations in the year 2012 in a row and the amount of red squat lobsters at each observation. Red Squat Lobsters live close to the coast. As we have seen above the ship drives in a criss cross pattern, that is it drives alternating towards the coast and from it. That is what you can see here. You cannot see how the population of red squat lobsters changes during the 2012 tour of the boat.

So, using the daily time series is nonsensical.

A further analysis with ARIMA methods might proof futile. Time Series Regression might still lead to good results, since all explaining variables will variate in the same manner.

```{r timeseries redSquatLobster 2012 - decompose, echo=FALSE}

#df.temp <- dplyr::filter(df.temp, redSquatLobster != 0) 
    #even if you filter out all 0s, you get no real decomposition
df.temp2 <- decompose(df.temp[,22])
plot(df.temp2)

#here are also no periods found

```

Standard decomposition still doesn't work.

```{r timeseries redSquatLobster 2012 - auto.arima, echo=FALSE}

#ARIMA methods from package forecast
#https://otexts.com/fpp2/regarima.html

#just 2012, no NAs
#deleting rows in time series data might be problematic
df.temp <- dplyr::filter(df, year == 2012) #6944 entries
df.temp <- dplyr::filter(df.temp, !is.na(redSquatLobster) & !is.na(distToCoast))

library(forecast)
fit <- auto.arima(as.matrix(df.temp[,"redSquatLobster"]), xreg= as.matrix(df.temp[,"distToCoast"]))
fit

```

Interpretation?
So the higher distToCoast, the lower the population of red squat lobsters.
If the population of red squat lobsters was high in the last period,
the population in the current period is bound to be lower.

```{r timeseries redSquatLobster 2012 - auto.arima 2, echo=FALSE}

cbind("Regression Errors" = residuals(fit, type="regression"),
      "ARIMA errors" = residuals(fit, type="innovation")) %>%
  autoplot(facets=TRUE)


checkresiduals(fit)

#interpretation ACF
#https://medium.com/@ooemma83/how-to-interpret-acf-and-pacf-plots-for-identifying-ar-ma-arma-or-arima-models-498717e815b6

```


The residuals are still quite large, which means, the fit of the time series
is not optimal. At least the residuals seem to be random.

Interpretation of ACF?

##    Work on aggregated models per cruise

### Examination of time series in aggregated data

```{r agg per cruise - first ts plot, echo=FALSE}

plot(ts(agg.cruise$redSquatLobster))
#just change the variable here for other results

```

Looking through timeplots there seem to be a lot of marine animals in general at the 59(?)th cruise.

Let's plot all variables as a ts plot and do a facet approach

```{r agg per cruise - all var ts plot, echo=FALSE}

#first norm the columns
agg.cruise.ts <- agg.cruise[,2:23]
agg.cruise.ts <- as.data.frame(scale(agg.cruise.ts))
#normalization with mean 0 and stand deviation of 1
#scale can be applied to a whole dataframe
#I hope it works column by column


library(ggfortify)
autoplot(ts(agg.cruise.ts), facets = FALSE)

#cant plot everything at once

```

This is too much information in one plot. One should split the variables into
several plots. The results should be comparable to the per year trends above.

```{r agg per cruise - depth distance ts plot, echo=FALSE}

library(ggfortify)
autoplot(ts(agg.cruise.ts[,c(1:3)]), facets = FALSE)

#just plot distanceToCoast,distanceToShelf,depth

```
One can see that the average depth and distance to the coast is not always the same in all cruises. There are some cruises, which were generally closer to the coast and some which were generally further away. The variance should not be as pronounced as it is here. 


```{r agg per cruise - expvar lobster ts plot, echo=FALSE}

library(ggfortify)
autoplot(ts(agg.cruise.ts[,c(3:5,14)]), facets = FALSE)

#just two explanatory variables at once

```
There are three peaks for lobster amount. The middle one
might be explained by a cruise with lower depth.The first one maybe by higher because of higher sea surface temperature and lower salinity.

```{r agg per cruise - expvar lobster ts plot - 2, echo=FALSE}

library(ggfortify)
autoplot(ts(agg.cruise.ts[,c(6:8,14)]), facets = FALSE)

#just two explanatory variables at once

```
Lobsters seem to thrive in cruises of high chlorophyll and low oxygen.

Conclusion
Really hard to say anything for sure. Maybe a high clorophyll level during a cruise has nothing to with high chlorophyll levels during the year, but rather with the cruise was more close to the coast. If more chlorophyll exists close to the coast.

But if you take average depth, distance to coast into consideration, a regression might still lead to meaningful results

###   Check correlations of per cruise data

####   correlations of explaining variables

```{r agg correlations explaining variables, echo=FALSE}

cordata <- agg.cruise[,c(2:9)]
cordata <- na.omit(cordata)

corr <- round(cor(cordata), 2)

library(ggcorrplot)

ggcorrplot(corr, hc.order = TRUE, type = "lower", lab = TRUE, lab_size = 3, method="circle", colors = c("blue", "white", "red"), outline.color = "gray", show.legend = TRUE, show.diag = FALSE, title="Correlogram of explaining variables")

```

The correlations between the explanatory variables are stronger in the aggregated data set

####   correlations between all variables

```{r agg correlations all variables, echo=FALSE}

#unfortunately I was not able to put all the explaining variables on one axis and all animal population variables to the other axis. I get a quadratic MatrixError

#since we have to omit NAs not all marine animals are viable here

cordata <- agg.cruise[,c(2:9, 11:15)]
cordata <- na.omit(cordata)

corr <- round(cor(cordata), 2)

library(ggcorrplot)

ggcorrplot(corr, type = "lower", lab = TRUE, lab_size = 1.7, method="circle", colors = c("blue", "white", "red"), outline.color = "gray", show.legend = TRUE, show.diag = FALSE, title="Correlogram of variables")

rm(cordata, corr)

```
Also the correlations between the explanatory variables and the marine animals are better.

###   Check correlations within every cruise

```{r correlations all variables within cruise - preperations, echo=FALSE}

#unfortunately I was not able to put all the explaining variables on one axis and all animal population variables to the other axis. I get a quadratic MatrixError

#since we have to omit NAs not all marine animals are viable here

cordata <- df[,c(7:14,19:27,40)]
cordata <- na.omit(cordata) #a lot of cruises are omitted by doing this


#-----------------------------------------
#Amount of levels present

nlevels(cordata$cruiseNo) #68

#Check if there are cruises with very few data points

countLevels <- dplyr::count(cordata, cordata$cruiseNo)
print(countLevels) 
print(dim(countLevels))
#So just 25 cruises have no NAs and were not omitted

#----------------------------------------

#maybe we have to drop all unused labels
#levels(droplevels(cordata$cruiseNo))

cordata$cruiseNo <- factor(cordata$cruiseNo)
#doesn't work?
levels(cordata$cruiseNo)

```
Too bad we have just 25 cruises where we have enough complete cases for ALL the variables
of interest. The function "cor" needs complete cases. Maybe there is another function?


```{r correlations all variables within cruise - MapList version, echo=FALSE}

#https://stackoverflow.com/questions/42536187/facet-or-grouped-correlation-and-correlogram-plots-in-r

library(ggplot2)
library(Hmisc) 
library(corrplot)

# split the data 
#B <- split(cordata, cordata$cruiseNo) #extremely large
##extract names
#nam<-names(B)
# Plot three pictures
#par(mfrow=c(5,5))
#col<- colorRampPalette(c("red","white","blue"))(40)
#for (i in seq_along(B)){
  # Calculate the correlation in all data.frames using lapply 
#  M<-cor(data.matrix(B[[i]]))
#  corrplot(M, type="upper",tl.col="black", tl.cex=0.7,tl.srt=45, col=col,
# addCoef.col = "black", p.mat = M$P, insig = "blank",sig.level = 0.01)
#  mtext(paste(nam[i]),line=1,side=3)
#  }

#rm(B)

#------------------------------------------------

library(stringr)

MapListCorr <- list()
n=1

for(i in levels(cordata$cruiseNo)){
  df.temp <- cordata[cordata$cruiseNo == i, 1:(dim(cordata)[2]-1)] 
  #[all data from a single cruise ,noCruise must be removed]
  corr <- round(cor(df.temp), 2)
  library(ggcorrplot)
  p1 <- ggcorrplot(corr, type = "lower", lab = TRUE, lab_size = 1.7, method="circle", colors = c("blue", "white",    "red"), outline.color = "gray", show.legend = TRUE, show.diag = FALSE, title=str_glue('Correlogram of variables during cruise {i}'))
  
  MapListCorr[[n]] <-  p1
  n=n+1
}

library(gridExtra)
n <- length(MapListCorr)
nCol <- floor(sqrt(n))
do.call("grid.arrange", c(MapListCorr, ncol=nCol))

#works somehow

```

It seems not to be possible to plot everything into one plot, so we try it with
several single plots.

```{r correlations all variables within cruise - print loop, echo=FALSE, warning =FALSE}

#the working version for now, without a maplist

library(stringr)


for(i in levels(cordata$cruiseNo)){
  df.temp <- cordata[cordata$cruiseNo == i, 1:(dim(cordata)[2]-1)] 
  #[all data from a single cruise ,noCruise must be removed]
  corr <- round(cor(df.temp), 2)
  library(ggcorrplot)
  p1 <- ggcorrplot(corr, type = "lower", lab = TRUE, lab_size = 1.7, method="circle", colors = c("blue", "white",    "red"), outline.color = "gray", show.legend = TRUE, show.diag = FALSE,  title=str_glue('Correlogram of variables during cruise {i}'))
  print(p1)
}

#can't use : use="complete.obs" to use all data available in cor


```

You can choose the cruise by clicking on the picture. Keep in mind that not
all cruises can be shown, since not all cruises have all the variables shown here.

The correlations vary a lot, so there might be a kind  of temporal hetereogeneity.
That is the correlations vary over time.

Maybe in the text information should be added, when the cruise was taking place.
Maybe cruises in fall and spring share similarities?



###   Analyze the per cruise data with non time related methods

```{r agg per cruise - regression lobster - 2, echo=FALSE}

#colnames(agg)

model <- lm(redSquatLobster ~ distToCoast + seaSurfaceTemperature + seaSurfaceSalinity + chlorophyll + oxygen + depthOxycline, data = agg.cruise)

summary(model)

#library('car') 
#vif(model)
#no multicollinearity found

```
In the scaled version you can see that the depth of oxycline has the highest impact on there being lobsters. And only this value seems to be significant.

```{r random forest agg, echo=FALSE}

require(randomForest)

set.seed(101)

#delete all rows with NAs in the used variables

df.temp <- na.omit(agg.cruise) #delete NAs
#that leaves just 19 out of 64 oobservations

#sum(is.na(df.temp)) #is zero
#sum(is.na(df.temp$redSquatLobster)) #is zero


train = sample(1:nrow(agg.cruise), 30) # gives out indexes

rf.lobster = randomForest(redSquatLobster ~ distToCoast + depth + chlorophyll + oxygen , data = df.temp, subset = train, mtry = 8, importance=TRUE, na.action=na.roughfix)
rf.lobster


```
Random Forest seems not to work with so few observations. We have a negative
variance explained.



```{r random forest agg - importance of variables, echo=FALSE}

#https://hackernoon.com/random-forest-regression-in-r-code-and-interpretation

# Get variable importance from the model fit
ImpData <- as.data.frame(importance(rf.lobster))
ImpData$Var.Names <- row.names(ImpData)

library(ggplot2)
ggplot(ImpData, aes(x=Var.Names, y=`%IncMSE`)) +
  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color="skyblue") +
  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )


```


###   arima on aggregated data

Try a quick arima:

```{r agg per cruise - arima lobster - 2, echo=FALSE}

library(forecast)
fit <- auto.arima(as.matrix(agg.cruise[,"redSquatLobster"]), xreg= as.matrix(agg.cruise[,"distToCoast"]))
fit

```
That looks worse than in the unaggregated data, but is also not very elaborate.

```{r agg per cruise - arima lobster - 3, echo=FALSE}

library(forecast)
ts.lobster <- ts(agg.cruise$redSquatLobster, frequency = 3)
plot(ts.lobster)

#ts.lobster <- ts(ts.lobster[30:68])
#You get better results, if you ignore the zero part, but it's not overwhelmingly better.

library(imputeTS)
ts.lobster <- na_ma(ts.lobster, k = 4, weighting = "exponential", maxgap = Inf)

library(tseries)
adf.test(diff(ts.lobster), alternative = "stationary", k = 0)


acf(diff(ts.lobster))
pacf(diff(ts.lobster))
#How can you see the ARIMA model from these graphics

fit <- arima(ts.lobster, c(1,1,2), seasonal = list(order = c(0, 1, 1), period = 3))
fit

fit2<- auto.arima(ts.lobster)
fit2

fit3<- auto.arima(ts.lobster, xreg = as.matrix(agg.cruise[,2:9]))
fit3


```
So the lobster time series is supposedly stationary according to the Augmented Dickey-Fuller Test.



#   looking at space and time together

##    Spatial Panel Data models

###   Panel Data on Aggregated Data

First try a panel model plm without adding any spatial information on the aggregated k80 data.

```{r panel data model}

# Packages needed
library(plm)

#Random effects (using plm)

form  <-  redSquatLobster ~ distToCoast+depth+oxygen+chlorophyll+seaSurfaceTemperature+seaSurfaceSalinity+depthOxycline

random <- plm(form, data=agg.region.k80.year.balanced.imp, index=c("region", "year"), model="random")
summary(random)


#---------------------------
#check for multicollinearity
library(car)
vif(random)
#looks good

```

Most significant regressors are sea surface temperature, depth and salinity.
Depth is understandable. sea surface temperature could be a hint towards
that the season might still play an important role. In some years, the 
data was taken more in autumn, in some more in spring.

Could we add a column for that and make a control variable for when
the data during the year was roughly taken? Can we eliminate the
effect of the implicit "time of the year" variable on the data? 

https://www.rpubs.com/zerohour/control1
Here a control with interaction terms is suggested. Should be easy to implement.


Multicollinearity was not present.

###   Spatial Panel Data on Aggregated Data

Now try a Spatial Panel Data with SPLM method.
spml method doesn't accept an unbalanced data.frame
so we have to fill in missing rows for missing years and regions
and somehow impute the newly generated rows with data

```{r spatial panel data model}

# Packages needed
library(splm)
#-------------------------------------
#create a spatial weight matrix

#one could also choose other region amounts

library(spdep)
nb <- poly2nb(tess.peruOcean.voronoi.k80, queen=TRUE) 
#cannot use agg.region here, because we need to know just the neighbourhood
#relation of the 80 regions
lw.V.k80 <- nb2listw(nb, style="W", zero.policy=TRUE)


#--------------------------------------
#actual regression

form  <-  redSquatLobster ~ distToCoast+depth+oxygen+chlorophyll+seaSurfaceTemperature+seaSurfaceSalinity+depthOxycline

random <- spml(form, data=agg.region.k80.year.balanced.imp, index=c("region", "year"), listw = lw.V.k80, model="random", spatial.error="b", Hess = FALSE)
summary(random)

summary.splm(random)

#there seem to not be an actual goodness of fit measure in the splm package

#---------------------------
#check for multicollinearity
library(car)
#vif(random)
#why has is stopped working?

rm(random, form)

```
Seasurface temperature and depth still the most significant regressors.

###   Visualize the Spatial Panel Model

####    Visualization with nonlinear ANOVA approach

Here an approach using ANOVA instead of a classical linear spatial regression method

```{r spatial panel data model - visualizing}

#https://cran.r-project.org/web/packages/pspatreg/vignettes/C_Examples_pspatreg_Panel_data.html



form3d_psanova_restr <- redSquatLobster ~ depth + chlorophyll +
                        pspt(region, year, 
                         nknots = c(18,18,8), psanova = TRUE, 
                         nest_sp1 = c(1, 2, 3), 
                         nest_sp2 = c(1, 2, 3),
                         nest_time = c(1, 2, 2),
                         f1t = FALSE, f2t = FALSE)


#--------------------------------------------------

#lw exists still from the spatial focus part

library(pspatreg)
sp3danovasarar1 <- pspatfit(form3d_psanova_restr, data = agg.region.k119.year.balanced.imp, 
                              listw = lw, method = "Chebyshev", 
                              type = "sar", cor = "ar1")  
summary(sp3danovasarar1)

#Looks quite nice


#--------------------------------------------------


plot_sptime(sp3danovasarar1, data = agg.region.k119.year.balanced.imp, 
            time_var = "year", reg_var = "region")

#subscripts out of bounds, but why

```
This does run through mostly, but in the end doesn't work.

####    Visualization with ggplot        

Now we try something a lot simpler,by simply plotting the value over the years
and facet it by region in a simple ggplot.

#####   anchovy

As an example we choose anchovy, since anchovy are present during the whole
time and also have a clear trend etc., respectively interesting time series features.

```{r spatial panel data model - visualizing - ggplot - anchovy}

#https://cran.r-project.org/web/packages/pspatreg/vignettes/C_Examples_pspatreg_Panel_data.html

agg.region.k80.year.balanced.imp %>%
  ggplot( aes(x=year, y=anchovy, group=region, color=region)) +
    geom_line()


```

You cannot really see something in this plot. But you can see that if in one year 
in one region the population of anchovy is high, that is  also true for other regions.
Sometimes the local maxima are shifted.

#####   sardine

```{r spatial panel data model - visualizing - ggplot - sardine}

#https://cran.r-project.org/web/packages/pspatreg/vignettes/C_Examples_pspatreg_Panel_data.html

agg.region.k80.year.balanced.imp %>%
  ggplot( aes(x=year, y=sardine, group=region, color=region)) +
    geom_line()


```
The local maxima between regions seem to be much more shifted than in anchovy.
Also in some regions there seem to be still sardines at the end of the 1990ies,
while it has declined already in other regions.

#####   jack mackerel

```{r spatial panel data model - visualizing - ggplot - jack mackerel}

#https://cran.r-project.org/web/packages/pspatreg/vignettes/C_Examples_pspatreg_Panel_data.html

agg.region.k80.year.balanced.imp %>%
  ggplot( aes(x=year, y=jackMackerel, group=region, color=region)) +
    geom_line()


```
With jack mackerel, steep grow seems to just have occured in a fraction of
the regions, while others seem to be much lower. If you look close to the x-axis,
you can see that jack mackerel is present in most of the regions nonetheless, 
because there are a lot of blue lines above the x-axis.


#####   red squat lobster

```{r spatial panel data model - visualizing - ggplot - red squat lobster}

#https://cran.r-project.org/web/packages/pspatreg/vignettes/C_Examples_pspatreg_Panel_data.html

agg.region.k80.year.balanced.imp %>%
  ggplot( aes(x=year, y=redSquatLobster, group=region, color=region)) +
    geom_line()


```
One could see from the animated spatial plots, that the red squat lobster
has moved regionally. One could also see that here, since the peaks
are a little wild here. Not in all region seem to be maxima at the same time.
Also the peaks that stand out on top are not always in the same colour.
That is also a hint that the location of red squat lobsters is shifting over time.


###   Spatial Panel Model on unaggregated data

In contrast to using regions, the neighbours of a  point are also points and 
have to be determined.Here a method is used, where the closest ten points are taken.

Problem is that several points are at the same location. Maybe we have to take 
the average of all points, which are in the same location.

Also points are not in the same position in time, since the data is not collected 
from stationary buoyes. That means a panel data analysis might not be possible, 
if we consider the individual points.

```{r panel data model - create neighbour matrix}

#https://www.insee.fr/en/statistiques/fichier/3635545/imet131-k-chapitre-7.pdf


# Packages needed
library(plm)
library(splm)
library(sp)
library(maps)
library(maptools)
library(spdep)

class(tess.peruOcean.voronoi)


#-------------------------------
#convert to spatial data frame

xy <- df[,c(1,2)]
spdf <- SpatialPointsDataFrame(coords = xy, data = df)

#----------------------------

#take a subsample, because of memory issues
ind <- sample(dim(spdf)[1], 10000)
df.temp <- spdf[ind,]


#----------------------------


# Creation of a k matrix plus close neighbours, k = 10
#map_crd <- coordinates(tess.peruOcean.voronoi)
map_crd <- coordinates(df.temp)
Points_PeruOcean <- SpatialPoints(df.temp)
PeruOcean.knn_10 <- knearneigh(Points_PeruOcean, k=10)
#the function looks for the next 10 neighbours of every points
#since we have over 300.000 this takes a while
#the function registers several points at the same location and gives a warning

K10_nb <- knn2nb(PeruOcean.knn_10)
wknn_10 <- nb2listw(K10_nb, style="W")

#---------------------------------------

class(PeruOcean.knn_10)

```



```{r panel data model - panel without spatial}

#https://www.insee.fr/en/statistiques/fichier/3635545/imet131-k-chapitre-7.pdf


#formula
form <- redSquatLobster ~ distToCoast+depth+oxygen+chlorophyll
#the name verdoorn is taken from the example and kept to make imitating easier



result_pooled <- plm(form , data = df.temp, model = "pooling")
#Warnung: duplicate couples (id-time) in resulting pdata.frame
#to find out which, use, e.g., table(index(your_pdataframe), useNA = "ifany")
###[!!!] maybe aggregate same day data?


summary(result_pooled)

#----------------------------------

result_fe1<- plm(form, data = df.temp, model = "within", effect="individual")
summary(result_fe1)
#no significant betas

#------------------------------------

result_re1<- plm(form, data = df.temp, model = "random", effect="individual")
summary(verdoorn_re1)


#----------------------------------------
#trouble shooting with plm


#Error in qtab(...) : negative length vectors are not allowed

#Error could stem from resulting data.frame being too large
#https://stackoverflow.com/questions/42479854/merge-error-negative-length-vectors-are-not-allowed
#or run out of memory



```



```{r spatial panel data model - Hausmann test}

#https://www.insee.fr/en/statistiques/fichier/3635545/imet131-k-chapitre-7.pdf


# Packages needed
library(plm)
library(splm)
library(sp)
library(maps)
library(maptools)
library(spdep)

#--------------------------------------

#If putting date in front might already help, just do that first

library(dplyr)
df.temp <- df.temp %>% relocate(date, .after=lat)
#worked, nice!
#but doesn't help

#we could also rename cruiseNo to be our new date
#that should make any true unbalanced dataset problems we have smaller
#we could also add a cruise middate variable 
#both approaches will lead to the double data points at the same time period problem.
#so is an aggregation unavoidable?

#---------------------------------------

# Hausman test (plm)
print(hausman_panel<-phtest(form, data = df.temp))
#success in the test (low p-value). But what does it say?
#I think it says that plm can theoretically create better betas than linear regression can do

# Hausman test robust to spatial autocorrelation (splm)
print(spat_hausman_ML_SEM<-sphtest(form,data=df.temp,listw =wknn_10, spatial.model = "error", method="ML"))
#creates error, because it is an unbalanced panel

print(spat_hausman_ML_SAR<-sphtest(form,data=df.temp,listw =wknn_10,spatial.model = "lag", method="ML"))

#--------------------------------------
#trouble shooting section
#putting your index column (such as "id" and "year" identifier column) as the first two column fixed it in my case.
#might work
#the function should at least expect to find the numbers created in the listw object and try to find it in the dataset
#a column has to be added in the orginal dataset in what area the row is in.

```

```{r spatial panel data model - additional tests}

#https://www.insee.fr/en/statistiques/fichier/3635545/imet131-k-chapitre-7.pdf


# Packages needed
library(plm)
library(splm)
library(sp)
library(maps)
library(maptools)
library(spdep)

# Fixed effects model
# Test 1
slmtest(form, data=df.temp, listw = wknn_10, test="lml", model="within")
#Error in bigW %*% hatY : not conformable for multiplication

# Test 2
slmtest(form, data=df.temp, listw = wknn_10, test="lme", model="within")

# Test 3
slmtest(form, data=df.temp, listw = wknn_10, test="rlml", model="within")

# Test 4
slmtest(form, data=df.temp, listw = wknn_10, test="rlme", model="within")

```
Unfortunately, an error occurs. Error message very cryptic. Maybe it has
to do with duplicate couples data points at the same location.


```{r spatial panel data model - additional tests}

#https://www.insee.fr/en/statistiques/fichier/3635545/imet131-k-chapitre-7.pdf


# Packages needed
library(plm)
library(splm)
library(sp)
library(maps)
library(maptools)
library(spdep)

#Estimates of pooled-data model and fixed-effect
# model with spatial errors autocorrelation

# Likelihood Maximum estimation
summary(verdoorn_SEM_pool <- spml(form, data = df.temp, listw = wknn_10, lag=FALSE,model="pooling"))

# Fixed-effect SEM
summary(verdoorn_SEM_FE<- spml(form, data = df.temp, listw = wknn_10, lag=FALSE,model="within", effect="individual", spatial.error="b"))

summary(verdoorn_SEM_FE<- spml(form, data = df.temp, listw = wknn_10, lag=FALSE,model="within", effect="individual", spatial.error="kkp"))

# Generalised moments method estimation
summary(verdoorn_SEM_FE_GM <- spgm(verdoorn, data=df.temp, listw = wknn_10, model="within", moments="fullweights", spatial.error = TRUE))

```




##    INLA

This is a Bayes method, which tries to form a joint distribution over the 
the geographic space.

### First attempt

This is my first attempt

```{r INLA - first try}

#https://www.paulamoraga.com/book-geospatial/sec-inla.html

#install.packages("INLA", repos = "https://inla.r-inla-download.org/R/stable", dep = TRUE)

library(INLA)

prior.prec <- list(prec = list(prior = "pc.prec", param = c(1, 0.01)))
   #this might not be helpful

formula <- redSquatLobster ~ f(depth, model = "iid", hyper = prior.prec)

res <- inla(formula,
  data = df,
  family = "gaussian", Ntrials = n,
  control.predictor = list(compute = TRUE),
  control.compute = list(dic = TRUE)
)

#constant crashes
#good also be about a good initial value


```

Maybe INLA can't handle that much data. try the per cruise aggregated dataset.


```{r INLA - second try with agg data}

#https://www.paulamoraga.com/book-geospatial/sec-inla.html

#install.packages("INLA", repos = "https://inla.r-inla-download.org/R/stable", dep = TRUE)

library(INLA)

sd(agg$redSquatLobster)

prior.prec <- list(prec = list(prior = "pc.prec", param = c(61, 0.01)))
   #this might not be helpful

formula <- redSquatLobster ~ f(depth, model = "iid", hyper = prior.prec)

res <- inla(formula,
  data = agg,
  family = "gaussian", Ntrials = n,
  control.predictor = list(compute = TRUE),
  control.compute = list(dic = TRUE)
)

#failed to get good initial values


```


```{r INLA - third try with new instructions}

#https://cran.r-project.org/web/packages/rSPDE/vignettes/rspde_inla.html

library(ggplot2)
library(INLA)
library(splancs)
library(viridis)
library(rSPDE)

#length with NAs    : length(df$redSquatLobster): 307.243
#length without NAs : length(Y)                 : 213.824

ind <- !is.na(df$redSquatLobster)
ind2 <- !is.na(df$distToCoast)
#sum(ind2) #no missing values here, so unnecessary
ind3 <- ind&ind2 #same as ind
Y = df$redSquatLobster[ind]
#sum(Y[Y == 0]) #no zeroes
coords <- as.matrix(df[ind,1:2])


domain <- inla.nonconvex.hull(as.matrix(st_coordinates(boundaries.peruOcean)), concave = -.07,convex = -0.05, resolution = c(100, 100))
library(sf)

max.edge    = diff(range(st_coordinates(boundaries.peruOcean)[,1]))/(3*5)
bound.outer = diff(range(st_coordinates(boundaries.peruOcean)[,1]))/3

mesh <- inla.mesh.2d(boundary = domain, max.edge = c(1,2)*max.edge, offset=c(max.edge, bound.outer), cutoff = max.edge/5)

plot(mesh, asp = 1, main = "")
lines(st_coordinates(boundaries.peruOcean), col = 3)
points(coords[, 1], coords[, 2], pch = 19, cex = 0.5, col = "red")

#str(boundaries.peruOcean) #sf

#https://rpubs.com/jafet089/886687
#Here is some information, how to do a good mesh

#corrected according to the link. Looks better!

```





```{r INLA - third try with new instructions - create INLA stack}

#https://cran.r-project.org/web/packages/rSPDE/vignettes/rspde_inla.html

library(ggplot2)
library(INLA)
library(splancs)
library(viridis)
library(rSPDE)

Abar <- rspde.make.A(mesh = mesh, loc = coords)
rspde_model <- rspde.matern(mesh = mesh)
mesh.index <- rspde.make.index(name = "field", mesh = mesh)

stk.dat <- inla.stack(
  data = list(y = Y), A = list(Abar, 1), tag = "est",
  effects = list(
    c(
      mesh.index
    ),
    list(
      distCoast = inla.group(df$distToCoast[ind]),
      Intercept = 1
    )
  )
)

#maybe the value for y must be a single columnn like redSquatLobster instead of a dataframe?
#Y is now just redSquatLLobster

```

```{r INLA - third try with new instructions - model fitting}

#https://cran.r-project.org/web/packages/rSPDE/vignettes/rspde_inla.html

library(ggplot2)
library(INLA)
library(splancs)
library(viridis)
library(rSPDE)

f.s <- y ~ -1 + Intercept + f(distCoast, model = "rw1") +
  f(field, model = rspde_model)

rspde_fit <- inla(f.s,
  family = "Gaussian", data = inla.stack.data(stk.dat),
  verbose = TRUE,
  control.inla = list(int.strategy = "eb"),
  control.predictor = list(A = inla.stack.A(stk.dat), compute = TRUE)
)

#it runs 1.5 minutes!

#----------------------------------
#trouble shooting section

#still failed to get good enough values -.-
#*** ERROR *** 	INLA.Data1: Gamma data[0] (y) = 0 or weight 1 is void

#https://groups.google.com/g/r-inla-discussion-group/c/NFSyzP3qK4w/m/3sjFymisAQAJ
#here it is suggested that the error goes back to a 0 being in the data
#double checked for NAs

#changed family from Gamma to Gaussian. Now it runs, but goes to assertion failed sometimes.
#---------------------------------

summary(rspde_fit)

```


Check results

```{r INLA - third try with new instructions - model result}

#https://cran.r-project.org/web/packages/rSPDE/vignettes/rspde_inla.html

library(ggplot2)
library(INLA)
library(splancs)
library(viridis)
library(rSPDE)

result_fit <- rspde.result(rspde_fit, "field", rspde_model)
summary(result_fit)

posterior_df_fit <- gg_df(result_fit)

ggplot(posterior_df_fit) + geom_line(aes(x = x, y = y)) + 
facet_wrap(~parameter, scales = "free") + labs(y = "Density")


```

Now we want to predict values on the whole mesh (kriging)

```{r INLA - third try with new instructions - predictions}

#https://cran.r-project.org/web/packages/rSPDE/vignettes/rspde_inla.html

library(ggplot2)
library(INLA)
library(splancs)
library(viridis)
library(rSPDE)

nxy <- c(150, 100)
projgrid <- rspde.mesh.projector(mesh,
  xlim = range(st_coordinates(boundaries.peruOcean)[,1]),
  ylim = range(st_coordinates(boundaries.peruOcean)[,2]), dims = nxy
)

xy.in <- inout(projgrid$lattice$loc, cbind(st_coordinates(boundaries.peruOcean)[,1], st_coordinates(boundaries.peruOcean)[,2]))

coord.prd <- projgrid$lattice$loc[xy.in, ]
plot(coord.prd, type = "p", cex = 0.1)
lines(st_coordinates(boundaries.peruOcean))
points(coords[, 1], coords[, 2], pch = 19, cex = 0.5, col = "red")


```

```{r INLA - third try with new instructions - predictions 2}

#https://cran.r-project.org/web/packages/rSPDE/vignettes/rspde_inla.html

library(ggplot2)
library(INLA)
library(splancs)
library(viridis)
library(rSPDE)

A.prd <- projgrid$proj$A[xy.in, ]

#---------------------------------------

#Problem: The distance to the coast shall be calculated


#boundaries.peruOcean has 342 entries, which one stand for the coast?
#from google maps
  #the north coast starts from - 3.35226 / -80.433214
  #the south coast end in -18.36891 / -70.41494
  #Idea: Just draw a line from the north to the south according to Google Maps coordinates

coastline = array(dim = c(100, 2))
coastline[,1] = seq(from = -80.433214,to = -70.41494, length.out = 100)
coastline[,2] = seq(from = - 3.35226,to = -18.36891, length.out = 100)
      #that's really the most complicated way an array was ever created

#st_coordinates(boundaries.peruOcean)[1:343,1:2]
    #coordinate system from google maps and here seem not to be the same

#plot(st_coordinates(boundaries.peruOcean)[2499:2500,1:2])
    #points go way beyond 343

#library(cartography)
#getOuterBorders(boundaries.peruOcean)

#str(st_coordinates(boundaries.peruOcean)[122:242,1:2])


#--------------------------------------------

distCoast.prd <- apply(spDists(coord.prd,
  coastline,  #here the coastline shall be stated
  longlat = TRUE
), 1, min)

```

```{r INLA - third try with new instructions - predictions 2}

#https://cran.r-project.org/web/packages/rSPDE/vignettes/rspde_inla.html

library(ggplot2)
library(INLA)
library(splancs)
library(viridis)
library(rSPDE)


ef.prd <- list(
  c(mesh.index),
  list(
    long = inla.group(coord.prd[
      ,
      1
    ]), lat = inla.group(coord.prd[, 2]),
    seaDist = inla.group(distCoast.prd),
    Intercept = 1
  )
)


stk.prd <- inla.stack(
  data = list(y = NA),
  A = list(A.prd, 1), tag = "prd",
  effects = ef.prd
)


stk.all <- inla.stack(stk.dat, stk.prd)


rspde_fitprd <- inla(f.s,
  family = "Gaussian",
  data = inla.stack.data(stk.all),
  verbose = TRUE,
  control.predictor = list(
    A = inla.stack.A(stk.all),
    compute = TRUE, link = 1
  ),
  control.compute = list(
    return.marginals = FALSE,
    return.marginals.predictor = FALSE
  ),
  control.inla = list(int.strategy = "eb")
)

#this also just works with verbose = TRUE

#watch out, runtime = 258.982 seconds


```

```{r INLA - third try with new instructions - check predictions}

#https://cran.r-project.org/web/packages/rSPDE/vignettes/rspde_inla.html

library(ggplot2)
library(INLA)
library(splancs)
library(viridis)
library(rSPDE)

id.prd <- inla.stack.index(stk.all, "prd")$data
m.prd <- rspde_fitprd$summary.fitted.values$mean[id.prd]
sd.prd <- rspde_fitprd$summary.fitted.values$sd[id.prd]

# Plot the predictions
pred_df <- data.frame(x1 = coord.prd[,1],
                      x2 = coord.prd[,2],
                      mean = m.prd,
                      sd = sd.prd)

ggplot(pred_df, aes(x = x1, y = x2, fill = mean)) +
  geom_raster() +
  scale_fill_viridis()


```
That looks already quite well. Buuut, numbers not realistic.
Marine animal pop much less spred out and more singular.


```{r INLA - third try with new instructions - check predictions 2}

#https://cran.r-project.org/web/packages/rSPDE/vignettes/rspde_inla.html

library(ggplot2)
library(INLA)
library(splancs)
library(viridis)
library(rSPDE)

#now we can plot std in prediction of area
ggplot(pred_df, aes(x = x1, y = x2, fill = sd)) +
  geom_raster() + scale_fill_viridis()


```

Possible errors are:
Wrong model
Wrong calculation of distance to the coast?
    tried to fix it
boundaries.peruOcan is the wrong sf file, since this is already
the grit version for partitioning the space

###   other example

https://punama.github.io/BDI_INLA/

Here is another example. It has at least very good graphics.


##    Other spatial examination - kriging


```{r kriging}

#https://www.r-bloggers.com/2015/08/spatio-temporal-kriging-in-r/

#install.packages("gstat")
library(gstat)
library(sp)
library(spacetime)
library(raster)
library(rgdal)
library(rgeos) 

#takes a little longer to run through
demo(stkrige) 

```

make a first variogram

```{r variogram}

#http://rstudio-pubs-static.s3.amazonaws.com/21593_ac892ba517084b309776e9fc17b2ebb0.html

#install.packages("geoR")
require("geoR")

#we run into memory problems here, so we must take aa sample

ind <- sample(dim(df)[1], 10000)
df.temp <- df[ind,]

#------------------------------

df.temp <- as.geodata(df.temp, coords.col = 1:2, data.col = 22)  #variogram for lobster column 
#
dup.coords(df.temp)
#There are some in the full dataset.
#Unlikely in the sample case

df.temp <- jitterDupCoords(df.temp, max = 1) #looks like before

### Building and plotting variogram
Var1.Lobster <- variog(df.temp, trend = "1st") 
         #Error: cannot allocate vector of size 170.3 Gb
Var2.Lobster <- variog(df.temp, trend = "2nd")

#### Plot side by side
plot.new()
par(mfrow = c(2, 1))
plot(Var1.Lobster, pch = 19, col = "blue", main = "1st order variogram")
plot(Var2.Lobster, pch = 19, col = "red", main = "2nd order variogram")

#### Plot overlaying
plot.new()
par(mfrow = c(1, 1))
plot(Var1.Lobster, pch = 19, col = "blue", main = "1st order variogram")
par(new = TRUE)
plot(Var2.Lobster, pch = 19, col = "red", main = "2nd order variogram")

```


```{r variogram - continued}

#http://rstudio-pubs-static.s3.amazonaws.com/21593_ac892ba517084b309776e9fc17b2ebb0.html

##### ??? Question: How to choose the 1st or 2nd order?

### Fitting 1st EC variogram
ini.vals <- expand.grid(seq(100000, 500000, by = 100), seq(0.001, 50, by = 100))
ols <- variofit(Var1.Lobster, ini = ini.vals, fix.nug = TRUE, wei = "equal")

summary(ols)

#---------------------

wls <- variofit(Var1.Lobster, ini = ini.vals, fix.nug = TRUE, wei = "npairs")
summary(wls)

#-----------------------
#### Plot overlaying
plot.new()
par(mfrow = c(1, 2))
plot(Var1.Lobster, main = "1st order EC Semivar", pch = 19, col = "blue")
lines(wls, lty = 2, col = "green")
lines(ols, lty = 3, col = "blue")


```

Somethings wrong here.

# Use Transformer for prediction

##    Temporal Fusion Transformer

The big advantage of using a temporal fusion transformer is, that the TFT
accepts and declares regressors, which are known in the future, not known in
the future and which are static. E.g. the location of a restaurant is static, since
it doesn't change in time.

Follow the tutorial on: https://mlverse.github.io/tft/articles/Getting-started.html


```{r  download temporal fusion transformer}
#you also have to install a bunch of packets like torch beforehand
#Rstudio will tell you in a yellow banner above. Please execute that.


#remotes::install_github("mlverse/tft")
suppressPackageStartupMessages(library(tidymodels))
library(tft)
set.seed(1)
torch::torch_manual_seed(1)

```

We already have lots of aggregations, we can use but we could still use a weekly
aggregation as it is done in the tutorial.



### work with weekly data


```{r  tft - weekly aggregation}

df.region.k199.min$date <- as.Date(df.region.k199$date)

df.temp <- df.region.k199.min %>% 
  mutate(Date = lubridate::floor_date(date, unit = "week")) %>% 
  group_by(Date, region_name) %>% 
  summarise(across(everything(), .fns = ~mean(.x, na.rm = TRUE)), .groups = "drop")


colnames(df.temp)[2] ="region"

df.temp %>% 
   ggplot(aes(x = date, y = anchovy, color = region)) +
   geom_line()

df.temp <- df.temp[, -c(1,3)]

```



```{r  split the data into train and testdata}


last_date <- max(df$date)
train <- df.temp %>% filter(date <= (last_date - lubridate::weeks(48)))
valid <- df.temp %>% filter(date > (last_date - lubridate::weeks(48)),
                              date <= (last_date - lubridate::weeks(12)))
test <- df.temp %>% filter(date > (last_date - lubridate::weeks(12)))


```






###   preperations

```{r  recipe the data}

rec <- recipe(anchovy ~ ., data = train) %>% 
  step_mutate(
    time_since_begining = as.numeric(difftime(
      time1 = date, 
      time2 = lubridate::ymd(min(df$date)), 
      units = "weeks"
    )),
    date_week = as.factor(lubridate::week(date)),
    date_month = as.factor(lubridate::month(date)),
    date_wday = as.factor(lubridate::wday(date))
  ) %>% 
  step_impute_mean(all_numeric_predictors()) %>% #do we have to explicitly state the var here?
  step_normalize(all_numeric_predictors())



```


```{r  make the specifications - without known}


spec <- tft_dataset_spec(rec, train) %>% 
  spec_covariate_index(date) %>% 
  spec_covariate_key(region) %>%
  spec_covariate_known(time_since_begining, starts_with("date_"))

#index is the variable which shall be iterated over
#with key you can tell a group of timeseries
#known are variables, which are known, even in the future
  #here that is 

```



```{r  make the specifications - with known}

#All the variables are known, since our testdata is froma full dataset
known <- colnames(train)[5:dim(train)[2]]
known <- paste(known,collapse=', ')
#known <- as.symbol(known)
#known

spec_known <- tft_dataset_spec(rec, train) %>% 
  spec_covariate_index(date) %>% 
  spec_covariate_key(region) %>%
  spec_covariate_known(as.symbol(known), time_since_begining, starts_with("date_"))

#index is the variable which shall be iterated over
#with key you can tell a group of timeseries
#known are variables, which are known, even in the future
  #here that is 

```

```{r  specify the specifications}

#lookback should be set between 10 and 1000
#horizon is the amount of weeks (?), which shall be forecasted


spec <- spec %>% 
  spec_time_splits(lookback = 5*12, horizon = 12)


```

```{r  look at the specifications}

spec <- prep(spec) #doesn't except any custom arguments with spec
spec

#prep runs, but this should look differently
#A <prepared_tft_dataset_spec> with



```

###   Initialize the model


```{r  initialize the model}

model <- temporal_fusion_transformer(
  spec, 
  hidden_state_size = 8,
  learn_rate = 1e-3, 
  dropout = 0.5, 
  num_attention_heads = 1, 
  num_lstm_layers = 1
)
model

#Error in Tensor_slice(tensor, environment(), drop = drop, mask = .d) :
#index 1 is out of bounds for dimension 1 with size 0

```



Find the best learning rate

```{r  find best learning rate the model}

result <- luz::lr_finder(
  model, 
  transform(spec, train), 
  end_lr = 1,
  dataloader_options = list(
    batch_size = 64
  ),
  verbose = TRUE
)
plot(result) + ggplot2::coord_cartesian(ylim = c(0.15, 0.5))

```


The learning rate should be set at: 1e-3 looks good.



Redefine model fixing the learnrate


```{r  redefine the model}

model <- temporal_fusion_transformer(
  spec, 
  hidden_state_size = 8,
  learn_rate = 1e-3, 
  dropout = 0.5, 
  num_attention_heads = 1, 
  num_lstm_layers = 1
)


```

##    Fit the model


```{r  fit the model}


fitted <- model %>% 
  fit(
    transform(spec),
    valid_data = transform(spec, new_data = valid),
    epochs = 100,
    callbacks = list(
      luz::luz_callback_keep_best_model(monitor = "valid_loss"),
      luz::luz_callback_early_stopping(
        monitor = "valid_loss", 
        patience = 5, 
        min_delta = 0.001
      )
    ),
    verbose = FALSE,
    dataloader_options = list(batch_size = 64, num_workers = 4)
  )

plot(fitted)


```

Evaluate the model



```{r  evaluate the model}

fitted %>% 
  luz::evaluate(
    transform(spec, new_data = test, past_data = bind_rows(train, valid))
  )

```


Make quick forecast with the forecast function


```{r  forecast the model}

forecasts <- generics::forecast(fitted, past_data = branch_21_min)
glimpse(forecasts)


```


```{r  plot the forecast of the model}

branch_21_min %>% 
  filter(Date > lubridate::ymd("2019-01-01")) %>% 
  full_join(forecasts) %>% #merge done here
  ggplot(aes(x = Date, y = Sales)) +
  geom_line() +
  geom_line(aes(y = .pred), color = "blue") +
  geom_ribbon(aes(ymin = .pred_lower, ymax = .pred_upper), alpha = 0.3) +
  facet_wrap(~Group)



```

Look into the model and the fitted variable


```{r  look into model and fitted variable}

str(model)
class(model)


```





###   work with yearly data

```{r  split the data into train and testdata}


last_date <- max(agg.region.k199.year.balanced.imp$year)
first_date <- min(agg.region.k199.year.balanced.imp$year)


split_year = 2006
split_year_2 = 2010


train <- agg.region.k199.year.balanced.imp %>% filter(year <= split_year)
valid <- agg.region.k199.year.balanced.imp %>% filter((year > split_year), (year <= split_year_2))
test <- agg.region.k199.year.balanced.imp %>% filter(year > split_year_2)



#we have to delete the geometry column
#geometry not deletable if it stay an sf object
train <- as.data.frame(train)[,1:24]
valid <- as.data.frame(valid)[,1:24]
test  <- as.data.frame(test)[,1:24]

#year must be a date type object, also it is convenient to rename it
train$Date <- as.Date(train$year, origin = "1983-01-01")
valid$Date <- as.Date(valid$year, origin = "1983-01-01")
test$Date  <- as.Date(test$year, origin = "1983-01-01")

for (i in 3:24){
train[,i] <- as.numeric(train[,i])
valid[,i] <- as.numeric(valid[,i])
test[,i]  <- as.numeric(test[,i])
}

str(train)

#remove year
train <- train[,-2]
valid <- valid[,-2]
test  <- test[,-2]


```

If you split the yearly data into 3 chunks, that might lead to an insufficient
amount of data. Train is 4.776, valid 796, test 398.

####   preperations

```{r  recipe the data}

rec <- recipe(redSquatLobster ~ ., data = train) %>% 
  step_impute_mean(all_numeric_predictors()) %>% #do we have to explicitly state the var here?
  step_normalize(all_numeric_predictors())



```


```{r  make the specifications - without known}


spec <- tft_dataset_spec(rec, train) %>% 
  spec_covariate_index(Date) %>% 
  spec_covariate_key(region) %>%
  spec_covariate_known(starts_with("date_"))

#index is the variable which shall be iterated over
#with key you can tell a group of timeseries
#known are variables, which are known, even in the future
  #here that is 

```



```{r  make the specifications - with known}

#All the variables are known, since our testdata is from a full dataset
known <- colnames(train)[5:dim(train)[2]]
known <- paste(known,collapse=', ')
#known <- as.symbol(known)
#known

spec_known <- tft_dataset_spec(rec, train) %>% 
  spec_covariate_index(year) %>% 
  spec_covariate_key(region) %>%
  spec_covariate_known(as.symbol(known), starts_with("date_"))

#index is the variable which shall be iterated over
#with key you can tell a group of timeseries
#known are variables, which are known, even in the future
  #here that is 

```

```{r  specify the specifications}

#lookback should be set between 10 and 1000
#horizon is the amount of weeks (?), which shall be forecasted


spec <- spec %>% 
  spec_time_splits(lookback = 10, horizon = 3)
#lookback is the amount of periods, which are taken for prediction
#horizon is the amount of periods, which is predicted in one go

```

```{r  look at the specifications}

spec <- prep(spec) #doesn't except any custom arguments with spec
spec

#prep runs, but this should look differently
#A <prepared_tft_dataset_spec> with



```

###   Initialize the model


```{r  initialize the model}


model <- temporal_fusion_transformer(
  spec, 
  hidden_state_size = 8,
  learn_rate = 1e-3, 
  dropout = 0.5, 
  num_attention_heads = 1, 
  num_lstm_layers = 1
)
model


#Error in Tensor_slice(tensor, environment(), drop = drop, mask = .d) :
#index 1 is out of bounds for dimension 1 with size 0

```

[!!!] I don't know how to fix that error!


Find the best learning rate

```{r  find best learning rate the model}

result <- luz::lr_finder(
  model, 
  transform(spec, train), 
  end_lr = 1,
  dataloader_options = list(
    batch_size = 64
  ),
  verbose = TRUE
)
plot(result) + ggplot2::coord_cartesian(ylim = c(0.15, 0.5))

```


The learning rate should be set at: 1e-3 looks good.



Redefine model fixing the learnrate


```{r  redefine the model}

model <- temporal_fusion_transformer(
  spec, 
  hidden_state_size = 8,
  learn_rate = 1e-3, 
  dropout = 0.5, 
  num_attention_heads = 1, 
  num_lstm_layers = 1
)


```

##    Fit the model


```{r  fit the model}


fitted <- model %>% 
  fit(
    transform(spec),
    valid_data = transform(spec, new_data = valid),
    epochs = 100,
    callbacks = list(
      luz::luz_callback_keep_best_model(monitor = "valid_loss"),
      luz::luz_callback_early_stopping(
        monitor = "valid_loss", 
        patience = 5, 
        min_delta = 0.001
      )
    ),
    verbose = FALSE,
    dataloader_options = list(batch_size = 64, num_workers = 4)
  )

plot(fitted)


```

Evaluate the model



```{r  evaluate the model}

fitted %>% 
  luz::evaluate(
    transform(spec, new_data = test, past_data = bind_rows(train, valid))
  )

```


Make quick forecast with the forecast function


```{r  forecast the model}

forecasts <- generics::forecast(fitted, past_data = branch_21_min)
glimpse(forecasts)


```


```{r  plot the forecast of the model}

branch_21_min %>% 
  filter(Date > lubridate::ymd("2019-01-01")) %>% 
  full_join(forecasts) %>% #merge done here
  ggplot(aes(x = Date, y = Sales)) +
  geom_line() +
  geom_line(aes(y = .pred), color = "blue") +
  geom_ribbon(aes(ymin = .pred_lower, ymax = .pred_upper), alpha = 0.3) +
  facet_wrap(~Group)



```

Look into the model and the fitted variable


```{r  look into model and fitted variable}

str(model)
class(model)


```

### Result

Unfortunately, neither weekly, nor yearly data has run through.
There seems to be a problem with Pytorch?

